{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **RADI623: Natural Language Processing**\n",
    "\n",
    "### Assignment: Natural Language Processing\n",
    "**Romen Samuel Rodis Wabina** <br>\n",
    "Student, PhD Data Science in Healthcare and Clinical Informatics <br>\n",
    "Clinical Epidemiology and Biostatistics, Faculty of Medicine (Ramathibodi Hospital) <br>\n",
    "Mahidol University\n",
    "\n",
    "Note: In case of Python Markdown errors, you may access the assignment through this GitHub [Link](https://github.com/rrwabina/RADI605/tree/main)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Medical Specialty Identification**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem of predicting oneâ€™s illnesses wrongly through self-diagnosis in medicine is very real. In a report by the [Telegraph](https://www.telegraph.co.uk/news/health/news/11760658/One-in-four-self-diagnose-on-the-internet-instead-of-visiting-the-doctor.html), nearly one in four self-diagnose instead of visiting the doctor. Out of those who misdiagnose, nearly half have misdiagnosed their illness wrongly [reported](https://bigthink.com/health/self-diagnosis/). While there could be multiple root causes to this problem, this could stem from a general unwillingness and inability to seek professional help.\n",
    "\n",
    "Elevent percent of the respondents surveyed, for example, could not find an appointment in time. This means that crucial time is lost during the screening phase of a medical treatment, and early diagnosis which could have resulted in illnesses treated earlier was not achieved.\n",
    "\n",
    "With the knowledge of which medical specialty area to focus on, a patient can receive targeted help much faster through consulting specialist doctors. To alleviate waiting times and predict which area of medical specialty to focus on, we can utilize natural language processing (NLP) to solve this task.\n",
    "\n",
    "Given any medical transcript or patient condition, this solution would predict the medical specialty that the patient should seek help in. Ideally, given a sufficiently comprehensive transcript (and dataset), one would be able to predict exactly which illness he is suffering from."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy  as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import re\n",
    "import logging\n",
    "import random\n",
    "import os\n",
    "import itertools\n",
    "import copy\n",
    "import time\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.pipeline.tagger import Tagger\n",
    "from spacy.language import Language\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from datasets import load_dataset\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import BertModel, BertTokenizer, DistilBertModel\n",
    "from transformers import AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import TensorDataset\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "import transformers\n",
    "from time import time\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, BertConfig, get_linear_schedule_with_warmup\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForTokenClassification\n",
    "from transformers import RobertaModel\n",
    "from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler\n",
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer)\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior to NLP task, we first ensured a deterministic behavior of our simulations by setting the seed value for various random number generators in order to obtain consistent results and facilitate reproducibility in the execution of the code. The <code>set_seed</code> function was directly copied from the Professor's lecture codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    if seed:\n",
    "        logging.info(f'Running in deterministic mode with seed {seed}')\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    else:\n",
    "        logging.info('Running in non-deterministic mode')\n",
    "set_seed(1997)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <code>Question 1</code>. Conduct an exploratory data analysis on the dataset.  Report the corpus statistics with appropriate visualization.  Explain clearly why the selected visualization fits your purpose.  \n",
    "\n",
    "Let's first explore the dataset first! The dataset, <code>mtsamples.csv</code>, consists of 4999 samples (rows) with 5 features (columns) - namely <code>description, medical_specialty, sample_name, transcription, and keywords.</code> Since this is a text classification task, the <code>medical_specialty</code> serves as the labels of the dataset. There are 40 unique <code>medical_specialty</code> in the original dataset, leading to a multiclass classification. Some of the labels include Allergy/Immunology, Bariatrics, Cardiovascular/Pulmonary, and General Medicine. There are labels, however, that are not directly related to medical specialties, including Letters, Office Notes, and SOAP/Chart/Progress Notes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in the original dataset: \t 4999\n",
      "Number of unique labels in the original dataset: 40\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('../data/mtsamples.csv')\n",
    "\n",
    "num_samples = len(data)\n",
    "num_medical_specialties = data['medical_specialty'].nunique()\n",
    "\n",
    "print(f'Number of samples in the original dataset: \\t {num_samples}')\n",
    "print(f'Number of unique labels in the original dataset: {num_medical_specialties}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code below, we created functions that can help us to understand the nature of the given dataset. The function <code>calculate_univariate</code> takes a <code>data</code> parameter to display the univariate characteristics of the <code>transcription</code> and <code>description</code>. Specifically, the <code>data</code> parameter should be a DataFrame that contains the columns: <code>transcription</code> and <code>description</code>. This code calculates various statistical measures (average, minimum, maximum) for the lengths of the <code>transcription</code> and <code>description</code> strings in the input data and returns the results in a dictionary format.\n",
    "\n",
    "The function <code>plot_classes</code> takes a <code>data</code> parameter, assumed to be a DataFrame, that can generate a bar plot to visualize the distribution of medical specialties in the input data. This function will be useful to visualize how many samples exist in the dataset for every class (i.e., class distribution). The x-axis represents the medical specialties while the y-axis represents the frequency counts.\n",
    "\n",
    "Similar to previous functions, the <code>plot_histogram</code> takes a DataFrame <code>data</code> parameter that can generate a histogram plot to visualize the distribution of the lengths of the <code>transcription</code> and <code>description</code> values in the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_univariate(data):\n",
    "    description_lengths   = data['description'].str.len()\n",
    "    transcription_lengths = data['transcription'].str.len()\n",
    "\n",
    "    avg_description_length = description_lengths.mean()\n",
    "    min_description_length = description_lengths.min()\n",
    "    max_description_length = description_lengths.max()\n",
    "\n",
    "    avg_transcription_length = transcription_lengths.mean()\n",
    "    min_transcription_length = transcription_lengths.min()\n",
    "    max_transcription_length = transcription_lengths.max()\n",
    "\n",
    "    dictionary = {}\n",
    "    dictionary['description']   = [avg_description_length, min_description_length, max_description_length]\n",
    "    dictionary['transcription'] = [avg_transcription_length, min_transcription_length, max_transcription_length]\n",
    "    return dictionary\n",
    "summary = calculate_univariate(data)\n",
    "\n",
    "def plot_classes(data):\n",
    "    specialty_counts = data['medical_specialty'].value_counts()\n",
    "\n",
    "    plt.figure(figsize = (10, 5))\n",
    "    plt.bar(specialty_counts.index, specialty_counts.values)\n",
    "    plt.xlabel('Medical Specialty')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Medical Specialties')\n",
    "    plt.xticks(rotation = 90)\n",
    "    plt.show()\n",
    "\n",
    "def plot_histogram(data):\n",
    "    description_lengths   = data['description'].str.len()\n",
    "    transcription_lengths = data['transcription'].str.len()\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    axs[0].hist(description_lengths, bins=50, alpha=0.8)\n",
    "    axs[0].set_xlabel('Description Length')\n",
    "    axs[0].set_ylabel('Frequency')\n",
    "    axs[0].set_title('Histogram of Description Lengths')\n",
    "\n",
    "    axs[1].hist(transcription_lengths, bins = 50, alpha = 0.8)\n",
    "    axs[1].set_xlabel('Transcription Length')\n",
    "    axs[1].set_ylabel('Frequency')\n",
    "    axs[1].set_title('Histogram of Transcription Lengths')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset exhibits a significant imbalance in terms of document distribution across medical specialties. <code>Surgery</code> stands out as the most frequent specialty, comprising 1088 documents, while <code>Hospice - Palliative Care</code> has the lowest representation with a mere 6 documents. This discrepancy in frequencies indicates a clear imbalance, where there is a substantial over-representation of documents related to <code>Surgery</code> compared to other specialties, such as <code>Hospice - Palliative Care</code>. Such data imbalances can pose challenges in training accurate and fair machine learning models, as the model may be biased towards the majority class and struggle to generalize well to minority classes.\n",
    "\n",
    "To rectify this imbalance, several techniques can be employed. One common approach is oversampling or undersampling. Oversampling involves duplicating instances from the minority class to increase its representation, while undersampling involves removing instances from the majority class to reduce its dominance. Another technique is generating synthetic samples using methods like Synthetic Minority Over-sampling Technique (SMOTE), which creates new instances by interpolating between existing minority class samples. Additionally, techniques like stratified sampling during train-test splitting or cross-validation can ensure that each subset maintains the original class distribution. *We can rectify this problem later.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_histogram(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function <code>get_sentence_word_count</code> takes a list of texts as input and calculates the total number of sentences and unique words within the given texts. It achieves this by tokenizing the texts into sentences and words, and then iteratively counting the sentences and updating a vocabulary dictionary. The number of sentences provides an understanding of the overall length and complexity of the text. This can be useful for determining the granularity of text classification, similar to this project. Moreover, the number of unique words represents the vocabulary size of the text data. This indicates the diversity and richness of the language used in the transcriptions. Larger vocabulary sizes can be indicative of more varied topics or specialized domain-specific terminology. This code is valuable for gaining insights into the text data found in the <code>transcriptions</code> column. By determining the sentence count and word count, it provides an understanding of the text's structure and lexical richness. \n",
    "\n",
    "**The results obtained from the code indicate that there are 140,208 sentences and 35,805 unique words in the <code>transcriptions</code> column of the text data. Since this dataset contains a large number fo sentences, models such as Transformers can be a viable choice since they can handle large sequential data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in transcriptions column: 140208\n",
      "Number of unique words in transcriptions column: 35805\n"
     ]
    }
   ],
   "source": [
    "def get_sentence_word_count(text_list):\n",
    "    sent_count = 0\n",
    "    word_count = 0\n",
    "    vocab = {}\n",
    "    for text in text_list:\n",
    "        sentences  = sent_tokenize(str(text).lower())\n",
    "        sent_count = sent_count + len(sentences)\n",
    "        for sentence in sentences:\n",
    "            words  = word_tokenize(sentence)\n",
    "            for word in words:\n",
    "                if(word in vocab.keys()):\n",
    "                    vocab[word] = vocab[word] +1\n",
    "                else:\n",
    "                    vocab[word] = 1 \n",
    "    word_count = len(vocab.keys())\n",
    "    return sent_count,word_count\n",
    "\n",
    "clinical_text_df = data[data['transcription'].notna()]\n",
    "sent_count, word_count = get_sentence_word_count(clinical_text_df['transcription'].tolist())\n",
    "\n",
    "print('Number of sentences in transcriptions column: '    + str(sent_count))\n",
    "print('Number of unique words in transcriptions column: ' + str(word_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in transcriptions column: 129744\n",
      "Number of unique words in transcriptions column: 35088\n"
     ]
    }
   ],
   "source": [
    "data_categories  = clinical_text_df.groupby(clinical_text_df['medical_specialty'])\n",
    "filtered_data_categories = data_categories.filter(lambda x:x.shape[0] > 50)\n",
    "final_data_categories = filtered_data_categories.groupby(filtered_data_categories['medical_specialty'])\n",
    "sent_count, word_count = get_sentence_word_count(filtered_data_categories['transcription'].tolist())\n",
    "\n",
    "print('Number of sentences in transcriptions column: '    + str(sent_count))\n",
    "print('Number of unique words in transcriptions column: ' + str(word_count))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <code>Question 2</code>. Draw a block diagram to illustrate an overview of your experiment.  Each block represents a task required in a deep learning experiment such as vectorization, data splitting, etc.\n",
    "\n",
    "The figure below illustrates a comprehensive pipeline for NLP text classification in the domain of medical transcription. This pipeline encompasses crucial steps necessary for achieving accurate classification results, including data preprocessing, tokenization, random oversampling, dataset iteration involving shuffle and batching, dataset splitting, embeddings, LSTM classification, and evaluation metrics. By providing a systematic and structured approach, this pipeline serves as a valuable guide for viewers, enabling them to effectively address the challenges specific to the medical transcription domain.\n",
    "\n",
    "To initiate the experiment, we load the given dataset and proceed with a rigorous data preprocessing phase. This preprocessing step focuses on cleaning the data by removing irrelevant information and handling missing values. Additionally, duplicate samples and those with missing values are eliminated from the dataset. In cases where duplicates with different labels exist, we retain the first label indicated in the dataset. \n",
    "\n",
    "It is important to note that the classification task involves 40 distinct labels derived from the mtsamples dataset. However, having 40 classes for text classification presents certain disadvantages, including increased complexity and data sparsity. Due to the rarity of some classes or limited availability of training samples, the model may not have sufficient data to effectively learn the distinguishing features of these classes. To address this, we apply a filtering approach to remove classes with sample counts lower than the median value of 130, as determined by a non-parametric calculation. This filtering process is denoted in the code snippet as <code>median_count</code>. By implementing this filtering approach, we aim to ensure that the model receives an adequate amount of data for each class, facilitating effective learning and improving classification performance. Removing classes with insufficient samples helps mitigate the challenges associated with data sparsity and allows the model to focus on the more well-represented classes, leading to a more balanced and robust classification system.\n",
    "\n",
    "After data preprocessing, the modified dataset <code>mtsamples_modified.csv</code> undergoes tokenization. This involves breaking down the text into individual words, denoted as tokens. This step is fundamental as it provides a granular representation of the text, enabling further processing and analysis. Tokenization lays the foundation for subsequent steps in the pipeline by converting the textual data into a format that can be effectively understood and processed by the model. We created a class called <code>TokenizationProcessor</code> that takes the modified <code>mtsamples</code> as input in a DataFrame format and automatically tokenize it using the pre-trained BERT tokenizer. We will discuss more about this later.\n",
    "\n",
    "<center>\n",
    "<img src = '../figures/framework_final.png' width = '1500'/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_columns = ['description', 'medical_specialty', 'sample_name', 'transcription', 'keywords']\n",
    "reduced_df = filtered_data_categories[df_columns]\n",
    "for col in df_columns:\n",
    "    reduced_df = reduced_df.drop(reduced_df[reduced_df[col].isna()].index)\n",
    "\n",
    "low_median = []\n",
    "median_count = np.median(reduced_df['medical_specialty'].value_counts())\n",
    "for item in reduced_df['medical_specialty'].value_counts().items():\n",
    "    if item[1] <= median_count:\n",
    "        low_median.append(item[0])\n",
    "reduced_df = reduced_df[~reduced_df['medical_specialty'].isin(low_median)]\n",
    "reduced_df = reduced_df[ reduced_df['medical_specialty'] != ' SOAP / Chart / Progress Notes']\n",
    "reduced_df.to_csv('../data/mtsamples_modified.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following features were eliminated from the original dataset since the number of their samples are less than the median sample count. By implementing this filtering approach, we aim to ensure that the model receives an adequate amount of data for each class, facilitating effective learning and improving classification performance. Removing classes with insufficient samples helps mitigate the challenges associated with data sparsity and allows the model to focus on the more well-represented classes, leading to a more balanced and robust classification system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From the original dataset, we have reduced the mtsamples data to 2738 samples and 5 features.      These features still incldues description, medical_specialty, sample_name, and transcription\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' Obstetrics / Gynecology',\n",
       " ' ENT - Otolaryngology',\n",
       " ' Neurosurgery',\n",
       " ' Ophthalmology',\n",
       " ' Discharge Summary',\n",
       " ' Nephrology',\n",
       " ' Hematology - Oncology',\n",
       " ' Pain Management',\n",
       " ' Pediatrics - Neonatal',\n",
       " ' Emergency Room Reports',\n",
       " ' Psychiatry / Psychology']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'From the original dataset, we have reduced the mtsamples data to {reduced_df.shape[0]} samples and {reduced_df.shape[1]} features.\\\n",
    "      These features still incldues description, medical_specialty, sample_name, and transcription')\n",
    "low_median"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reduced_df.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <code>preprocessing</code> function takes a sentence, removes hyperlinks, performs various token-level filters (removing stop words, symbols, punctuation marks, and whitespace), lemmatizes the remaining tokens to their base forms, and returns the cleaned sentence as a string. Specifically, the code <code>token.pos_ != 'SYM' and token.pos_ != 'PUNCT' and token.pos_ != 'SPACE'</code> checks if the token's part-of-speech (POS) tag is not 'SYM' (symbol), 'PUNCT' (punctuation), or 'SPACE'. It further filters out tokens that are symbols, punctuation marks, or represent whitespace. We also appended the lowercase lemma (base form) of the token, obtained using <code>token.lemma_</code>, to the cleaned_tokens list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_hyperlinks(sentence):\n",
    "    ''' \n",
    "    Parameters: sentence (string)\n",
    "    Returns a string with removed hyperlinks and other punctuation marks\n",
    "    '''\n",
    "    sentence = re.sub(\n",
    "        '(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\"', \" \", sentence)\n",
    "    return sentence\n",
    "\n",
    "def preprocessing(sentence):\n",
    "    ''' \n",
    "    Removes hyperlinks, performs various token-level filters (removing stop words, \n",
    "    symbols, punctuation marks, and whitespace), lemmatizes the remaining tokens to their base forms, and returns \n",
    "    the cleaned sentence as a string\n",
    "    Parameters: sentence (string)\n",
    "    '''\n",
    "    sentence = remove_hyperlinks(sentence)\n",
    "    doc = nlp(sentence)\n",
    "    cleaned_tokens = []\n",
    "    for token in doc:\n",
    "        if token.is_stop == False and \\\n",
    "            token.pos_ != 'SYM' and \\\n",
    "            token.pos_ != 'PUNCT' and token.pos_ != 'SPACE':\n",
    "            cleaned_tokens.append(token.lemma_.lower().strip())\n",
    "    return ' '.join(cleaned_tokens)\n",
    "\n",
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            label = labels[word_id]\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "    return new_labels\n",
    "\n",
    "def tokenize_and_align_labels(tokenizer, examples):\n",
    "    tokenized_inputs = tokenizer(examples['tokens'], \n",
    "                                 truncation = True, \n",
    "                                 is_split_into_words = True)\n",
    "    all_labels = examples['ner_tags']\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "    tokenized_inputs['labels'] = new_labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "def to_tokens(tokenizer, sentence):\n",
    "    inputs = tokenizer(sentence)\n",
    "    return tokenizer.convert_ids_to_tokens(inputs.input_ids)\n",
    "\n",
    "def load_preprocessing(path = '../data/mtsamples_modified.csv', preprocess = False):\n",
    "    df = pd.read_csv(path)\n",
    "    if preprocess:\n",
    "        df = load_dataset('csv', data_files = {'../data/mtsamples_modified.csv'}, streaming = True)\n",
    "        for i, row in df.iterrows():\n",
    "            df.at[i, 'description']   = preprocessing(row['description'])\n",
    "            df.at[i, 'medical_specialty'] = preprocessing(row['medical_specialty'])\n",
    "            df.at[i, 'sample_name']   = preprocessing(row['sample_name'])\n",
    "            df.at[i, 'transcription'] = preprocessing(row['transcription']) if not pd.isnull(row['transcription']) else np.NaN  \n",
    "            df.at[i, 'keywords']      = preprocessing(row['keywords']) if not pd.isnull(row['keywords']) else np.NaN  \n",
    "    return df\n",
    "\n",
    "def split_data(df):\n",
    "    shuffle = df.sample(frac = 1, random_state = 42)\n",
    "\n",
    "    train_data,  test_data = train_test_split(shuffle,    test_size = 0.30, random_state = 42)\n",
    "    train_data, valid_data = train_test_split(train_data, test_size = 0.15, random_state = 42) \n",
    "\n",
    "    train_data.to_csv('../data/train.csv', index = False)\n",
    "    valid_data.to_csv('../data/valid.csv', index = False)\n",
    "    test_data. to_csv('../data/test.csv' , index = False)\n",
    "\n",
    "    data_files = {\n",
    "        'train': '../data/train.csv',\n",
    "        'valid': '../data/valid.csv',\n",
    "        'test' : '../data/test.csv'}\n",
    "    dataset = load_dataset('csv', data_files = data_files, streaming = True)\n",
    "    return dataset \n",
    "\n",
    "def compute_review_length(example):\n",
    "    return {'review_length': len(example['transcription'].split())}\n",
    "\n",
    "def bert_tokenizer(df, use_special):\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    input_ids, attention_masks = [], []\n",
    "\n",
    "    if use_special:\n",
    "        for index, row in df.iterrows():\n",
    "            encoded_dict = tokenizer.encode_plus(\n",
    "                row['description'],\n",
    "                row['medical_specialty'],\n",
    "                row['sample_name'],\n",
    "                row['transcription'],\n",
    "                row['keywords'],\n",
    "                padding = 'max_length',\n",
    "                truncation = True,\n",
    "                return_attention_mask = True,\n",
    "                return_tensors = 'pt')\n",
    "            input_ids.append(encoded_dict['input_ids'])\n",
    "            attention_masks.append(encoded_dict['attention_mask'])\n",
    "        input_ids = torch.cat(input_ids, dim = 0)\n",
    "        attention_masks = torch.cat(attention_masks, dim = 0)\n",
    "\n",
    "    else:\n",
    "        for description in df['description']:\n",
    "            encoded_dict = tokenizer.encode_plus(\n",
    "                description,\n",
    "                add_special_tokens = True, \n",
    "                max_length = 512, \n",
    "                padding = 'max_length',\n",
    "                truncation = True,\n",
    "                return_attention_mask = True,\n",
    "                return_tensors = 'pt')\n",
    "            input_ids.append(encoded_dict['input_ids'])\n",
    "            attention_masks.append(encoded_dict['attention_mask'])\n",
    "        input_ids = torch.cat(input_ids, dim = 0)\n",
    "        attention_masks = torch.cat(attention_masks, dim = 0)\n",
    "    return input_ids, attention_masks\n",
    "\n",
    "def process(examples):\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"sentence\"], truncation = True, max_length = 512)\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <code>TokenizationProcessor</code> class facilitates tokenization of text data for NLP text classification tasks. Let's break down the code and understand its usage and purpose. This class is initialized with two parameters: <code>max_sequence_length</code> and <code>tokenizer</code>. The <code>max_sequence_length</code> specifies the maximum length of the tokenized sequences, and the tokenizer is an instance of the <code>BertTokenizer</code> class from the Hugging Face's transformers library, initialized with the <code>'bert-base-uncased'</code> model. \n",
    "\n",
    "We created the <code>preprocess</code> function that tokenizes a single example by using the tokenizer on the transcription text. It applies truncation to limit the sequence length to max_sequence_length, adds padding to make all sequences of equal length, and returns the tokenized transcription along with the attention mask and the corresponding medical specialty label. Ultimately, the <code>preprocess</code> function is applied to the streamed dataset using the map function, which tokenizes and preprocesses the data in batches using the function <code>tokenize_and_split</code>. The tokenized dataset is then shuffled using a buffer size of 10,000 and a seed of 42. The tokenized dataset is then processed to obtain the input IDs, attention masks, and token type IDs. The input IDs are padded with zeros to match the length of the longest sequence in the dataset. In pretrained BERT, the max_length is 512. The resulting tensors for input IDs, attention masks, and token type IDs are returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizationProcessor:\n",
    "    def __init__(self, max_sequence_length, tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')):\n",
    "        self.max_sequence_length = max_sequence_length \n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def preprocess(self, example):\n",
    "        tokenized_transcription = self.tokenizer(example['transcription'], \n",
    "                                                truncation = True, \n",
    "                                                max_length = self.max_sequence_length, \n",
    "                                                padding = 'max_length',\n",
    "                                                return_tensors = 'pt')\n",
    "        \n",
    "        return {'input_ids': tokenized_transcription['input_ids'],\n",
    "                'attention_mask': tokenized_transcription['attention_mask'],\n",
    "                'medical_specialty': example['medical_specialty']}\n",
    "\n",
    "    def tokenize_and_split(self, examples):\n",
    "        return self.tokenizer(examples['transcription'],\n",
    "                              truncation = True,\n",
    "                              max_length = self.max_sequence_length,\n",
    "                              return_overflowing_tokens = True)\n",
    "\n",
    "    def process_dataset(self, dataset_path):\n",
    "        dataset = load_dataset('csv', data_files = dataset_path)\n",
    "        dataset_streamed = load_dataset('csv', data_files = dataset_path, streaming = True)\n",
    "\n",
    "        tokenized_dataset = dataset_streamed.map(self.preprocess, batched = True, batch_size = 16)\n",
    "        tokenized_dataset = tokenized_dataset.shuffle(buffer_size = 10_000, seed = 42)\n",
    "\n",
    "        for split in dataset.keys():\n",
    "            assert len(dataset[split]) == len(dataset[split].unique('Unnamed: 0'))\n",
    "\n",
    "        dataset = dataset.rename_column(original_column_name = 'Unnamed: 0', \n",
    "                                        new_column_name = 'patient_id')\n",
    "        \n",
    "        tokenized_dataset = dataset.map(self.tokenize_and_split,\n",
    "                                        batched = True,\n",
    "                                        remove_columns = dataset['train'].column_names)\n",
    "\n",
    "        input_ids = np.array(tokenized_dataset['train']['input_ids'])\n",
    "        sequence_length = max(len(ids) for ids in input_ids)\n",
    "        input_ids = [ids + [0] * (sequence_length - len(ids)) for ids in input_ids]\n",
    "        input_ids = torch.tensor(input_ids)\n",
    "\n",
    "        attention_mask = tokenized_dataset['train']['attention_mask']\n",
    "        attention_mask = [mask + [0] * (sequence_length - len(mask)) for mask in attention_mask]\n",
    "        attention_mask = torch.tensor(attention_mask)\n",
    "\n",
    "        token_type_ids = tokenized_dataset['train']['token_type_ids']\n",
    "        token_type_ids = [mask + [0] * (sequence_length - len(mask)) for mask in token_type_ids]\n",
    "        token_type_ids = torch.tensor(token_type_ids)\n",
    "\n",
    "        return input_ids, attention_mask, token_type_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "class TokenizationProcessor:\n",
    "    def __init__(self, max_sequence_length, tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "\n",
    "    def preprocess(self, example):\n",
    "        max_sequence_length = 256\n",
    "        tokenized_transcription = self.tokenizer(example['transcription'], \n",
    "                                                truncation=True, \n",
    "                                                max_length = self.max_sequence_length, \n",
    "                                                padding = 'max_length',\n",
    "                                                return_tensors = 'pt')\n",
    "        \n",
    "        return {'input_ids': tokenized_transcription['input_ids'],\n",
    "                'attention_mask': tokenized_transcription['attention_mask'],\n",
    "                'medical_specialty': example['medical_specialty']}\n",
    "\n",
    "    def tokenize_and_split(self, examples):\n",
    "        return self.tokenizer(examples['transcription'],\n",
    "                              truncation = True,\n",
    "                              max_length = self.max_sequence_length,\n",
    "                              return_overflowing_tokens = True)\n",
    "\n",
    "    def process_dataset(self, dataset_path):\n",
    "        dataset = load_dataset('csv', data_files = dataset_path)\n",
    "        dataset_streamed = load_dataset('csv', data_files = dataset_path, streaming = True)\n",
    "\n",
    "        tokenized_dataset = dataset_streamed.map(self.preprocess, batched = True, batch_size = 16)\n",
    "        tokenized_dataset = tokenized_dataset.shuffle(buffer_size = 10_000, seed = 42)\n",
    "\n",
    "        for split in dataset.keys():\n",
    "            assert len(dataset[split]) == len(dataset[split].unique('Unnamed: 0'))\n",
    "        dataset = dataset.rename_column(original_column_name = 'Unnamed: 0', new_column_name = 'patient_id')\n",
    "        tokenized_dataset = dataset.map(self.tokenize_and_split,\n",
    "                                        batched = True,\n",
    "                                        remove_columns = dataset['train'].column_names)\n",
    "\n",
    "        input_ids = np.array(tokenized_dataset['train']['input_ids'])\n",
    "        sequence_length = max(len(ids) for ids in input_ids)\n",
    "        input_ids = [ids + [0] * (sequence_length - len(ids)) for ids in input_ids]\n",
    "        input_ids = torch.tensor(input_ids)\n",
    "\n",
    "        attention_mask = tokenized_dataset['train']['attention_mask']\n",
    "        attention_mask = [mask + [0] * (sequence_length - len(mask)) for mask in attention_mask]\n",
    "        attention_mask = torch.tensor(attention_mask)\n",
    "\n",
    "        token_type_ids = tokenized_dataset['train']['token_type_ids']\n",
    "        token_type_ids = [mask + [0] * (sequence_length - len(mask)) for mask in token_type_ids]\n",
    "        token_type_ids = torch.tensor(token_type_ids)\n",
    "\n",
    "        return input_ids, attention_mask, token_type_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetText(df):\n",
    "    text_column = df['transcription'].astype('str')\n",
    "    label_encoder = LabelEncoder()\n",
    "    labels = label_encoder.fit_transform(df['medical_specialty'])\n",
    "    labels = torch.tensor(labels)\n",
    "    num_classes = len(label_encoder.classes_)\n",
    "    return text_column, labels, num_classes\n",
    "\n",
    "def TokenizeDataset(text_column, use_medical_tokenizer = False):\n",
    "    if not use_medical_tokenizer:\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    else:\n",
    "        token_path = '../data/tokenizer.json'\n",
    "        tokenizer = Tokenizer.from_file(token_path)\n",
    "        tokenizer = BertTokenizerFast(tokenizer_object = tokenizer)\n",
    "\n",
    "    encoded_inputs = tokenizer.batch_encode_plus(\n",
    "                        text_column.tolist(),\n",
    "                        max_length = 64,\n",
    "                        padding = 'max_length',\n",
    "                        truncation = True,\n",
    "                        return_attention_mask = True,\n",
    "                        return_tensors = 'pt')\n",
    "\n",
    "    input_ids = encoded_inputs['input_ids']\n",
    "    attention_mask = encoded_inputs['attention_mask']\n",
    "    token_type_ids = encoded_inputs['token_type_ids']\n",
    "    return input_ids, attention_mask, token_type_ids\n",
    "\n",
    "\n",
    "def TokenizeSentenceDataset(text_column, use_medical_tokenizer = False):\n",
    "    if not use_medical_tokenizer:\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    else:\n",
    "        token_path = '../data/tokenizer.json'\n",
    "        tokenizer = Tokenizer.from_file(token_path)\n",
    "        tokenizer = BertTokenizerFast(tokenizer_object=tokenizer)\n",
    "\n",
    "    sentences = [sent_tokenize(text) for text in text_column]\n",
    "    sentences = [sentence for sublist in sentences for sentence in sublist]\n",
    "\n",
    "    encoded_inputs = tokenizer.batch_encode_plus(\n",
    "        sentences,\n",
    "        max_length = 512,\n",
    "        padding = 'max_length',\n",
    "        truncation = True,\n",
    "        return_attention_mask = True,\n",
    "        return_tensors = 'pt')\n",
    "\n",
    "    input_ids = encoded_inputs['input_ids']\n",
    "    attention_mask = encoded_inputs['attention_mask']\n",
    "    token_type_ids = encoded_inputs['token_type_ids']\n",
    "    return input_ids, attention_mask, token_type_ids\n",
    "\n",
    "\n",
    "df = load_preprocessing(preprocess = False)\n",
    "text_column, labels, num_classes = GetText(df)\n",
    "input_ids, attention_mask, token_type_ids = TokenizeDataset(text_column, \n",
    "                                                            use_medical_tokenizer = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below defines a function <code>GENERATE_DATALOADER</code> that generates data loaders for a given set of input IDs, attention masks, and labels from the <code>TokenizationProcessor</code> class. The data loaders are used to efficiently load and process data in batches during training, validation, and testing phases. Particularly, the <code>GENERATE_DATALOADER</code> class takes input IDs, attention masks, labels, and the optional parameters (batch size and use_sampler with default values of 64 and True, respectively).\n",
    "\n",
    "If <code>use_sampler</code> is True, the code performs random oversampling to address class imbalance in the dataset. Othersie,  the code proceeds with the original dataset without oversampling. Here, we utilized the RandomOverSampler from the imbalanced-learn library to balance the number of samples for each class. The input IDs and attention masks are then concatenated, and oversampling is applied to both the concatenated features (X) and the labels (y). The oversampled dataset is then randomly split into training, validation, and testing using <code>random_split</code> from PyTorch. The proportions for the split are set to 60% for training, 20% for validation, and the remaining portion for testing. The resulting data loaders, including the training, validation, and testing data loaders, are returned by the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GENERATE_DATALOADER(input_ids, attention_mask, labels, batch_size = 64, use_sampler = True):\n",
    "    if use_sampler:\n",
    "        oversampler = RandomOverSampler(random_state = 42)\n",
    "        X = np.concatenate((input_ids, attention_mask), axis = -1)\n",
    "        y = np.ravel(labels)\n",
    "\n",
    "        X_resampled, y_resampled = oversampler.fit_resample(X, y)\n",
    "\n",
    "        input_ids_resampled      = X_resampled[:, :input_ids.shape[1]]\n",
    "        attention_mask_resampled = X_resampled[:, input_ids.shape[1]:]\n",
    "        labels_resampled = y_resampled\n",
    "\n",
    "        dataset = TensorDataset(torch.tensor(input_ids_resampled),\n",
    "                                torch.tensor(attention_mask_resampled),\n",
    "                                torch.tensor(labels_resampled))\n",
    "        \n",
    "        train_size = int(0.6 * len(dataset))\n",
    "        valid_size = int(0.2 * len(dataset))\n",
    "        tests_size = len(dataset) - train_size - valid_size\n",
    "        train_dataset, valid_dataset, test_dataset = random_split(dataset, [train_size, valid_size, tests_size])\n",
    "        \n",
    "    else:\n",
    "        dataset = TensorDataset(torch.tensor(input_ids), \n",
    "                                torch.tensor(attention_mask), \n",
    "                                torch.tensor(labels))\n",
    "        \n",
    "        train_size = int(0.6 * len(dataset))\n",
    "        valid_size = int(0.2 * len(dataset))\n",
    "        tests_size = len(dataset) - train_size - valid_size\n",
    "\n",
    "        train_dataset, valid_dataset, test_dataset = random_split(dataset, [train_size, valid_size, tests_size])\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        sampler = RandomSampler(train_dataset),\n",
    "        batch_size = batch_size)\n",
    "    validation_dataloader = DataLoader(\n",
    "        valid_dataset,\n",
    "        sampler = SequentialSampler(valid_dataset),\n",
    "        batch_size = batch_size)\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset,\n",
    "        sampler = SequentialSampler(test_dataset),\n",
    "        batch_size = batch_size)\n",
    "    return train_dataloader, validation_dataloader, test_dataloader\n",
    "\n",
    "class NLPDATASET(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        sequence = self.sequences[index]\n",
    "        label = self.labels[index]\n",
    "        return sequence, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        _, (hidden, _) = self.lstm(inputs)\n",
    "        hidden = hidden.squeeze(0)  \n",
    "        output = self.fc(hidden)\n",
    "        return output\n",
    "    \n",
    "class BasicClassifier(nn.Module):\n",
    "    def __init__(self, in_features, hidden_size, out_features):\n",
    "        super(BasicClassifier, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(in_features, hidden_size)\n",
    "        self.fc2 = torch.nn.Linear(hidden_size, 32)\n",
    "        self.fc3 = torch.nn.Linear(32, out_features)\n",
    "                \n",
    "    def forward(self, inputs):\n",
    "        x = F.relu(self.fc1(inputs.squeeze(1)))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        logits = self.fc3(x)\n",
    "        probs = F.relu(logits)\n",
    "        return probs\n",
    "\n",
    "def BERT_EMBEDDING(input_ids, attention_mask, token_type_ids):\n",
    "    bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "    bert_model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(input_ids = input_ids, \n",
    "                             attention_mask = attention_mask, \n",
    "                             token_type_ids = token_type_ids)\n",
    "        bert_embeddings = outputs.last_hidden_state\n",
    "\n",
    "    batch_size = bert_embeddings.size(0)\n",
    "    sequence_length = bert_embeddings.size(1)\n",
    "    bert_embeddings = bert_embeddings.view(batch_size, sequence_length, -1)\n",
    "    embeddings  = bert_embeddings.permute(1, 0, 2)\n",
    "    return bert_model, embeddings\n",
    "\n",
    "def ROBERTA_EMBEDDING(input_ids, attention_mask, token_type_ids):\n",
    "    model_name = 'roberta-base'\n",
    "    model = RobertaModel.from_pretrained(model_name)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids = input_ids,\n",
    "                        attention_mask = attention_mask,\n",
    "                        token_type_ids = token_type_ids)\n",
    "        embeddings = outputs.last_hidden_state\n",
    "\n",
    "    batch_size = embeddings.size(0)\n",
    "    sequence_length = embeddings.size(1)\n",
    "    embeddings = embeddings.view(batch_size, sequence_length, -1)\n",
    "    embeddings = embeddings.permute(1, 0, 2)\n",
    "    return model, embeddings\n",
    "\n",
    "def LSTM_BASELINES(bert_model, embeddings, basic_classifier = False):\n",
    "    input_size = bert_model.config.hidden_size\n",
    "    hidden_size, num_classes = 50, 9\n",
    "    if basic_classifier:\n",
    "        lstm_model   = BasicClassifier(input_size, hidden_size, num_classes)\n",
    "    else:\n",
    "        lstm_model   = LSTMClassifier(input_size, hidden_size, num_classes)\n",
    "    lstm_output  = lstm_model(embeddings)\n",
    "    output_probs = nn.functional.softmax(lstm_output, dim = 0)\n",
    "    _, predicted_labels = torch.max(output_probs, dim = 0)\n",
    "    return output_probs, predicted_labels\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "def get_accuracy(preds, y):\n",
    "    batch_corr = (preds == y).sum()\n",
    "    acc = batch_corr / len(y)\n",
    "    return acc\n",
    "\n",
    "def evaluate_predictions(predictions, labels):\n",
    "    predicted_labels = torch.argmax(predictions, dim = 1)\n",
    "    true_labels = labels.numpy()\n",
    "\n",
    "    accuracy  = accuracy_score(true_labels,  predicted_labels)\n",
    "    precision = precision_score(true_labels, predicted_labels, average = 'weighted')\n",
    "    recall = recall_score(true_labels, predicted_labels, average = 'weighted')\n",
    "    f1 = f1_score(true_labels, predicted_labels, average = 'weighted')\n",
    "    return {\n",
    "        'Accuracy':     np.round(accuracy, 4),\n",
    "        'Precision':    np.round(precision, 4),\n",
    "        'Recall':       np.round(recall, 4),\n",
    "        'F1-score':     np.round(f1, 4)}\n",
    "\n",
    "def eval_predictions(predicted_labels, labels):\n",
    "    true_labels = labels.numpy()\n",
    "    accuracy  = accuracy_score(true_labels,  predicted_labels)\n",
    "    precision = precision_score(true_labels, predicted_labels, average = 'weighted')\n",
    "    recall = recall_score(true_labels, predicted_labels, average = 'weighted')\n",
    "    f1 = f1_score(true_labels, predicted_labels, average = 'weighted')\n",
    "\n",
    "    print('Classification Metrics: ')\n",
    "    print(f'\\t Accuracy:  \\t {np.round(accuracy, 5)}')\n",
    "    print(f'\\t Precision: \\t {np.round(precision, 5)}')\n",
    "    print(f'\\t Recall:    \\t {np.round(recall, 5)}')\n",
    "    print(f'\\t F1-score:  \\t {np.round(f1, 5)}')\n",
    "\n",
    "def count_parameters(model):\n",
    "    params = [p.numel() for p in model.parameters() if p.requires_grad]\n",
    "    for item in params:\n",
    "        print(f'{item:>6}')\n",
    "    print(f'______\\n{sum(params):>6}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(preds, y):\n",
    "    batch_corr = (preds == y).sum()\n",
    "    acc = batch_corr / len(y)\n",
    "    return acc\n",
    "\n",
    "def plot_metrics(train_losses, valid_losses, train_accurs, valid_accurs):\n",
    "    alpha = 0.3\n",
    "    smoothed_train_losses = [train_losses[0]]\n",
    "    smoothed_valid_losses = [valid_losses[0]]\n",
    "    smoothed_train_accurs = [train_accurs[0]]\n",
    "    smoothed_valid_accurs = [valid_accurs[0]]\n",
    "    \n",
    "    for i in range(1, len(train_losses)):\n",
    "        smoothed_train_losses.append(alpha * train_losses[i] + (1-alpha) * smoothed_train_losses[-1])\n",
    "        smoothed_valid_losses.append(alpha * valid_losses[i] + (1-alpha) * smoothed_valid_losses[-1])\n",
    "        smoothed_train_accurs.append(alpha * train_accurs[i] + (1-alpha) * smoothed_train_accurs[-1])\n",
    "        smoothed_valid_accurs.append(alpha * valid_accurs[i] + (1-alpha) * smoothed_valid_accurs[-1])\n",
    "    \n",
    "    smoothed_train_losses = train_losses\n",
    "    smoothed_train_accurs = train_accurs\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (12, 5))\n",
    "    ax1.plot(smoothed_train_losses, label = 'Train')\n",
    "    ax1.plot(smoothed_valid_losses, label = 'Valid')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title('Losses')\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2.plot(smoothed_train_accurs, label='Train')\n",
    "    ax2.plot(smoothed_valid_accurs, label='Valid')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.set_title('Accuracies')\n",
    "    ax2.legend()\n",
    "    plt.show()\n",
    "\n",
    "def _train(model, loader, optimizer, criterion, batch_size = 16, device = 'cpu'):\n",
    "    epoch_train_loss = 0\n",
    "    epoch_train_accu = 0\n",
    "    model.train()\n",
    "    epoch_train_prediction = []\n",
    "\n",
    "    for idx, data in enumerate(loader):\n",
    "        inputs, attens, labels = data\n",
    "        inputs, attens, labels = inputs.to(device), attens.to(device), labels.to(device, dtype = torch.long)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(input_ids = inputs, attention_mask = attens)\n",
    "        embedds = outputs.last_hidden_state\n",
    "\n",
    "        batch_size, seq_length = embedds.size(0), embedds.size(1)\n",
    "        embeddings = embedds.view(batch_size, seq_length, -1)\n",
    "        embeddings = embeddings.permute(1, 0, 2)\n",
    "\n",
    "        input_size = model.config.hidden_size\n",
    "        hidden_size, num_classes = 50, 9\n",
    "\n",
    "        lstm_model = LSTMClassifier(input_size, hidden_size, num_classes)\n",
    "        lstm_output  = lstm_model(embeddings)\n",
    "        loss = criterion(lstm_output, labels)\n",
    "\n",
    "        output_probs = nn.functional.softmax(lstm_output, dim = 0)    \n",
    "        _, predicted_labels = torch.max(output_probs, dim = 1) \n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "          \n",
    "        epoch_train_prediction.append(predicted_labels)\n",
    "        accuracy = get_accuracy(predicted_labels, labels)\n",
    "        epoch_train_loss += loss.item()\n",
    "        epoch_train_accu += accuracy.item()\n",
    "    epoch_train_loss = epoch_train_loss / len(loader)\n",
    "    epoch_train_accu = epoch_train_accu / len(loader)\n",
    "    return epoch_train_loss, epoch_train_accu, epoch_train_prediction\n",
    "    \n",
    "def _evals(model, loader, criterion, batch_size = 64, device = 'cpu', display = False):\n",
    "    epoch_valid_loss = 0\n",
    "    epoch_valid_accu = 0\n",
    "    model.eval()\n",
    "    epoch_valid_prediction = []\n",
    "    with torch.no_grad():\n",
    "        for idx, data in enumerate(loader):\n",
    "            inputs, attens, labels = data \n",
    "            inputs, attens, labels = inputs.to(device), attens.to(device), labels.to(device,  dtype = torch.long)\n",
    "\n",
    "            outputs = model(input_ids = inputs, attention_mask = attens)\n",
    "            embedds = outputs.last_hidden_state\n",
    "\n",
    "            batch_size, seq_length = embedds.size(0), embedds.size(1)\n",
    "            embeddings = embedds.view(batch_size, seq_length, -1)\n",
    "            embeddings = embeddings.permute(1, 0, 2)  \n",
    "\n",
    "            \n",
    "            input_size = model.config.hidden_size\n",
    "            hidden_size, num_classes = 256, 9\n",
    "\n",
    "            lstm_model = LSTMClassifier(input_size, hidden_size, num_classes)\n",
    "            lstm_output  = lstm_model(embeddings)\n",
    "            loss = criterion(lstm_output, labels)\n",
    "            loss = np.round(loss.item() * 0.10, 3)\n",
    "\n",
    "            output_probs = nn.functional.softmax(lstm_output, dim = 0)    \n",
    "            _, predicted_labels = torch.max(output_probs, dim = 1)   \n",
    "            epoch_valid_prediction.append(predicted_labels)\n",
    "            accuracy = np.round(get_accuracy(predicted_labels, labels), 5)\n",
    "            epoch_valid_loss += loss.item()\n",
    "            epoch_valid_accu += accuracy.item()\n",
    "    epoch_valid_loss = epoch_valid_loss / len(loader)\n",
    "    epoch_valid_accu = epoch_valid_accu / len(loader)\n",
    "    if display:\n",
    "        print(f'Loss: {loss} \\t Accuracy: {accuracy}')\n",
    "    return epoch_valid_loss, epoch_valid_accu, epoch_valid_prediction\n",
    "\n",
    "def train(num_epochs, model, train_loader, valid_loader, test_loader, optimizer, criterion, device, accuracy = True):\n",
    "    best_valid_loss = float('inf')\n",
    "    train_losses, valid_losses = [], []\n",
    "    train_accurs, valid_accurs = [], []\n",
    "    trainpredict, testspredict = [], []\n",
    "\n",
    "    epoch_times = []\n",
    "    list_best_epochs = []\n",
    "    start = time()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        train_loss, train_accu, tr_predict = _train(model, train_loader, optimizer, criterion, device)\n",
    "        valid_loss, valid_accu, ts_predict = _evals(model, valid_loader, criterion, device)\n",
    "        \n",
    "        if accuracy:\n",
    "            print(f'Epoch: {epoch + 1} \\t Training: Loss {np.round(train_loss, 5)}   \\t Accuracy: {np.round(train_accu, 5)} \\t Validation Loss  {np.round(valid_loss, 5)} \\t Accuracy: {np.round(valid_accu, 5)}')\n",
    "        else:\n",
    "            print(f'Epoch: {epoch + 1} \\t Training: Loss {np.round(train_loss, 5)} \\t Validation Loss  {np.round(valid_loss, 5)}')\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        train_accurs.append(train_accu)\n",
    "        valid_losses.append(valid_loss)\n",
    "        valid_accurs.append(valid_accu)\n",
    "        trainpredict.append(tr_predict)\n",
    "        testspredict.append(ts_predict)\n",
    "\n",
    "        end_time = time()\n",
    "        epoch_mins, epoch_secs = epoch_time(start, end_time)\n",
    "        \n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            best_model = copy.deepcopy(model)\n",
    "            best_epoch = epoch\n",
    "        list_best_epochs.append(best_epoch)\n",
    "    test_loss, test_accu, test_predict  = _evals(best_model, test_loader, criterion, device)\n",
    "    print(f'Training time: {np.round(time() - start, 4)} seconds')\n",
    "    print(f'Final Best Model from Best Epoch {best_epoch + 1} Test Loss = {test_loss}, Test Accuracy = {test_accu}')\n",
    "    return train_losses, valid_losses, train_accurs, valid_accurs, test_loss, test_accu, best_epoch, epoch_times, test_predict, best_model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <code>Question 3 - 5</code>. Create the baseline model with BERT and LSTM \n",
    "Specific instructions:\n",
    "- Use BERT without fine tuning on the LSTM with 50 neurons for the baseline model. Other hyperparameters are not controlled. \n",
    "- Report the baseline results with appropriate metrics obtained from the baseline model. \n",
    "- Criticize the results from the baseline model based on theoretical and practical perspective with supporting literature.   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve Question 3-5, we first define a function called <code>main</code> that summarizes all of our training and inferences to this medical specialty classification task. This function takes input parameters <code>df, bert_model, train_process</code>. The <code>df</code> is acquired from the function <code>load_processing</code> which then takes as an input to <code>main</code> to extract data from the provided dataframe. The function returns three values but we only consider the second value <code>labels</code>. It then creates the <code>TokenizationProcessor</code> object with a maximum sequence length of 128 and a tokenizer initialized with the 'bert-base-uncased' pretrained model. This object will tokenize the modified dataset using the aforementioned parameters. Then it calls the <code>GENERATE_DATALOADER</code> function, passing the input_ids, attention_mask, and labels tensors, and specifying use_sampler = True. This function generates three data loaders: train_loader, valid_loader, and test_loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(df, bert_model, train_process = False):\n",
    "    _, labels, _ = GetText(df)\n",
    "    start = time()\n",
    "    processor = TokenizationProcessor(max_sequence_length = 128, tokenizer = BertModel.from_pretrained('bert-base-uncased'))\n",
    "    input_ids, attention_mask, token_type_ids = processor.process_dataset('../data/mtsamples_modified.csv')\n",
    "    if input_ids.size(0) != labels.size(0):    \n",
    "        desired_length = input_ids.size(0)\n",
    "        padding_length = desired_length - len(labels)\n",
    "        padding = torch.zeros(padding_length, dtype = torch.long)\n",
    "        labels  = torch.cat((labels, padding))\n",
    "\n",
    "    train_loader, valid_loader, test_loader = GENERATE_DATALOADER(input_ids, attention_mask, labels, use_sampler = True)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    if train_process:\n",
    "        num_epochs = 2\n",
    "        optimizer  = torch.optim.Adam(bert_model.parameters(), lr = 0.001)\n",
    "        train_losses, valid_losses, train_accurs, valid_accurs, test_loss, test_accu, best_epoch, epoch_times, test_predict, best_model = train(num_epochs, bert_model, \n",
    "                                                                                                                                                train_loader, valid_loader, \n",
    "                                                                                                                                                test_loader, optimizer, criterion, \n",
    "                                                                                                                                                device, accuracy = True)\n",
    "        return train_losses, valid_losses, train_accurs, valid_accurs, test_loss, test_accu, best_epoch, epoch_times, test_predict, best_model\n",
    "    else:\n",
    "        epoch_valid_loss, epoch_valid_accu, epoch_valid_prediction = _evals(bert_model, \n",
    "                                                                            test_loader, \n",
    "                                                                            criterion, \n",
    "                                                                            batch_size = 32, \n",
    "                                                                            device = 'cpu', \n",
    "                                                                            display = False)\n",
    "        for idx, data in enumerate(test_loader):\n",
    "            _, _, labels = data \n",
    "            break\n",
    "        eval_predictions(epoch_valid_prediction[0], labels)\n",
    "    print(f'Fit and predict time: {np.round(time() - start, 4)} seconds')\n",
    "    return epoch_valid_loss, epoch_valid_accu, epoch_valid_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "WARNING:datasets.builder:Found cached dataset csv (C:/Users/Renan/.cache/huggingface/datasets/csv/default-b7921370535cb5b2/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 16.33it/s]\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at C:\\Users\\Renan\\.cache\\huggingface\\datasets\\csv\\default-b7921370535cb5b2\\0.0.0\\6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-bb432f5c21b2e9d1.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Metrics: \n",
      "\t Accuracy:  \t 0.7031\n",
      "\t Precision: \t 0.73228\n",
      "\t Recall:    \t 0.56248\n",
      "\t F1-score:  \t 0.59376\n",
      "Fit and predict time: 5720.4624 seconds\n"
     ]
    }
   ],
   "source": [
    "df = load_preprocessing(preprocess = False)\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "epoch_valid_loss, epoch_valid_accu, epoch_valid_prediction = main(df, bert_model, train_process = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "WARNING:datasets.builder:Found cached dataset csv (C:/Users/Renan/.cache/huggingface/datasets/csv/default-6d6e490111dd0d23/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 190.06it/s]\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at C:\\Users\\Renan\\.cache\\huggingface\\datasets\\csv\\default-6d6e490111dd0d23\\0.0.0\\6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-edb2f1d343437643.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Metrics: \n",
      "\t Accuracy:  \t 0.75\n",
      "\t Precision: \t 0.76667\n",
      "\t Recall:    \t 0.75\n",
      "\t F1-score:  \t 0.66073\n",
      "Fit and predict time: 391.2881 seconds\n"
     ]
    }
   ],
   "source": [
    "df = load_preprocessing(preprocess = False)\n",
    "bert_model = RobertaModel.from_pretrained('roberta-base')\n",
    "epoch_valid_loss, epoch_valid_accu, epoch_valid_prediction = main(df, bert_model, train_process = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "WARNING:datasets.builder:Found cached dataset csv (C:/Users/Renan/.cache/huggingface/datasets/csv/default-6d6e490111dd0d23/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 496.07it/s]\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at C:\\Users\\Renan\\.cache\\huggingface\\datasets\\csv\\default-6d6e490111dd0d23\\0.0.0\\6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-edb2f1d343437643.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Metrics: \n",
      "\t Accuracy:  \t 0.84375\n",
      "\t Precision: \t 0.74519\n",
      "\t Recall:    \t 0.84375\n",
      "\t F1-score:  \t 0.73438\n",
      "Fit and predict time: 190.5946 seconds\n"
     ]
    }
   ],
   "source": [
    "df = load_preprocessing(preprocess = False)\n",
    "bert_model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "epoch_valid_loss, epoch_valid_accu, epoch_valid_prediction = main(df, bert_model, train_process = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "WARNING:datasets.builder:Found cached dataset csv (C:/Users/Renan/.cache/huggingface/datasets/csv/default-6d6e490111dd0d23/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 331.91it/s]\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at C:\\Users\\Renan\\.cache\\huggingface\\datasets\\csv\\default-6d6e490111dd0d23\\0.0.0\\6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-edb2f1d343437643.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Metrics: \n",
      "\t Accuracy:  \t 0.23438\n",
      "\t Precision: \t 0.12773\n",
      "\t Recall:    \t 0.23438\n",
      "\t F1-score:  \t 0.16211\n",
      "Fit and predict time: 467.0552 seconds\n"
     ]
    }
   ],
   "source": [
    "df = load_preprocessing(preprocess = False)\n",
    "bert_model = RobertaModel.from_pretrained('roberta-base')\n",
    "epoch_valid_loss, epoch_valid_accu, epoch_valid_prediction = main(df, bert_model, train_process = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "WARNING:datasets.builder:Found cached dataset csv (C:/Users/Renan/.cache/huggingface/datasets/csv/default-6d6e490111dd0d23/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 253.45it/s]\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at C:\\Users\\Renan\\.cache\\huggingface\\datasets\\csv\\default-6d6e490111dd0d23\\0.0.0\\6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-edb2f1d343437643.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Metrics: \n",
      "\t Accuracy:  \t 0.85938\n",
      "\t Precision: \t 0.44868\n",
      "\t Recall:    \t 0.85938\n",
      "\t F1-score:  \t 0.5609\n",
      "Fit and predict time: 231.6236 seconds\n"
     ]
    }
   ],
   "source": [
    "df = load_preprocessing(preprocess = False)\n",
    "bert_model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "epoch_valid_loss, epoch_valid_accu, epoch_valid_prediction = main(df, bert_model, train_process = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bionlp/bluebert_pubmed_uncased_L-12_H-768_A-12 were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "WARNING:datasets.builder:Found cached dataset csv (C:/Users/Renan/.cache/huggingface/datasets/csv/default-6d6e490111dd0d23/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 124.39it/s]\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at C:\\Users\\Renan\\.cache\\huggingface\\datasets\\csv\\default-6d6e490111dd0d23\\0.0.0\\6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-edb2f1d343437643.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Metrics: \n",
      "\t Accuracy:  \t 0.78125\n",
      "\t Precision: \t 0.59286\n",
      "\t Recall:    \t 0.78125\n",
      "\t F1-score:  \t 0.7107\n",
      "Fit and predict time: 444.3507 seconds\n"
     ]
    }
   ],
   "source": [
    "df = load_preprocessing(preprocess = False)\n",
    "bert_model = BertModel.from_pretrained('bionlp/bluebert_pubmed_uncased_L-12_H-768_A-12')\n",
    "epoch_valid_loss, epoch_valid_accu, epoch_valid_prediction = main(df, bert_model, train_process = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bionlp/bluebert_pubmed_uncased_L-12_H-768_A-12 were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "WARNING:datasets.builder:Found cached dataset csv (C:/Users/Renan/.cache/huggingface/datasets/csv/default-6d6e490111dd0d23/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 198.19it/s]\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at C:\\Users\\Renan\\.cache\\huggingface\\datasets\\csv\\default-6d6e490111dd0d23\\0.0.0\\6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-edb2f1d343437643.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Metrics: \n",
      "\t Accuracy:  \t 0.625\n",
      "\t Precision: \t 0.63393\n",
      "\t Recall:    \t 0.625\n",
      "\t F1-score:  \t 0.6716\n",
      "Fit and predict time: 436.1029 seconds\n"
     ]
    }
   ],
   "source": [
    "df = load_preprocessing(preprocess = False)\n",
    "bert_model = BertModel.from_pretrained('bionlp/bluebert_pubmed_uncased_L-12_H-768_A-12')\n",
    "epoch_valid_loss, epoch_valid_accu, epoch_valid_prediction = main(df, bert_model, train_process = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "WARNING:datasets.builder:Found cached dataset csv (C:/Users/Renan/.cache/huggingface/datasets/csv/default-6d6e490111dd0d23/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 251.94it/s]\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at C:\\Users\\Renan\\.cache\\huggingface\\datasets\\csv\\default-6d6e490111dd0d23\\0.0.0\\6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-edb2f1d343437643.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Metrics: \n",
      "\t Accuracy:  \t 0.46875\n",
      "\t Precision: \t 0.35506\n",
      "\t Recall:    \t 0.46875\n",
      "\t F1-score:  \t 0.43313\n",
      "Fit and predict time: 422.9488 seconds\n"
     ]
    }
   ],
   "source": [
    "# 256\n",
    "df = load_preprocessing(preprocess = False)\n",
    "bert_model = RobertaModel.from_pretrained('roberta-base')\n",
    "epoch_valid_loss, epoch_valid_accu, epoch_valid_prediction = main(df, bert_model, train_process = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "WARNING:datasets.builder:Found cached dataset csv (C:/Users/Renan/.cache/huggingface/datasets/csv/default-6d6e490111dd0d23/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 329.02it/s]\n"
     ]
    }
   ],
   "source": [
    "df = load_preprocessing(preprocess = False)\n",
    "bert_model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "epoch_valid_loss, epoch_valid_accu, epoch_valid_prediction = main(df, bert_model, train_process = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "WARNING:datasets.builder:Found cached dataset csv (C:/Users/Renan/.cache/huggingface/datasets/csv/default-6e058dda65a7852a/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 199.99it/s]\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at C:\\Users\\Renan\\.cache\\huggingface\\datasets\\csv\\default-6e058dda65a7852a\\0.0.0\\6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-8281c6f57cd8910a.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Metrics: \n",
      "\t Accuracy:  \t 0.5469\n",
      "\t Precision: \t 0.6808000000000001\n",
      "\t Recall:    \t 0.5469\n",
      "\t F1-score:  \t 0.5883499999999999\n",
      "Fit and predict time: 11349.2461 seconds\n"
     ]
    }
   ],
   "source": [
    "df = load_preprocessing(preprocess = False)\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "epoch_valid_loss, epoch_valid_accu, epoch_valid_prediction = main(df, bert_model, train_process = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "WARNING:datasets.builder:Found cached dataset csv (C:/Users/Renan/.cache/huggingface/datasets/csv/default-6e058dda65a7852a/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 247.82it/s]\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at C:\\Users\\Renan\\.cache\\huggingface\\datasets\\csv\\default-6e058dda65a7852a\\0.0.0\\6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-8281c6f57cd8910a.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Metrics: \n",
      "\t Accuracy:  \t 0.0469\n",
      "\t Precision: \t 0.0592\n",
      "\t Recall:    \t 0.0469\n",
      "\t F1-score:  \t 0.0496\n",
      "Fit and predict time: 5327.9902 seconds\n"
     ]
    }
   ],
   "source": [
    "df = load_preprocessing(preprocess = False)\n",
    "bert_model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "epoch_valid_loss, epoch_valid_accu, epoch_valid_prediction = main(df, bert_model, train_process = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_preprocessing(preprocess = False)\n",
    "bert_model = AutoModelForTokenClassification.from_pretrained('dmis-lab/biobert-v1.1')\n",
    "epoch_valid_loss, epoch_valid_accu, epoch_valid_prediction = main(df, bert_model, train_process = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.219 \t Accuracy: 0.3333300054073334\n",
      "Classification Metrics: \n",
      "\t Accuracy:  \t 0.1094\n",
      "\t Precision: \t 0.127\n",
      "\t Recall:    \t 0.1094\n",
      "\t F1-score:  \t 0.1127\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "bert_model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "epoch_valid_loss, epoch_valid_accu, epoch_valid_prediction = _evals(bert_model, train_loader, criterion, batch_size = 32, device = 'cpu', display = False)\n",
    "\n",
    "for idx, data in enumerate(train_loader):\n",
    "    inputs, attens, labels = data \n",
    "    break\n",
    "eval_predictions(epoch_valid_prediction[0], labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Metrics: \n",
      "\t Accuracy:  \t 0.1406\n",
      "\t Precision: \t 0.314\n",
      "\t Recall:    \t 0.1406\n",
      "\t F1-score:  \t 0.1511\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "bert_model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "epoch_valid_loss, epoch_valid_accu, epoch_valid_prediction = _evals(bert_model, train_loader, criterion, batch_size = 32, device = 'cpu', display = False)\n",
    "\n",
    "for idx, data in enumerate(train_loader):\n",
    "    inputs, attens, truths = data \n",
    "    break\n",
    "eval_predictions(epoch_valid_prediction[0], truths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset csv (C:/Users/Renan/.cache/huggingface/datasets/csv/default-6e058dda65a7852a/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 199.83it/s]\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at C:\\Users\\Renan\\.cache\\huggingface\\datasets\\csv\\default-6e058dda65a7852a\\0.0.0\\6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-4deee30912ae3225.arrow\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit and predict time: 1423.1821 seconds\n",
      "Classification Metrics: \n",
      "\t Accuracy:  \t 0.125\n",
      "\t Precision: \t 0.1192\n",
      "\t Recall:    \t 0.125\n",
      "\t F1-score:  \t 0.1195\n"
     ]
    }
   ],
   "source": [
    "df = load_preprocessing(preprocess = False)\n",
    "text_column, labels, num_classes = GetText(df)\n",
    "\n",
    "start = time()\n",
    "processor = TokenizationProcessor(max_sequence_length = 128)\n",
    "input_ids, attention_mask, token_type_ids = processor.process_dataset('../data/mtsamples_modified.csv')\n",
    "train_loader, valid_loader, test_loader = GENERATE_DATALOADER(input_ids, attention_mask, labels, use_sampler = True)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "bert_model = RobertaModel.from_pretrained('roberta-base')\n",
    "epoch_valid_loss, epoch_valid_accu, epoch_valid_prediction = _evals(bert_model, train_loader, criterion, batch_size = 32, device = 'cpu', display = False)\n",
    "print(f'Fit and predict time: {np.round(time() - start, 4)} seconds')\n",
    "for idx, data in enumerate(train_loader):\n",
    "    inputs, attens, labels = data \n",
    "    break\n",
    "eval_predictions(epoch_valid_prediction[0], labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset csv (C:/Users/Renan/.cache/huggingface/datasets/csv/default-6e058dda65a7852a/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 38.41it/s]\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at C:\\Users\\Renan\\.cache\\huggingface\\datasets\\csv\\default-6e058dda65a7852a\\0.0.0\\6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-6d21b3da66e469de.arrow\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \t Training: Loss 2.21786   \t Accuracy: 0.10886 \t Validation Loss  0.22245 \t Accuracy: 0.11806\n",
      "Epoch: 2 \t Training: Loss 2.22267   \t Accuracy: 0.10977 \t Validation Loss  0.22231 \t Accuracy: 0.12753\n",
      "Epoch: 3 \t Training: Loss 2.21839   \t Accuracy: 0.10979 \t Validation Loss  0.2219 \t Accuracy: 0.11789\n",
      "Epoch: 4 \t Training: Loss 2.23015   \t Accuracy: 0.11333 \t Validation Loss  0.22176 \t Accuracy: 0.12259\n",
      "Epoch: 5 \t Training: Loss 2.22338   \t Accuracy: 0.11081 \t Validation Loss  0.22307 \t Accuracy: 0.10745\n",
      "Training time: 16727.862 seconds\n",
      "Final Best Model from Best Epoch 4 Test Loss = 0.22244827586206897, Test Accuracy = 0.11000551694426043\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(101)\n",
    "df = load_preprocessing(preprocess = False)\n",
    "text_column, labels, num_classes = GetText(df)\n",
    "\n",
    "processor = TokenizationProcessor(max_sequence_length = 128)\n",
    "input_ids, attention_mask, token_type_ids = processor.process_dataset('../data/mtsamples_modified.csv')\n",
    "train_loader, valid_loader, test_loader = GENERATE_DATALOADER(input_ids, attention_mask, labels, use_sampler = True)\n",
    "\n",
    "bert_model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "num_epochs = 5\n",
    "criterion  = nn.CrossEntropyLoss()\n",
    "optimizer  = torch.optim.Adam(bert_model.parameters(), lr = 0.001)\n",
    "train_losses, valid_losses, train_accurs, valid_accurs, test_loss, test_accu, best_epoch, epoch_times, test_predict, best_model = train(num_epochs, bert_model, train_loader, valid_loader, test_loader, \n",
    "                                                                                                                                        optimizer, criterion, device, accuracy = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Metrics: \n",
      "\t Accuracy:  \t 0.2031\n",
      "\t Precision: \t 0.2449\n",
      "\t Recall:    \t 0.2031\n",
      "\t F1-score:  \t 0.1901\n"
     ]
    }
   ],
   "source": [
    "for idx, data in enumerate(valid_loader):\n",
    "    _, _, labels = data \n",
    "    break\n",
    "eval_predictions(test_predict[0], labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vocabulary used in the field of medicine is extremely specialized, unique, and frequently made up of complicated jargon that is only applicable to that field. Therefore, using a specialized tokenizer created especially for medical texts can have a number of advantages in terms of precision, comprehension, and context preservation.\n",
    "\n",
    "First, many medical phrases contain compound words, acronyms, abbreviations, and unique symbols that may not be properly handled by general tokenizers. We can construct particular rules and heuristics to successfully handle such scenarios by building our own tokenizer. For instance, we can provide rules to decompose compound phrases into meaningful sub-tokens, manage abbreviations as separate tokens, and maintain unique symbols that represent crucial information, like the \"+\" or \"-\" marks in measurements used in medicine. Better tokenization of medical terminology is made possible by this level of personalization, avoiding the loss of important information during preprocessing.\n",
    "\n",
    "Secondly, a dedicated medical tokenizer can incorporate domain knowledge and contextual understanding. Medical texts often contain jargon, anatomical terms, drug names, and other domain-specific concepts. By creating a tokenizer that understands this domain knowledge, we can ensure that these terms are correctly identified and tokenized. This enables downstream natural language processing tasks, such as information extraction or named entity recognition, to extract accurate and meaningful information from medical texts. Additionally, a custom tokenizer can handle specific linguistic patterns prevalent in medical literature, such as Latin or Greek roots, suffixes, and prefixes, which are essential for understanding medical terms.\n",
    "\n",
    "Furthermore, medical texts frequently involve sensitive patient information, protected health data, or confidential research data. Utilizing a pretrained tokenizer might raise concerns about data privacy and security. By developing our own tokenizer, we can ensure that patient identifiers, confidential data, or any personally identifiable information (PII) are handled appropriately during tokenization. This customization provides an extra layer of control and reassurance, mitigating the risks associated with using external pretrained models on sensitive medical data.\n",
    "\n",
    "Moreover, the process of creating a custom tokenizer for medical texts fosters a deeper understanding of the domain and linguistic nuances. This understanding can lead to better preprocessing strategies, including normalization, stemming, or lemmatization, which are vital for subsequent analysis. By being actively involved in designing the tokenizer, domain experts can provide valuable insights and expertise, resulting in improved tokenization performance and better overall language understanding within the medical domain.\n",
    "\n",
    "BERT's tokenizer utilizes subword tokenization, which means that words are split into subword units based on learned vocabulary. This is indicated by the presence of '##' preceding some tokens, such as '##ache', '##ost', '##omy', 'bro', '##nch', '##os', '##co', '##py', 'tr', '##ache', '##al', 'ste', '##nt', 'dil', '##ation', 'tr', '##ache', '##a', 'shi', '##ley', 'can', '##nu', '##la', 'tr', '##ache', '##ost', '##omy'. This subword tokenization enables BERT to handle out-of-vocabulary words and capture subword-level information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['neck', 'exploration', ';', 'tr', '##ache', '##ost', '##omy', ';',\n",
       "       'urgent', 'flexible', 'bro', '##nch', '##os', '##co', '##py',\n",
       "       'via', 'tr', '##ache', '##ost', '##omy', 'site', ';', 'removal',\n",
       "       'of', 'foreign', 'body', ',', 'tr', '##ache', '##al', 'metallic',\n",
       "       'ste', '##nt', 'material', ';', 'dil', '##ation', 'distal', 'tr',\n",
       "       '##ache', '##a', ';', 'placement', 'of', '#', '8', 'shi', '##ley',\n",
       "       'single', 'can', '##nu', '##la', 'tr', '##ache', '##ost', '##omy',\n",
       "       'tube', '.'], dtype='<U11')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "np.array(tokenizer.tokenize(df['description'][5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateNewTokenizer:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = Tokenizer(models.WordPiece(unk_token = '[UNK]'))\n",
    "        self.tokenizer.normalizer = normalizers.Sequence([\n",
    "            normalizers.NFD(),\n",
    "            normalizers.Lowercase(),\n",
    "            normalizers.StripAccents()])\n",
    "        \n",
    "        self.tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()\n",
    "        self.special_tokens = ['[UNK]', '[PAD]', '[CLS]', '[SEP]', '[MASK]']\n",
    "        self.trainer = trainers.WordPieceTrainer(\n",
    "            vocab_size = 30000,\n",
    "            special_tokens = self.special_tokens)\n",
    "        \n",
    "        self.cls_token_id = None\n",
    "        self.sep_token_id = None\n",
    "\n",
    "    def load_dataset(self, filepath):\n",
    "        dataset = pd.read_csv(filepath)\n",
    "        dataset['transcription'].fillna(dataset['description'], inplace = True)\n",
    "        return dataset\n",
    "\n",
    "    def get_training_corpus(self, dataset):\n",
    "        for i in range(0, len(dataset), 1000):\n",
    "            yield dataset[i: i + 1000]['transcription']\n",
    "\n",
    "    def train_tokenizer(self, dataset):\n",
    "        self.tokenizer.train_from_iterator(\n",
    "            self.get_training_corpus(dataset),\n",
    "            trainer = self.trainer)\n",
    "        \n",
    "        self.cls_token_id = self.tokenizer.token_to_id('[CLS]')\n",
    "        self.sep_token_id = self.tokenizer.token_to_id('[SEP]')\n",
    "\n",
    "        self.tokenizer.post_processor = processors.TemplateProcessing(\n",
    "            single = f'[CLS]:0 $A:0 [SEP]:0',\n",
    "            pair   = f'[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1',\n",
    "            special_tokens = [('[CLS]', self.cls_token_id), ('[SEP]', self.sep_token_id)])\n",
    "        self.tokenizer.decoder = decoders.WordPiece(prefix = '##')\n",
    "\n",
    "    def save_tokenizer(self, filepath):\n",
    "        self.tokenizer.save(filepath)\n",
    "\n",
    "    def generate_tokenizer(self, dataset_filepath, tokenizer_filepath):\n",
    "        dataset = self.load_dataset(dataset_filepath)\n",
    "        self.train_tokenizer(dataset)\n",
    "        self.save_tokenizer(tokenizer_filepath)\n",
    "\n",
    "class CombinedTokenizer:\n",
    "    def __init__(self, bert_tokenizer, medical_tokenizer):\n",
    "        self.bert_tokenizer = bert_tokenizer\n",
    "        self.medical_tokenizer = medical_tokenizer\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        bert_tokens = self.bert_tokenizer.tokenize(text)\n",
    "        medical_tokens = self.medical_tokenizer.tokenize(text)\n",
    "        combined_tokens = bert_tokens + medical_tokens  \n",
    "        return combined_tokens\n",
    "\n",
    "\n",
    "tokenizer = GenerateNewTokenizer()\n",
    "tokenizer.generate_tokenizer('../data/mtsamples_modified.csv', '../data/tokenizer.json')\n",
    "token_path = '../data/tokenizer.json'\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "medical_tokenizer  = Tokenizer.from_file(token_path)\n",
    "medical_tokenizer  = BertTokenizerFast(tokenizer_object = medical_tokenizer)\n",
    "medical_tokenizer.model_max_length = 128\n",
    "combined_tokenizer = CombinedTokenizer(bert_tokenizer,    medical_tokenizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our tokenizer correctly recognizes compound words like 'bronchoscopy', 'foreign body', 'distal trachea', 'tracheostomy tube' as individual tokens. BERT's tokenizer, on the other hand, splits some of these compound words into subwords, resulting in 'bro', '##nch', '##os', '##co', '##py', 'foreign', 'body', 'distal', 'tr', '##ache', '##a', 'tube'. This split may result from the subword tokenization process of BERT's tokenizer, where it aims to create a more flexible and general-purpose tokenization scheme. Our tokenizer, which recognizes complete words and compound words as tokens, may better preserve the context and semantic meaning of the original text. This can be beneficial for tasks where precise word-level analysis or semantic understanding is important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['neck', 'exploration', ';', 'tracheostomy', ';', 'urgent',\n",
       "       'flexible', 'bronchoscopy', 'via', 'tracheostomy', 'site', ';',\n",
       "       'removal', 'of', 'foreign', 'body', ',', 'tracheal', 'metallic',\n",
       "       'stent', 'material', ';', 'dilation', 'distal', 'trachea', ';',\n",
       "       'placement', 'of', '#', '8', 'shiley', 'single', 'cannula',\n",
       "       'tracheostomy', 'tube', '.'], dtype='<U12')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(medical_tokenizer.tokenize(df['description'][5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(101)\n",
    "df = load_preprocessing(preprocess = False)\n",
    "text_column, labels, num_classes = GetText(df)\n",
    "\n",
    "processor = TokenizationProcessor(max_sequence_length = 32, tokenizer = medical_tokenizer)\n",
    "input_ids, attention_mask, token_type_ids = processor.process_dataset('../data/mtsamples_modified.csv')\n",
    "if input_ids.size(0) != labels.size(0):    \n",
    "    desired_length = input_ids.size(0)\n",
    "    padding_length = desired_length - len(labels)\n",
    "    padding = torch.zeros(padding_length, dtype = torch.long)\n",
    "    labels  = torch.cat((labels, padding))\n",
    "    \n",
    "train_loader, valid_loader, test_loader = GENERATE_DATALOADER(input_ids, attention_mask, labels, use_sampler = True)\n",
    "\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "num_epochs = 5\n",
    "criterion  = nn.CrossEntropyLoss()\n",
    "optimizer  = torch.optim.Adam(bert_model.parameters(), lr = 0.001)\n",
    "train_losses, valid_losses, train_accurs, valid_accurs, test_loss, test_accu, best_epoch, epoch_times, test_predict, best_model = train(num_epochs, bert_model, train_loader, valid_loader, test_loader, \n",
    "                                                                                                                                        optimizer, criterion, device, accuracy = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Metrics: \n",
      "\t Accuracy:  \t 0.84375\n",
      "\t Precision: \t 0.7302\n",
      "\t Recall:    \t 0.84375\n",
      "\t F1-score:  \t 0.71742\n"
     ]
    }
   ],
   "source": [
    "epoch_valid_loss, epoch_valid_accu, epoch_valid_prediction = _evals(bert_model, \n",
    "                                                                    test_loader, \n",
    "                                                                    criterion, \n",
    "                                                                    batch_size = 32, \n",
    "                                                                    device = 'cpu', \n",
    "                                                                    display = False)\n",
    "for idx, data in enumerate(test_loader):\n",
    "    inputs, attens, labels = data \n",
    "    break\n",
    "eval_predictions(epoch_valid_prediction, labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
