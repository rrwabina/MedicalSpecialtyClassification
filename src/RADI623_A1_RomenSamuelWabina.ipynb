{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **RADI623: Natural Language Processing**\n",
    "\n",
    "### Assignment: Natural Language Processing\n",
    "**Romen Samuel Rodis Wabina** <br>\n",
    "Student, PhD Data Science in Healthcare and Clinical Informatics <br>\n",
    "Clinical Epidemiology and Biostatistics, Faculty of Medicine (Ramathibodi Hospital) <br>\n",
    "Mahidol University\n",
    "\n",
    "Note: In case of Python Markdown errors, you may access the assignment through this GitHub [Link](https://github.com/rrwabina/NLP-Medical-Specialty-Classification)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Medical Specialty Identification**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem of predicting oneâ€™s illnesses wrongly through self-diagnosis in medicine is very real. In a report by the [Telegraph](https://www.telegraph.co.uk/news/health/news/11760658/One-in-four-self-diagnose-on-the-internet-instead-of-visiting-the-doctor.html), nearly one in four self-diagnose instead of visiting the doctor. Out of those who misdiagnose, nearly half have misdiagnosed their illness wrongly [reported](https://bigthink.com/health/self-diagnosis/). While there could be multiple root causes to this problem, this could stem from a general unwillingness and inability to seek professional help.\n",
    "\n",
    "Elevent percent of the respondents surveyed, for example, could not find an appointment in time. This means that crucial time is lost during the screening phase of a medical treatment, and early diagnosis which could have resulted in illnesses treated earlier was not achieved.\n",
    "\n",
    "With the knowledge of which medical specialty area to focus on, a patient can receive targeted help much faster through consulting specialist doctors. To alleviate waiting times and predict which area of medical specialty to focus on, we can utilize natural language processing (NLP) to solve this task.\n",
    "\n",
    "Given any medical transcript or patient condition, this solution would predict the medical specialty that the patient should seek help in. Ideally, given a sufficiently comprehensive transcript (and dataset), one would be able to predict exactly which illness he is suffering from."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy  as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import re\n",
    "import logging\n",
    "import random\n",
    "import os\n",
    "import itertools\n",
    "import copy\n",
    "import time\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.pipeline.tagger import Tagger\n",
    "from spacy.language import Language\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from datasets import load_dataset\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import BertModel, BertTokenizer, DistilBertModel\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DistilBertTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import TensorDataset\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "import transformers\n",
    "from time import time\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, BertConfig, get_linear_schedule_with_warmup\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForTokenClassification\n",
    "from transformers import RobertaModel\n",
    "from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler\n",
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer)\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior to NLP task, we first ensured a deterministic behavior of our simulations by setting the seed value for various random number generators in order to obtain consistent results and facilitate reproducibility in the execution of the code. The <code>set_seed</code> function was directly copied from the Professor's lecture codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    if seed:\n",
    "        logging.info(f'Running in deterministic mode with seed {seed}')\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    else:\n",
    "        logging.info('Running in non-deterministic mode')\n",
    "set_seed(1997)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <code>Question 1</code>. Conduct an exploratory data analysis on the dataset.  Report the corpus statistics with appropriate visualization.  Explain clearly why the selected visualization fits your purpose.  \n",
    "\n",
    "Let's first explore the dataset first! The dataset, <code>mtsamples.csv</code>, consists of 4999 samples (rows) with 5 features (columns) - namely <code>description, medical_specialty, sample_name, transcription, and keywords.</code> Since this is a text classification task, the <code>medical_specialty</code> serves as the labels of the dataset. There are 40 unique <code>medical_specialty</code> in the original dataset, leading to a multiclass classification. Some of the labels include Allergy/Immunology, Bariatrics, Cardiovascular/Pulmonary, and General Medicine. There are labels, however, that are not directly related to medical specialties, including Letters, Office Notes, and SOAP/Chart/Progress Notes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in the original dataset: \t 4999\n",
      "Number of unique labels in the original dataset: 40\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('../data/mtsamples.csv')\n",
    "\n",
    "num_samples = len(data)\n",
    "num_medical_specialties = data['medical_specialty'].nunique()\n",
    "\n",
    "print(f'Number of samples in the original dataset: \\t {num_samples}')\n",
    "print(f'Number of unique labels in the original dataset: {num_medical_specialties}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code below, we created functions that can help us to understand the nature of the given dataset. The function <code>calculate_univariate</code> takes a <code>data</code> parameter to display the univariate characteristics of the <code>transcription</code> and <code>description</code>. Specifically, the <code>data</code> parameter should be a DataFrame that contains the columns: <code>transcription</code> and <code>description</code>. This code calculates various statistical measures (average, minimum, maximum) for the lengths of the <code>transcription</code> and <code>description</code> strings in the input data and returns the results in a dictionary format.\n",
    "\n",
    "The function <code>plot_classes</code> takes a <code>data</code> parameter, assumed to be a DataFrame, that can generate a bar plot to visualize the distribution of medical specialties in the input data. This function will be useful to visualize how many samples exist in the dataset for every class (i.e., class distribution). The x-axis represents the medical specialties while the y-axis represents the frequency counts.\n",
    "\n",
    "Similar to previous functions, the <code>plot_histogram</code> takes a DataFrame <code>data</code> parameter that can generate a histogram plot to visualize the distribution of the lengths of the <code>transcription</code> and <code>description</code> values in the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_univariate(data):\n",
    "    description_lengths   = data['description'].str.len()\n",
    "    transcription_lengths = data['transcription'].str.len()\n",
    "\n",
    "    avg_description_length = description_lengths.mean()\n",
    "    min_description_length = description_lengths.min()\n",
    "    max_description_length = description_lengths.max()\n",
    "\n",
    "    avg_transcription_length = transcription_lengths.mean()\n",
    "    min_transcription_length = transcription_lengths.min()\n",
    "    max_transcription_length = transcription_lengths.max()\n",
    "\n",
    "    dictionary = {}\n",
    "    dictionary['description']   = [avg_description_length, min_description_length, max_description_length]\n",
    "    dictionary['transcription'] = [avg_transcription_length, min_transcription_length, max_transcription_length]\n",
    "    return dictionary\n",
    "summary = calculate_univariate(data)\n",
    "\n",
    "def plot_classes(data):\n",
    "    specialty_counts = data['medical_specialty'].value_counts()\n",
    "\n",
    "    plt.figure(figsize = (12, 5))\n",
    "    plt.bar(specialty_counts.index, specialty_counts.values)\n",
    "    plt.xlabel('Medical Specialty')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Medical Specialties')\n",
    "    plt.xticks(rotation = 90)\n",
    "    plt.show()\n",
    "\n",
    "def plot_histogram(data):\n",
    "    description_lengths   = data['description'].str.len()\n",
    "    transcription_lengths = data['transcription'].str.len()\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    axs[0].hist(description_lengths, bins=50, alpha=0.8)\n",
    "    axs[0].set_xlabel('Description Length')\n",
    "    axs[0].set_ylabel('Frequency')\n",
    "    axs[0].set_title('Histogram of Description Lengths')\n",
    "\n",
    "    axs[1].hist(transcription_lengths, bins = 50, alpha = 0.8)\n",
    "    axs[1].set_xlabel('Transcription Length')\n",
    "    axs[1].set_ylabel('Frequency')\n",
    "    axs[1].set_title('Histogram of Transcription Lengths')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset exhibits a significant imbalance in terms of document distribution across medical specialties. <code>Surgery</code> stands out as the most frequent specialty, comprising 1088 documents, while <code>Hospice - Palliative Care</code> has the lowest representation with a mere 6 documents. This discrepancy in frequencies indicates a clear imbalance, where there is a substantial over-representation of documents related to <code>Surgery</code> compared to other specialties, such as <code>Hospice - Palliative Care</code>. Such data imbalances can pose challenges in training accurate and fair machine learning models, as the model may be biased towards the majority class and struggle to generalize well to minority classes.\n",
    "\n",
    "To rectify this imbalance, several techniques can be employed. One common approach is oversampling or undersampling. Oversampling involves duplicating instances from the minority class to increase its representation, while undersampling involves removing instances from the majority class to reduce its dominance. Another technique is generating synthetic samples using methods like Synthetic Minority Over-sampling Technique (SMOTE), which creates new instances by interpolating between existing minority class samples. Additionally, techniques like stratified sampling during train-test splitting or cross-validation can ensure that each subset maintains the original class distribution. *We can rectify this problem later.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsgAAAFgCAYAAACmDI9oAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAAA0VElEQVR4nO3de7htZVn///dHQFABEdjxRUC3CmZYCrRFTOyLmoYooeUBM0UjsULS1AzphN+yH3bQPJSKQYIZQigKagUiapQCGwQE1NgiBMhhi3LygBzu3x/jWTL2Ys2159qsOec6vF/XNa81xjNO9xhzrmfe8xnPGCNVhSRJkqTOAyYdgCRJkrSQmCBLkiRJPSbIkiRJUo8JsiRJktRjgixJkiT1mCBLkiRJPSbIi0ySS5PsM+k4JinJC5JcneT2JLtPOp5BkhyR5B/vx/L/luSg+YxpIUlyZZJfmnQc0jCsexdP3TuMJC9Lcvr9WP79Sf5kPmNaSJJ8PslvTTqOSTJBXkBmShiSvDLJ2VPjVfX4qvr8etazMkkl2XhEoU7a3wCvrarNq+or0ye2ff9+q8RvSnJmkpeMO8iq+suqGqqCSXJkkn+etvxzquq4+Y4ryYeS/MV8r3ehbVMalnXv0Gase5M8otW3U69+HXx7kqdNMOYZVdVHqurZw8w7/bPQlv/tqvrz+Y5rpu+CUZvENheDpfpPrBFKsnFV3TXBEB4JXLqeeZ5YVWuSbAs8B3hvksdV1VtHH96COEaSlpgFUK/MWPdW1f8Cm0+NJylaHTx93gWwDwsiBi18tiAvMv2WjiR7Jlmd5NYkNyR5R5vti+3vze3X+1OSPCDJHye5KsmNSY5P8tDeel/Rpt2U5E+mbefIJCcn+ecktwKvbNv+UpKbk1yX5L1JHthbXyX53SSXJ7ktyZ8neUyS/27xntSff9o+zhhrkk2T3A5sBFyU5JvrO15V9Z2q+jDwO8BbkmzTtvHQJMe02K9N8hdJNmrTdk7yhSS3JPlOkhN7sT0+yRlJvtuO+RGzHKOf/CrvtSwdkuTbbbtvatP2BY4AXtLer4ta+U9Occ32/vXWfVCS/20x/9H6js2AY/+8JBe29/W/kzyhN+3KJG9KcnE7Nicm2aw3/c1tv76d5LdaTDsnOQR4GfDmtn+n9Ta520zrS7Jtkk+1OL6b5D+TWF9pYmLdO6e6t7fOVyb5ryTvTHITcGSL53Ntn7+T5CNJtpp2rGesa2arG5LslOTjSda2db93lhjWaRVux+33klzRYvrrdjx+Bng/8JT2nt7c5l/nrFiSVydZ02I6NcnDp637t9t7cnOSv0+SYY9hbz17tffx5iQXpdflJ933xZ+3/bwtyenpGoimps/4OcuA75/mkTOtL8lm7TN5U4vlvCTbzXV/Fryq8rVAXsCVwC9NK3slcPZM8wBfAl7ehjcH9mrDK4ECNu4t95vAGuDRbd6PAx9u03YFbgf2Bh5Idxrtzt52jmzjz6f7UfUg4OeBvejOQqwEvga8vre9Aj4JbAk8HrgDOLNt/6HAZcBBA47DwFh76955luN4n+nAJsBdwHPa+CnAB4CHAD8FnAu8pk07Afijtq+bAXu38i2A64A3tvItgCfPcoyOBP552ntyQtvmzwFrpx3jf54W8+eB3xri/Zta9wfbdp/YjvfPDDg+HwL+Yoby3YEbgSfTfREeRPd527T32TsXeDiwdXvPf7tN2xe4vr3XDwb+uf8+zLTN9azv/6P7UtqkvZ4GZNL/o76W5gvr3vXG2lv3wLp3pvnacbwLOKzF/CBgZ+BZwKbACrofFn837VjPqW6gJe/AO+nq2H7dPVMM09/fAs5q23sE8D/cW/+uM28r+xCtTgOeAXwH2KPt03uAL05b96eArdq61wL7Djh2RzLtu6CV7wDcBOzXPgvPauMr2vTPA98EHtv27/PAUXP4nM30/TNofa8BTqOr6zei+0xuOen/4/l+2SKz8Hyi/SK7uf1S/YdZ5r0T2DnJtlV1e1V9eZZ5Xwa8o6quqKrbgbcAB6brK/dC4LSqOruqfgz8Kd0/dN+XquoTVXVPVf2wqs6vqi9X1V1VdSVdsvl/py3zV1V1a1VdClwCnN62fwvwb3QJ2Vxj3SBVdSddBbZ1+6W7H92Xyver6ka6SvXANvuddKcSH15VP6qqqVaG5wHXV9XftvLbquqcQcdoQChvbdv8KvBPwEuH3IVhjslb23tzEd0XxROHXPeUQ4APVNU5VXV3df2f76D7Mp7y7qr6dlV9l66C3K2Vvxj4p6q6tKp+QFfhDmPQ+u4EtgceWVV3VtV/VquZpRGx7h1B3dt8u6re02L+YVWtqaozquqOqloLvGOGfZhr3bAnXUL9B62O7dfd94lhQJxvr6rvVtdl5O+YW/18bFVdUFV30B23pyRZ2ZvnqKq6ua37rN7+DOs3gM9U1WfaZ+EMYDXdd9mUf6qq/2n7d1JvG8N8zmYyaH13AtvQ/Qi6u30mb53j/ix4JsgLz/OraqupF/C7s8x7MN2vu6+3UxzPm2XehwNX9cavovslvV2bdvXUhJbg3DRt+av7I0ke205zXZ/u1N9fAttOW+aG3vAPZxjfnJnNFusGSbIJXUvFd+mS302A63pfhh+ga0kGeDNdi8S56a5c/81WvhPdL+pBrp5l2kzzXEW3r8MY5phc3xv+AYOP7yCPBN44LUnYaVqMg7axzmeI4Y7FbOv7a7qWrNPbKc/Dh1yftKGse0dQ9zbT92G7JB9N173tVrozTtP3Ya51w07AVTW4b/HY6uf24+ImulbfKfNRP79oWv28N92PhfVtY5jP2UwGre/DwH8AH03Xpe6v2nfskmKCvIhV1eVV9VK6xO7twMlJHsLMvwy/TfcPNuURdKecbqDrNrDj1IQkD6L7dbjO5qaNvw/4OrBLVW1J14dpzn2qBpgt1g11QFvHuXQVxR3Atr0vxC2r6vEAVXV9Vb26qh5OdyrpH5Ls3JZ79CzbGOYX+U694UfQ7eswy47imEx3NfC2fpJQVQ+uqhOGWHadzxDr7icMd2zunblrnX9jVT0a+BXgDUmeOZd1SKNi3Ttn0/fhL1vZz7V9+A2G3IdZ6oargUfM0to9tvq5fRa2Aa4dYpvDupquu0u/fn5IVR01xLLr+5zNtX6+s6reWlW7Ar9Ad3b1FXNZx2JggryIJfmNJCuq6h7g5lZ8D13/pntYN5k7Afj9JI9KsjldBXVi+7V9MrB/kl9Id/HGkay/stoCuBW4Pcnj6C6Cmy+zxTonSbZO8jLg7+lOn91UVdcBpwN/m2TLdiHGY5L837bMi5JMVSbfo6s87qHrQ7Z9ktenu2hliyRPnmNIf5LkwUkeD7wKmLoA8AZgZQZfiDZvx6TZqF1oMfV6IF0f5t9O8uR0HpLkuUm2GGJ9JwGvSvIzSR4MTL8/6A3M/uNiHekuFtw5SYBbgLvp3gNp4qx777ct6PrE3pJkB+APhl1wlrrhXLpE8KhWd22W5KlzjOsPkjwsyU7A61i3ft4xAy5upDtur0qyW5JN6Y7bOa0LzIZ4wLT6eVO6Vvb9k/xykqn6e5/ed9Vs1vc5W9/3zzqSPD3Jz6W7sP1Wui4XS65+NkFe3PYFLk13dfG7gANb/64fAG8D/quditkLOJbutMgXgW8BP6K7YIHWT+0w4KN0FcztdBdr3THLtt8E/DpwG11ideIs887VwFjn4KJ2XNYAvwX8flX9aW/6K+guVriMLgk+mXtPVT0JOKctfyrwutYn7za6CyP2pzv1dDnw9DnG9YUW05nA31TV1I3q/7X9vSnJBTMsNx/HpO9wulOtU6/PVdVq4NXAe+mOyRq6i1PWq6r+DXg3Xd+6NcBUn8ypz9AxwK7t8/iJIVa5C/BZus/il4B/qKqzholFGgPr3vvnrXQXtN0CfJruYsBhzVg3VNXddHXzzsD/AtcAc73//SeB84ELW1zHtPLP0d3e7vok35m+UFV9lq5R4GN07+NjuPealg3xUtatn79ZVVfTnQk9gu6H2NV0PyzWm8cN8Tlb3/fPdP+H7jvzVroLKL9A97lZUlJe96JpWsvBzXSn8L414XCWhHQXa3wL2GQErTELTrpbI11CdweMJb+/0nyw7p2cdPdu3qVmuHfzUuPnbDi2IAuAJPu3U/8PobsFzFfpbrUjDSXdY2g3TfIwun6Zp5kcS7Oz7tU4+DmbOxNkTTmA7kKDb9OdwjqwPL2guXkN3Wm7b9L1C5zPvpHSUmXdq3HwczZHdrGQJEmSemxBliRJknru79NxJmrbbbetlStXTjoMSZqT888//ztVtWLSccwn62NJi9Gg+nhRJ8grV65k9erVkw5DkuYkyVXrn2txsT6WtBgNqo/tYiFJkiT1mCBLkiRJPSbIkiRJUo8JsiRJktRjgixJkiT1mCBLkiRJPSbIkiRJUo8JsiRJktRjgixJkiT1mCBLkiRJPSbIkiRJUo8JsiRJktSz8aQD0Lr2f8/Z9yk77bC9JxCJJC1OM9WjYF0qaXi2IEuSJEk9JsiSJElSjwmyJC0DSa5M8tUkFyZZ3cq2TnJGksvb34e18iR5d5I1SS5Ossdko5ek8TJBlqTl4+lVtVtVrWrjhwNnVtUuwJltHOA5wC7tdQjwvrFHKkkTNLIEOclmSc5NclGSS5O8tZU/Ksk5rWXixCQPbOWbtvE1bfrKUcUmSQLgAOC4Nnwc8Pxe+fHV+TKwVZLtJxCfJE3EKFuQ7wCeUVVPBHYD9k2yF/B24J1VtTPwPeDgNv/BwPda+TvbfJKk+VHA6UnOT3JIK9uuqq5rw9cD27XhHYCre8te08rWkeSQJKuTrF67du2o4paksRvZbd6qqoDb2+gm7VXAM4Bfb+XHAUfSnb47oA0DnAy8N0naepacQbchkqQR2buqrk3yU8AZSb7en1hVlWRO9W1VHQ0cDbBq1aolWVdLWp5G2gc5yUZJLgRuBM4AvgncXFV3tVn6rRI/abFo028BtplhnbZYSNIcVdW17e+NwCnAnsANU10n2t8b2+zXAjv1Ft+xlUnSsjDSBLmq7q6q3egq1z2Bx83DOo+uqlVVtWrFihX3d3WStOQleUiSLaaGgWcDlwCnAge12Q4CPtmGTwVe0e5msRdwS68rhiQteWN5kl5V3ZzkLOApdBd7bNxaifutElMtFtck2Rh4KHDTOOKTpCVuO+CUJNDV+/9SVf+e5DzgpCQHA1cBL27zfwbYD1gD/AB41fhDlqTJGVmCnGQFcGdLjh8EPIvuwruzgBcCH+W+LRYHAV9q0z+3VPsfS9I4VdUVwBNnKL8JeOYM5QUcOobQJGlBGmUL8vbAcUk2ouvKcVJVfSrJZcBHk/wF8BXgmDb/McCHk6wBvgscOMLYJEmSpBmN8i4WFwO7z1B+BV1/5OnlPwJeNKp4JEmSpGH4JD1JkiSpxwRZkiRJ6jFBliRJknpMkCVJkqQeE2RJkiSpxwRZkiRJ6jFBliRJknpMkCVJkqQeE2RJkiSpxwRZkiRJ6hnZo6aXuv3fc/Z9yk47bO8JRCJJkqT5ZAuyJEmS1GOCLEmSJPWYIEuSJEk9JsiSJElSjxfpSZI0jRdiS8ubLciSJElSjy3Ii8BMLRlga4YkDaofJen+sAVZkiRJ6jFBliRJknpMkCVJkqQeE2RJkiSpxwRZkiRJ6jFBliRJknpMkCVJkqQe74M8Bt6nU5IkafEwQV7EfBSqJEnS/LOLhSRJktRjgixJkiT1mCBLkiRJPSbIkiRJUo8X6UmSli3vMiRpJrYgS5IkST0myJIkSVKPXSzmkafqJEmSFj9bkCVJkqSekSXISXZKclaSy5JcmuR1rfzIJNcmubC99ust85Yka5J8I8kvjyo2SZIkaZBRdrG4C3hjVV2QZAvg/CRntGnvrKq/6c+cZFfgQODxwMOBzyZ5bFXdPcIYl7VBXUJ8XLUkSVrORtaCXFXXVdUFbfg24GvADrMscgDw0aq6o6q+BawB9hxVfJIkSdJMxtIHOclKYHfgnFb02iQXJzk2ycNa2Q7A1b3FrmGGhDrJIUlWJ1m9du3aUYYtSZKkZWjkCXKSzYGPAa+vqluB9wGPAXYDrgP+di7rq6qjq2pVVa1asWLFfIcrSZKkZW6kCXKSTeiS449U1ccBquqGqrq7qu4BPsi93SiuBXbqLb5jK5MkSZLGZpR3sQhwDPC1qnpHr3z73mwvAC5pw6cCBybZNMmjgF2Ac0cVnyRJkjSTUd7F4qnAy4GvJrmwlR0BvDTJbkABVwKvAaiqS5OcBFxGdweMQ72DhSTNjyQbAauBa6vqea0h4qPANsD5wMur6sdJNgWOB34euAl4SVVdOaGwJWkiRpYgV9XZQGaY9JlZlnkb8LZRxSRJy9jr6O4mtGUbfzvdLTc/muT9wMF014gcDHyvqnZOcmCb7yWTCFiSJsVHTUvSEpdkR+C5dA0Qb2hd4J4B/Hqb5TjgSLoE+YA2DHAy8N4kqaoaZ8yjMOje75I0nY+alqSl7++ANwP3tPFtgJur6q423r+t5k9uudmm39Lmvw9vuylpqTJBlqQlLMnzgBur6vz5Xre33ZS0VNnFQpKWtqcCv5JkP2Azuj7I7wK2SrJxayXu31Zz6pab1yTZGHgo3cV6krRs2IIsSUtYVb2lqnasqpXAgcDnquplwFnAC9tsBwGfbMOntnHa9M8thf7HkjQXJsiStDz9Id0Fe2vo+hgf08qPAbZp5W8ADp9QfJI0MXaxkKRloqo+D3y+DV/BvU8y7c/zI+BFYw1MkhYYW5AlSZKkHhNkSZIkqccuFkvMoBvhn3bY3mOORJIkaXEyQdZ9zJRkm2BLkqTlwi4WkiRJUo8JsiRJktRjgixJkiT1mCBLkiRJPSbIkiRJUo8JsiRJktRjgixJkiT1mCBLkiRJPSbIkiRJUo8JsiRJktTjo6YlSQve/u85e9IhSFpGbEGWJEmSekyQJUmSpB4TZEmSJKnHBFmSJEnqMUGWJEmSekyQJUmSpB4TZEmSJKnHBFmSJEnqMUGWJEmSekyQJUmSpB4TZEmSJKnHBFmSJEnqMUGWJEmSekyQJUmSpJ6RJchJdkpyVpLLklya5HWtfOskZyS5vP19WCtPkncnWZPk4iR7jCo2SZIkaZBRtiDfBbyxqnYF9gIOTbIrcDhwZlXtApzZxgGeA+zSXocA7xthbJIkSdKMNh7ViqvqOuC6Nnxbkq8BOwAHAPu02Y4DPg/8YSs/vqoK+HKSrZJs39YjSdJE7f+es2csP+2wvccciaRRG0sf5CQrgd2Bc4Dteknv9cB2bXgH4OreYte0sunrOiTJ6iSr165dO7qgJUmStCyNPEFOsjnwMeD1VXVrf1prLa65rK+qjq6qVVW1asWKFfMYqSRJkjTiBDnJJnTJ8Ueq6uOt+IYk27fp2wM3tvJrgZ16i+/YyiRJkqSxGeVdLAIcA3ytqt7Rm3QqcFAbPgj4ZK/8Fe1uFnsBt9j/WJIkSeM2sov0gKcCLwe+muTCVnYEcBRwUpKDgauAF7dpnwH2A9YAPwBeNcLYJEmSpBmN8i4WZwMZMPmZM8xfwKGjikeSJEkahk/SkyRJknpMkCVJkqQeE2RJkiSpxwRZkiRJ6jFBliRJknqGSpCT/NyoA5EkrZ/1sSSN3rAtyP+Q5Nwkv5vkoSONSJI0G+tjSRqxoRLkqnoa8DK6R0Gfn+RfkjxrpJFJku5jQ+rjJJu1pPqiJJcmeWsrf1SSc5KsSXJikge28k3b+Jo2feWo90uSFpKhHxRSVZcn+WNgNfBuYPf2OOkjqurjowpQkrSuDaiP7wCeUVW3J9kEODvJvwFvAN5ZVR9N8n7gYOB97e/3qmrnJAcCbwdeMoZdW5T2f8/Z9yk77bC9JxCJpPkyVIKc5Al0j35+LnAGsH9VXZDk4cCXABPkJW6mLwDwS0Aatw2pj9uTSm9vo5u0VwHPAH69lR8HHEmXIB/QhgFOBt6bJG09krTkDdsH+T3ABcATq+rQqroAoKq+DfzxqIKTJN3HBtXHSTZKciFwI11i/U3g5qq6q81yDbBDG94BuLqt9y7gFmCb+d8VSVqYhu1i8Vzgh1V1N0CSBwCbVdUPqurDI4tOkjTdBtXHbf7dkmwFnAI87v4GkuQQ4BCARzziEfd3dZK0YAzbgvxZ4EG98Qe3MknSeN2v+riqbgbOAp4CbJVkqqFkR+DaNnwt3UWAtOkPBW6aYV1HV9Wqqlq1YsWKOe6GJC1cwybIm1XVVP812vCDRxOSJGkWc66Pk6xoLcckeRDwLOBrdInyC9tsBwGfbMOntnHa9M/Z/1jScjJsgvz9JHtMjST5eeCHowlJkjSLDamPtwfOSnIxcB5wRlV9CvhD4A1J1tD1MT6mzX8MsE0rfwNw+DzvgyQtaMP2QX498K9Jvg0E+D94yx9JmoTXM8f6uKouBnafofwKYM8Zyn8EvGg+gpWkxWioBLmqzkvyOOCnW9E3qurO0YUlSZqJ9bEkjd7QDwoBngSsbMvskYSqOn4kUUmSZmN9LEkjNOyDQj4MPAa4ELi7FRdghSxJY2R9LEmjN2wL8ipgV69ilqSJsz6WpBEb9i4Wl9BdCCJJmizrY0kasWFbkLcFLktyLnDHVGFV/cpIotKitv97zp6x/LTD9h5zJNKSZH0sSSM2bIJ85CiDkCQN7chJByBJS92wt3n7QpJHArtU1WeTPBjYaLShaTmwtVmaG+tjSRq9ofogJ3k1cDLwgVa0A/CJEcUkSRrA+liSRm/Yi/QOBZ4K3ApQVZcDPzWqoCRJA1kfS9KIDdsH+Y6q+nESAJJsTHffTWlog7pTSJoT62NJGrFhW5C/kOQI4EFJngX8K3Da6MKSJA1gfSxJIzZsgnw4sBb4KvAa4DPAH48qKEnSQNbHkjRiw97F4h7gg+0lSZoQ62NJGr2hEuQk32KGPm5V9eh5j0gjYf9faWmwPpak0Rv2Ir1VveHNgBcBW89/OJKk9bA+lqQRG7aLxU3Tiv4uyfnAn85/SJKkQayPFwcfgiQtbsN2sdijN/oAuhaMYVufJUnzxPpYkkZv2Er1b3vDdwFXAi+e92gkSetjfSxJIzZsF4unjzoQSdL6WR9L0ugN28XiDbNNr6p3zLDMscDzgBur6mdb2ZHAq+nu4QlwRFV9pk17C3AwcDfwe1X1H0PugyQtGxtSH0uS5mYud7F4EnBqG98fOBe4fJZlPgS8Fzh+Wvk7q+pv+gVJdgUOBB4PPBz4bJLHVtXdQ8YnScvFhtTHkqQ5GDZB3hHYo6pug5+0BH+6qn5j0AJV9cUkK4dc/wHAR6vqDuBbSdYAewJfGnJ5SVou5lwfS5LmZthHTW8H/Lg3/uNWtiFem+TiJMcmeVgr2wG4ujfPNa3sPpIckmR1ktVr166daRZJWsrmsz6WJM1g2AT5eODcJEe21opzgOM2YHvvAx4D7AZcx7pXYw+lqo6uqlVVtWrFihUbEIIkLWrzVR9LkgYY9i4Wb0vyb8DTWtGrquorc91YVd0wNZzkg8Cn2ui1wE69WXdsZZKknvmqjyVJgw3bggzwYODWqnoXcE2SR811Y0m2742+ALikDZ8KHJhk07beXeguOpEk3df9ro8lSYMNe5u3P6O7cvqngX8CNgH+GXjqLMucAOwDbJvkGuDPgH2S7AYU3c3tXwNQVZcmOQm4jO7G94d6B4vFYdDjVCWNxobUx5KkuRn2LhYvAHYHLgCoqm8n2WK2BarqpTMUHzPL/G8D3jZkPJK0XM25PpYkzc2wXSx+XFVF1/JLkoeMLiRJ0iysjyVpxIZNkE9K8gFgqySvBj4LfHB0YUmSBrA+lqQRW28XiyQBTgQeB9xK1+/tT6vqjBHHJknqsT6WpPFYb4JcVZXkM1X1c4CVsCRNiPWxJI3HsBfpXZDkSVV13kijkWYx6I4Zpx2295gjkSbK+liSRmzYBPnJwG8kuRL4PhC6xownjCowSdKMrI8lacRmTZCTPKKq/hf45THFI0magfWxJI3P+lqQPwHsUVVXJflYVf3aGGKSfACJdF+fwPpYksZifQlyesOPHmUgC5WJmqQFYtnXx5I0LutLkGvAsCRpvKyPl4CZGl280FhaeNaXID8xya10LRcPasNw70UhW440OknSFOtjSRqTWRPkqtpoXIFIkgazPpak8Rn2UdOSJEnSsmCCLEmSJPWYIEuSJEk9JsiSJElSjwmyJEmS1LO+27xJi9KgB7x4v1EtN0l2Ao4HtqO7f/LRVfWuJFsDJwIrgSuBF1fV95IEeBewH/AD4JVVdcEkYpekSbEFWZKWtruAN1bVrsBewKFJdgUOB86sql2AM9s4wHOAXdrrEOB94w9ZkibLBFmSlrCqum6qBbiqbgO+BuwAHAAc12Y7Dnh+Gz4AOL46Xwa2SrL9eKOWpMkyQZakZSLJSmB34Bxgu6q6rk26nq4LBnTJ89W9xa5pZTOt75Akq5OsXrt27WiClqQJMEGWpGUgyebAx4DXV9Wt/WlVVXT9k+ekqo6uqlVVtWrFihXzFKkkTZ4JsiQtcUk2oUuOP1JVH2/FN0x1nWh/b2zl1wI79RbfsZVJ0rJhgixJS1i7K8UxwNeq6h29SacCB7Xhg4BP9spfkc5ewC29rhiStCx4mzdJWtqeCrwc+GqSC1vZEcBRwElJDgauAl7cpn2G7hZva+hu8/aqsUYrSQuACbIkLWFVdTaQAZOfOcP8BRw60qAkaYGzi4UkSZLUY4IsSZIk9ZggS5IkST32Qdaysv97zr5P2WmH7T2BSCRJ0kJlC7IkSZLUY4IsSZIk9djFQoveTN0mJGmxs0uYNDm2IEuSJEk9JsiSJElSjwmyJEmS1DOyBDnJsUluTHJJr2zrJGckubz9fVgrT5J3J1mT5OIke4wqLkmSJGk2o2xB/hCw77Syw4Ezq2oX4Mw2DvAcYJf2OgR43wjjkiRJkgYaWYJcVV8Evjut+ADguDZ8HPD8Xvnx1fkysFWS7UcVmyRJkjTIuPsgb1dV17Xh64Ht2vAOwNW9+a5pZfeR5JAkq5OsXrt27egilSRJ0rI0sfsgV1UlqQ1Y7mjgaIBVq1bNeXlJkhYS7+UuLTzjbkG+YarrRPt7Yyu/FtipN9+OrUySJEkaq3EnyKcCB7Xhg4BP9spf0e5msRdwS68rhiRJkjQ2I+tikeQEYB9g2yTXAH8GHAWclORg4CrgxW32zwD7AWuAHwCvGlVckiRJ0mxGliBX1UsHTHrmDPMWcOioYpEkSZKG5ZP0JEmSpJ6J3cVCkqSZeFeHwQYdm9MO23vMkUhLmy3IkiRJUo8JsiRJktRjgixJkiT1mCBLkiRJPSbIkiRJUo8JsiRJktRjgixJkiT1mCBLkiRJPSbIkiRJUo8JsiRJktRjgixJkiT1mCBLkiRJPSbIkiRJUo8JsiRJktSz8aQDkCZt//ecPWP5aYftPeZIJEnSQmALsiRJktRjgixJkiT1mCBLkiRJPSbIkiRJUo8JsiRJktTjXSykEfHuGJIkLU62IEuSJEk9JsiSJElSjwmyJEmS1GMfZGmAmfoQ239Yi1GSY4HnATdW1c+2sq2BE4GVwJXAi6vqe0kCvAvYD/gB8MqqumAScUvSpNiCLElL34eAfaeVHQ6cWVW7AGe2cYDnALu01yHA+8YUoyQtGCbIkrTEVdUXge9OKz4AOK4NHwc8v1d+fHW+DGyVZPuxBCpJC4QJsiQtT9tV1XVt+Hpguza8A3B1b75rWtl9JDkkyeokq9euXTu6SCVpzEyQJWmZq6oCagOWO7qqVlXVqhUrVowgMkmaDC/Sk+bAh39oCbkhyfZVdV3rQnFjK78W2Kk3346tTAuYFxVL88sWZElank4FDmrDBwGf7JW/Ip29gFt6XTEkaVmwBVmSlrgkJwD7ANsmuQb4M+Ao4KQkBwNXAS9us3+G7hZva+hu8/aqsQcsSRNmgixJS1xVvXTApGfOMG8Bh442os6gLkuSNGl2sZAkSZJ6JtKCnORK4DbgbuCuqlo16KlOk4hPkiRJy9ckW5CfXlW7VdWqNj7oqU6SJEnS2CykLhaDnuokSZIkjc2kEuQCTk9yfpJDWtmgpzqtwyc3SZIkaZQmdReLvavq2iQ/BZyR5Ov9iVVVSWZ8qlNVHQ0cDbBq1ao5P/lJkiRJms1EWpCr6tr290bgFGBP2lOdAKY91UmSJEkam7G3ICd5CPCAqrqtDT8b+H/c+1Sno1j3qU6SJGmOBt1n2kdQS+s3iS4W2wGnJJna/r9U1b8nOY+Zn+okSZLmiYmztH5jT5Cr6grgiTOU38QMT3WSJEmSxslHTUsL2EwtPbbySJI0WgvpPsiSJEnSxNmCLM2DQX36JEnS4mMLsiRJktRjgixJkiT1mCBLkiRJPSbIkiRJUo8JsiRJktTjXSykBcC7YEiStHDYgixJkiT1mCBLkiRJPXaxaDzFLUmSJLAFWZIkSVqHCbIkSZLUY4IsSZIk9dgHWZIkzWjQ9TmnHbb3mCORxssEWRozLwiVtBBZN0n3souFJEmS1GOCLEmSJPWYIEuSJEk99kGWFhkvmpEkabRsQZYkSZJ6TJAlSZKkHrtYSMvQTN005tJFw24ekqSlzARZ0qy8N6okabmxi4UkSZLUYwuytETc324TkjQs6xstdSbI0hJm9whJ4+K1CVpKTJAlSdLIzOWHusm0Fgr7IEuSJEk9tiBLkqQFwb7NWihsQZYkSZJ6bEGWNBG2FEkaF+sbzdWyTJC9sl8aDZ/QJ2lc/C7XKNnFQpIkSepZli3IkhYXT49Kmm/jPmNlPba4LLgEOcm+wLuAjYB/rKqjJhySpCVqLl9Yy+3LzbpYC4VdKTQJCypBTrIR8PfAs4BrgPOSnFpVl002Mkkbyi+3xce6WFqX10csHON6LxZUggzsCaypqisAknwUOACwUpak8bEu1rJ1f3/UL4RGgbkkkSb/M0tVTTqGn0jyQmDfqvqtNv5y4MlV9drePIcAh7TRnwa+McfNbAt8Zx7CXciW+j66f4ub+wePrKoV4whmQwxTF7fypVofL8S4jGl4CzGuhRgTLMy4xh3TjPXxQmtBXq+qOho4ekOXT7K6qlbNY0gLzlLfR/dvcXP/lo6lWh8vxLiMaXgLMa6FGBMszLgWSkwL7TZv1wI79cZ3bGWSpPGxLpa0rC20BPk8YJckj0ryQOBA4NQJxyRJy411saRlbUF1saiqu5K8FvgPulsLHVtVl87zZjb4dOAistT30f1b3Ny/BW5MdTEs3GO1EOMypuEtxLgWYkywMONaEDEtqIv0JEmSpElbaF0sJEmSpIkyQZYkSZJ6llWCnGTfJN9IsibJ4ZOOZ0MkOTbJjUku6ZVtneSMJJe3vw9r5Uny7ra/FyfZY3KRDyfJTknOSnJZkkuTvK6VL4l9TLJZknOTXNT2762t/FFJzmn7cWK7MIokm7bxNW36yonuwJCSbJTkK0k+1caXzP4luTLJV5NcmGR1K1sSn89xGmd9PEu9cmSSa9t7eWGS/XrLvKXF9o0kvzyKuOfrs5TkoDb/5UkOup8x/XTveFyY5NYkrx/3sco8fdcNOjZJfr4d+zVt2WxgTH+d5Ottu6ck2aqVr0zyw97xev/6tj1o/zYwrnl7vzKg/t6AmE7sxXNlkgvHfazmpKqWxYvuQpNvAo8GHghcBOw66bg2YD9+EdgDuKRX9lfA4W34cODtbXg/4N+AAHsB50w6/iH2b3tgjza8BfA/wK5LZR9bnJu34U2Ac1rcJwEHtvL3A7/Thn8XeH8bPhA4cdL7MOR+vgH4F+BTbXzJ7B9wJbDttLIl8fkc4zEca308S71yJPCmGebftcW0KfCoFutG8x33fHyWgK2BK9rfh7Xhh83j+3Q98MhxHyvm4btutmMDnNvmTVv2ORsY07OBjdvw23sxrezPN209M2570P5tYFzz9n4xoP6ea0zTpv8t8KfjPlZzeS2nFuSfPDq1qn4MTD06dVGpqi8C351WfABwXBs+Dnh+r/z46nwZ2CrJ9mMJdANV1XVVdUEbvg34GrADS2QfW5y3t9FN2quAZwAnt/Lp+ze13ycDzxympWOSkuwIPBf4xzYeltD+DbAkPp9jNNb6eJZ6ZZADgI9W1R1V9S1gTYt5HHHP9bP0y8AZVfXdqvoecAaw7zzF8kzgm1V11XrinfdjNU/fdTMemzZty6r6cnUZ1vG9dc0ppqo6varuaqNfprtn+EDr2fag/ZtzXLOY0/u1nvp7g2Jq63wxcMJs6xjFsZqL5ZQg7wBc3Ru/htkryMVku6q6rg1fD2zXhhf1Pqc73b47XSvrktnHdN0PLgRupKuwvwnc3Ktk+/vwk/1r028BthlrwHP3d8CbgXva+DYsrf0r4PQk56d71DIsoc/nmEzsuEyrVwBe206PH9s7TTsovvmOez4+S6M8lgeybhIzyWMF83dsdmjD8xkbwG/StXJOeVS6rmZfSPK0XqyDtj1o/zbUfLxfs9XfG+ppwA1VdXmvbNLH6j6WU4K8LLRfWYv+3n1JNgc+Bry+qm7tT1vs+1hVd1fVbnQtDXsCj5tsRPMnyfOAG6vq/EnHMkJ7V9UewHOAQ5P8Yn/iYv98LmUz1CvvAx4D7AZcR3fad5wW7Gep9TP9FeBfW9Gkj9U6Ftr/WZI/Au4CPtKKrgMeUVW707qcJdly2PXNw/4tqPdrmpey7g+vSR+rGS2nBHkpPzr1hqnTtu3vja18Ue5zkk3ovsQ+UlUfb8VLah8Bqupm4CzgKXSnBKce3NPfh5/sX5v+UOCm8UY6J08FfiXJlXSn6J4BvIuls39U1bXt743AKXQ/cpbc53PExn5cZqpXquqG9oP1HuCDdO/lbPHNa9zz9Fka1bF8DnBBVd3QYpzosWrm69hcy7pdIe5XbEleCTwPeFlL1mhdGG5qw+fTnSl87Hq2PWj/5mwe36+bGFx/z1lbz68CJ/ZineixGmQ5JchL+dGppwJTV+ceBHyyV/6KdPYCbumdkliQWt+kY4CvVdU7epOWxD4mWZF7r3J+EPAsuv6QZwEvbLNN37+p/X4h8LmpCnghqqq3VNWOVbWS7n/sc1X1MpbI/iV5SJItpobpLtC5hCXy+RyjsdbHg+qVaf3BX0D3XtJiOTDdXVYeBexCd7HQvMU9j5+l/wCeneRh7TT6s1vZ/bVOK98kj1XPvBybNu3WJHu1z8YreuuakyT70nUp+5Wq+kGvfEWSjdrwo+mOyxXr2fag/duQuObl/Wr18aD6e0P8EvD1qvpJ14lJH6uBap6v+lvIL7orXf+H7tfJH006ng3chxPoTkfcSdcf52C6PkJnApcDnwW2bvMG+Pu2v18FVk06/iH2b2+6UyUXAxe2135LZR+BJwBfaft3CfdexftoukpqDd0pzU1b+WZtfE2b/uhJ78Mc9nUf7r2LxZLYv7YfF7XXpVP1yFL5fI75WI6tPp6lXvlwe18upvvC3b63zB+12L5B7w4H8xX3fH6W6Pq+rmmvV83D8XoIXcvhQ3tlYz1WzNN33aBjA6yiq4O/CbyX9mThDYhpDV3f3anP1dRdeX6tva8XAhcA+69v24P2bwPjmrf3iwH191xjauUfAn572rxjO1ZzefmoaUmSJKlnOXWxkCRJktbLBFmSJEnqMUGWJEmSekyQJUmSpB4TZEmSJKnHBFkjleTuJBcmuTTJRUnemGRkn7skq5K8ez3zrEzy63NZZg7bvzLJtvOxrgHrf2WSh49re5IWpyTbtLr3wiTXJ7m2N/7ACcX030PMc8Rclxly2x9K8sL1z7nB698nyS+Ma3savY3XP4t0v/ywuscqk+SngH8BtgT+bL43lGTjqloNrF7PrCuBX2+xMOQyC8Ur6e4J+e0JxyFpAavuyWS7ASQ5Eri9qv5manqrL+8aRyxT26qqX1j/3BwB/OXUyJDLLAT7ALcD85LQa/JsQdbYVPc41UOA17YnHm2U5K+TnJfk4iSvge4JQEm+2Fo6LknytFa+b5ILWkv0ma3syCQfTvJfwIfbr/hPTZv2pSSXJ3l1C+Uo4Glt/b8/bZmtk3yixfPlJE/orevYJJ9PckWS3xt2v9tTgj7W9vO8JE9d3zqT/EmSbyQ5O8kJSd7UWiNWAR9psT+ozX5YOy5fTfK4DX1/JC1trVXz/UnOAf4qyZ6tfvxKkv9O8tNtvlcm+XiSf29151+18o3aOi5p9c3vt/Kdk3y21c0XJHlMq1f/M8mpwGVtvtvb331aHf/pVs+9P8kDkhwFPKjVbx+Ztkza98XUtl/SW9fnk5yc5OtJPpIkQx6PQd9BA9eZZL9Wdn6Sdyf5VJKVwG8Dv99if1rbxC+243pFbE1edGxB1lhV1RXpHin5U8ABdI8EfVKSTYH/SnI63XPa/6Oq3tbmfXCSFXTPk//FqvpWkq17q90V2Luqfphkn2mbfAKwF92Tob6S5NPA4cCbqup50FWGvfnfCnylqp6f5BnA8bRWGOBxwNOBLYBvJHlfVd05xG6/C3hnVZ2d5BF0jz/9mUHrbNv7NeCJwCZ0TxY6v6pOTvLaFvvqFjvAd6pqjyS/C7wJ+K0hYpK0PO0I/EJV3Z1kS+BpVXVXkl+ia7n9tTbfbsDuwB10ddN76OrtHarqZwGSbNXm/QhwVFWdkmQzusa3nYA9gJ+tqm/NEMeedHX3VcC/A79aVYcnee3UWcdpfrXF9ERgW+C8JF9s03YHHk93Zu2/gKcCZw9xLA5m5u+gGdeZZDXwAe79HjoBoKquTPJ+eq30SQ4Gtqd7iuPj6J5md/IQMWmBMEHWJD0beELvl/VD6Z7Bfh5wbJJNgE9U1YUtif3iVEVbVd/trefUqvrhgG18sk37YZKz6Crlm2eJaW/aF0RVfS5dP74t27RPV9UdwB1JbgS2o3uE5vr8ErBrr1FjyySbz7LOp7a4fwT8KMlp61n/x9vf8+m+RCRpkH+tqrvb8EOB45LsQvco7k16851ZVbcAJLkMeCTd44Af3ZLlTwOnJ9mCLmk+BaDVW1M/3s8dkBxPTbuizXsCXd07WwK5N3BCi/2GJF8AngTc2tZ1TVvXhXTd6IZJkAd9B/14wDpvB67o7dMJdGdFB/lEVd0DXJZkuyHi0QJigqyxSvJo4G7gRiDAYVX1HzPM94vAc4EPJXkH8L1ZVvv9WaZNf5b6/Xm2+h294bsZ/v/nAcBeU18cU9oXyIauc6a4NnR5SctHv778c+CsqnpB6ybw+d60+9RNVfW9JE8EfpmuS8GLgdcNua3pFkLdPON3UGuQmc+6eWpbWkTsg6yxad0k3g+8t6qKrqvB77SWYpI8NslDkjwSuKGqPgj8I91pui/T9ed6VJt36xk3cl8HJNksyTZ0F1GcB9xG16VhJv8JvKxtYx+67gu3znVfpzkdOGxqJMlu65n/v4D9W9ybA8/rTZstdkmai4cC17bhV65v5nR3zHlAVX0M+GNgj6q6DbgmyfPbPJsmefAQ294zyaPS3dXoJdzb4nvn1HfCNP8JvKT1G14B/CJw7hDbmc2M30GzzP8Nuhb0lW38Jb1p1s1LjK1NGrUHtdNTmwB3AR8G3tGm/SPdaasL2gUQa4Hn0yWyf5DkTrpTWq+oqrVJDgE+3irUG4FnDbH9i4Gz6Pqs/XlVfTvJWuDuJBcBHwK+0pv/SLruHRcDPwAO2oB9vjjJPW34JOD3gL9v69wY+CJd68uMquq8dBe2XAzcAHwVuKVN/hDw/iQ/BJ6yAbFJ0pS/outi8cd0XSbWZwfgn3LvrTrf0v6+HPhAkv8H3Am8aIh1nQe8F9iZro4+pZUfTVeHXlBVL+vNfwpdnXcRXWvzm6vq+sztwuQPJPm7Nnw1XXe2ldz3O2hG7TqX3wX+Pcn32z5MOQ04OckB9BpEtHila8iTlp7McGujxSLJ5lV1e2uJ+SJwSFVdMOm4JOn+amfnfnKh9GLSq5sD/D1weVW9c9Jxaf7ZxUJamI5uLe8XAB8zOZakBeHVrW6+lK6LygcmG45GxRZkSZIkqccWZEmSJKnHBFmSJEnqMUGWJEmSekyQJUmSpB4TZEmSJKnn/wc2QuInxXvjUwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_histogram(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function <code>get_sentence_word_count</code> takes a list of texts as input and calculates the total number of sentences and unique words within the given texts. It achieves this by tokenizing the texts into sentences and words, and then iteratively counting the sentences and updating a vocabulary dictionary. The number of sentences provides an understanding of the overall length and complexity of the text. This can be useful for determining the granularity of text classification, similar to this project. Moreover, the number of unique words represents the vocabulary size of the text data. This indicates the diversity and richness of the language used in the transcriptions. Larger vocabulary sizes can be indicative of more varied topics or specialized domain-specific terminology. This code is valuable for gaining insights into the text data found in the <code>transcriptions</code> column. By determining the sentence count and word count, it provides an understanding of the text's structure and lexical richness. \n",
    "\n",
    "**The results obtained from the code indicate that there are 140,208 sentences and 35,805 unique words in the <code>transcriptions</code> column of the text data. Since this dataset contains a large number fo sentences, models such as Transformers can be a viable choice since they can handle large sequential data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in transcriptions column: 140208\n",
      "Number of unique words in transcriptions column: 35805\n"
     ]
    }
   ],
   "source": [
    "def get_sentence_word_count(text_list):\n",
    "    sent_count = 0\n",
    "    word_count = 0\n",
    "    vocab = {}\n",
    "    for text in text_list:\n",
    "        sentences  = sent_tokenize(str(text).lower())\n",
    "        sent_count = sent_count + len(sentences)\n",
    "        for sentence in sentences:\n",
    "            words  = word_tokenize(sentence)\n",
    "            for word in words:\n",
    "                if(word in vocab.keys()):\n",
    "                    vocab[word] = vocab[word] +1\n",
    "                else:\n",
    "                    vocab[word] = 1 \n",
    "    word_count = len(vocab.keys())\n",
    "    return sent_count,word_count\n",
    "\n",
    "clinical_text_df = data[data['transcription'].notna()]\n",
    "sent_count, word_count = get_sentence_word_count(clinical_text_df['transcription'].tolist())\n",
    "\n",
    "print('Number of sentences in transcriptions column: '    + str(sent_count))\n",
    "print('Number of unique words in transcriptions column: ' + str(word_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in transcriptions column: 129744\n",
      "Number of unique words in transcriptions column: 35088\n"
     ]
    }
   ],
   "source": [
    "data_categories  = clinical_text_df.groupby(clinical_text_df['medical_specialty'])\n",
    "filtered_data_categories = data_categories.filter(lambda x:x.shape[0] > 50)\n",
    "final_data_categories = filtered_data_categories.groupby(filtered_data_categories['medical_specialty'])\n",
    "sent_count, word_count = get_sentence_word_count(filtered_data_categories['transcription'].tolist())\n",
    "\n",
    "print('Number of sentences in transcriptions column: '    + str(sent_count))\n",
    "print('Number of unique words in transcriptions column: ' + str(word_count))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <code>Question 2</code>. Draw a block diagram to illustrate an overview of your experiment.  Each block represents a task required in a deep learning experiment such as vectorization, data splitting, etc.\n",
    "\n",
    "The figure below illustrates a comprehensive pipeline for NLP text classification in the domain of medical transcription. This pipeline encompasses crucial steps necessary for achieving accurate classification results, including data preprocessing, tokenization, random oversampling, dataset iteration involving shuffle and batching, dataset splitting, embeddings, LSTM classification, and evaluation metrics. By providing a systematic and structured approach, this pipeline serves as a valuable guide for viewers, enabling them to effectively address the challenges specific to the medical transcription domain.\n",
    "\n",
    "**Here, we first create the codes through classes and functions on how to use NLP for medical specialty classification.**\n",
    "\n",
    "To initiate the experiment, we load the given dataset and proceed with a rigorous data preprocessing phase. This preprocessing step focuses on cleaning the data by removing irrelevant information and handling missing values. Additionally, duplicate samples and those with missing values are eliminated from the dataset. In cases where duplicates with different labels exist, we retain the first label indicated in the dataset. \n",
    "\n",
    "It is important to note that the classification task involves 40 distinct labels derived from the mtsamples dataset. However, having 40 classes for text classification presents certain disadvantages, including increased complexity and data sparsity. Due to the rarity of some classes or limited availability of training samples, the model may not have sufficient data to effectively learn the distinguishing features of these classes. To address this, we apply a filtering approach to remove classes with sample counts lower than the median value of 130, as determined by a non-parametric calculation. This filtering process is denoted in the code snippet as <code>median_count</code>. By implementing this filtering approach, we aim to ensure that the model receives an adequate amount of data for each class, facilitating effective learning and improving classification performance. Removing classes with insufficient samples helps mitigate the challenges associated with data sparsity and allows the model to focus on the more well-represented classes, leading to a more balanced and robust classification system. \n",
    "\n",
    "After data preprocessing, the modified dataset <code>mtsamples_modified.csv</code> containing 9 labels undergoes tokenization. This involves breaking down the text into individual words, denoted as tokens. This step is fundamental as it provides a granular representation of the text, enabling further processing and analysis. Tokenization lays the foundation for subsequent steps in the pipeline by converting the textual data into a format that can be effectively understood and processed by the model. We created a class called <code>TokenizationProcessor</code> that takes the modified <code>mtsamples</code> as input in a DataFrame format and automatically tokenize it using the pre-trained BERT tokenizer. We will discuss more about this later.\n",
    "\n",
    "\n",
    "<center>\n",
    "<img src = '../figures/framework_super.png' width = '1800'/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_columns = ['description', 'medical_specialty', 'sample_name', 'transcription', 'keywords']\n",
    "reduced_df = filtered_data_categories[df_columns]\n",
    "for col in df_columns:\n",
    "    reduced_df = reduced_df.drop(reduced_df[reduced_df[col].isna()].index)\n",
    "\n",
    "low_median = []\n",
    "median_count = np.median(reduced_df['medical_specialty'].value_counts())\n",
    "for item in reduced_df['medical_specialty'].value_counts().items():\n",
    "    if item[1] <= median_count:\n",
    "        low_median.append(item[0])\n",
    "reduced_df = reduced_df[~reduced_df['medical_specialty'].isin(low_median)]\n",
    "reduced_df = reduced_df[ reduced_df['medical_specialty'] != ' SOAP / Chart / Progress Notes']\n",
    "reduced_df.to_csv('../data/mtsamples_modified.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following features were eliminated from the original dataset since the number of their samples are less than the median sample count. By implementing this filtering approach, we aim to ensure that the model receives an adequate amount of data for each class, facilitating effective learning and improving classification performance. Removing classes with insufficient samples helps mitigate the challenges associated with data sparsity and allows the model to focus on the more well-represented classes, leading to a more balanced and robust classification system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From the original dataset, we have reduced the mtsamples data to 2738 samples and 5 features.      These features still incldues description, medical_specialty, sample_name, and transcription\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' Obstetrics / Gynecology',\n",
       " ' ENT - Otolaryngology',\n",
       " ' Neurosurgery',\n",
       " ' Ophthalmology',\n",
       " ' Discharge Summary',\n",
       " ' Nephrology',\n",
       " ' Hematology - Oncology',\n",
       " ' Pain Management',\n",
       " ' Pediatrics - Neonatal',\n",
       " ' Emergency Room Reports',\n",
       " ' Psychiatry / Psychology']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'From the original dataset, we have reduced the mtsamples data to {reduced_df.shape[0]} samples and {reduced_df.shape[1]} features.\\\n",
    "      These features still incldues description, medical_specialty, sample_name, and transcription')\n",
    "low_median"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <code>preprocessing</code> function takes a sentence, removes hyperlinks, performs various token-level filters (removing stop words, symbols, punctuation marks, and whitespace), lemmatizes the remaining tokens to their base forms, and returns the cleaned sentence as a string. Specifically, the code <code>token.pos_ != 'SYM' and token.pos_ != 'PUNCT' and token.pos_ != 'SPACE'</code> checks if the token's part-of-speech (POS) tag is not 'SYM' (symbol), 'PUNCT' (punctuation), or 'SPACE'. It further filters out tokens that are symbols, punctuation marks, or represent whitespace. We also appended the lowercase lemma (base form) of the token, obtained using <code>token.lemma_</code>, to the cleaned_tokens list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_hyperlinks(sentence):\n",
    "    ''' \n",
    "    Parameters: sentence (string)\n",
    "    Returns a string with removed hyperlinks and other punctuation marks\n",
    "    '''\n",
    "    sentence = re.sub(\n",
    "        '(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\"', \" \", sentence)\n",
    "    return sentence\n",
    "\n",
    "def preprocessing(sentence):\n",
    "    ''' \n",
    "    Removes hyperlinks, performs various token-level filters (removing stop words, \n",
    "    symbols, punctuation marks, and whitespace), lemmatizes the remaining tokens to their base forms, and returns \n",
    "    the cleaned sentence as a string\n",
    "    Parameters: sentence (string)\n",
    "    '''\n",
    "    sentence = remove_hyperlinks(sentence)\n",
    "    doc = nlp(sentence)\n",
    "    cleaned_tokens = []\n",
    "    for token in doc:\n",
    "        if token.is_stop == False and \\\n",
    "            token.pos_ != 'SYM' and \\\n",
    "            token.pos_ != 'PUNCT' and token.pos_ != 'SPACE':\n",
    "            cleaned_tokens.append(token.lemma_.lower().strip())\n",
    "    return ' '.join(cleaned_tokens)\n",
    "\n",
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    ''' \n",
    "    This function places a label of -100 to the special tokens. By default, -100 is an index that is ignored in the \n",
    "    loss function we will use (cross entropy). Then, each token gets the same label as the token that started the \n",
    "    word it's inside, since they are part of the same entity. \n",
    "    Parameters:\n",
    "        labels\n",
    "        word_ids\n",
    "    '''\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            label = labels[word_id]\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "    return new_labels\n",
    "\n",
    "def tokenize_and_align_labels(tokenizer, examples):\n",
    "    tokenized_inputs = tokenizer(examples['tokens'], \n",
    "                                 truncation = True, \n",
    "                                 is_split_into_words = True)\n",
    "    all_labels = examples['ner_tags']\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "    tokenized_inputs['labels'] = new_labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "def to_tokens(tokenizer, sentence):\n",
    "    inputs = tokenizer(sentence)\n",
    "    return tokenizer.convert_ids_to_tokens(inputs.input_ids)\n",
    "\n",
    "def load_preprocessing(path = '../data/mtsamples_modified.csv', preprocess = False):\n",
    "    df = pd.read_csv(path)\n",
    "    if preprocess:\n",
    "        df = load_dataset('csv', data_files = {'../data/mtsamples_modified.csv'}, streaming = True)\n",
    "        for i, row in df.iterrows():\n",
    "            df.at[i, 'description']   = preprocessing(row['description'])\n",
    "            df.at[i, 'medical_specialty'] = preprocessing(row['medical_specialty'])\n",
    "            df.at[i, 'sample_name']   = preprocessing(row['sample_name'])\n",
    "            df.at[i, 'transcription'] = preprocessing(row['transcription']) if not pd.isnull(row['transcription']) else np.NaN  \n",
    "            df.at[i, 'keywords']      = preprocessing(row['keywords']) if not pd.isnull(row['keywords']) else np.NaN  \n",
    "    return df\n",
    "\n",
    "def split_data(df):\n",
    "    shuffle = df.sample(frac = 1, random_state = 42)\n",
    "\n",
    "    train_data,  test_data = train_test_split(shuffle,    test_size = 0.30, random_state = 42)\n",
    "    train_data, valid_data = train_test_split(train_data, test_size = 0.15, random_state = 42) \n",
    "\n",
    "    train_data.to_csv('../data/train.csv', index = False)\n",
    "    valid_data.to_csv('../data/valid.csv', index = False)\n",
    "    test_data. to_csv('../data/test.csv' , index = False)\n",
    "\n",
    "    data_files = {\n",
    "        'train': '../data/train.csv',\n",
    "        'valid': '../data/valid.csv',\n",
    "        'test' : '../data/test.csv'}\n",
    "    dataset = load_dataset('csv', data_files = data_files, streaming = True)\n",
    "    return dataset \n",
    "\n",
    "def compute_review_length(example):\n",
    "    return {'review_length': len(example['transcription'].split())}\n",
    "\n",
    "def bert_tokenizer(df, use_special):\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    input_ids, attention_masks = [], []\n",
    "\n",
    "    if use_special:\n",
    "        for index, row in df.iterrows():\n",
    "            encoded_dict = tokenizer.encode_plus(\n",
    "                row['description'],\n",
    "                row['medical_specialty'],\n",
    "                row['sample_name'],\n",
    "                row['transcription'],\n",
    "                row['keywords'],\n",
    "                padding = 'max_length',\n",
    "                truncation = True,\n",
    "                return_attention_mask = True,\n",
    "                return_tensors = 'pt')\n",
    "            input_ids.append(encoded_dict['input_ids'])\n",
    "            attention_masks.append(encoded_dict['attention_mask'])\n",
    "        input_ids = torch.cat(input_ids, dim = 0)\n",
    "        attention_masks = torch.cat(attention_masks, dim = 0)\n",
    "\n",
    "    else:\n",
    "        for description in df['description']:\n",
    "            encoded_dict = tokenizer.encode_plus(\n",
    "                description,\n",
    "                add_special_tokens = True, \n",
    "                max_length = 512, \n",
    "                padding = 'max_length',\n",
    "                truncation = True,\n",
    "                return_attention_mask = True,\n",
    "                return_tensors = 'pt')\n",
    "            input_ids.append(encoded_dict['input_ids'])\n",
    "            attention_masks.append(encoded_dict['attention_mask'])\n",
    "        input_ids = torch.cat(input_ids, dim = 0)\n",
    "        attention_masks = torch.cat(attention_masks, dim = 0)\n",
    "    return input_ids, attention_masks\n",
    "\n",
    "def process(examples):\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"sentence\"], truncation = True, max_length = 512)\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <code>TokenizationProcessor</code> class facilitates tokenization of text data for NLP text classification tasks. Let's break down the code and understand its usage and purpose. This class is initialized with two parameters: <code>max_sequence_length</code> and <code>tokenizer</code>. The <code>max_sequence_length</code> specifies the maximum length of the tokenized sequences, and the tokenizer is an instance of the <code>BertTokenizer</code> class from the Hugging Face's transformers library, initialized with the <code>'bert-base-uncased'</code> model. \n",
    "\n",
    "We created the <code>preprocess</code> function that tokenizes a single example by using the tokenizer on the transcription text. It applies truncation to limit the sequence length to max_sequence_length, adds padding to make all sequences of equal length, and returns the tokenized transcription along with the attention mask and the corresponding medical specialty label. Ultimately, the <code>preprocess</code> function is applied to the streamed dataset using the map function, which tokenizes and preprocesses the data in batches using the function <code>tokenize_and_split</code>. The tokenized dataset is then shuffled using a buffer size of 10,000 and a seed of 42. The tokenized dataset is then processed to obtain the input IDs, attention masks, and token type IDs. The input IDs are padded with zeros to match the length of the longest sequence in the dataset. In pretrained BERT, the max_length is 512. The resulting tensors for input IDs, attention masks, and token type IDs are returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizationProcessor:\n",
    "    ''' \n",
    "    TokenizationProcessor\n",
    "    Parameters: max_sequence_length (integer) - maximum length of the sentence to tokenize. By default, max_length of BERT tokenizer is 512\n",
    "                tokenizer: The object that breaks texts into smaller tokens. The default is BERT Tokenizer - Transformer-based tokenizer. \n",
    "    Outputs: \n",
    "        input_ids\n",
    "        attention_masks\n",
    "        token_type_ids\n",
    "    '''\n",
    "    def __init__(self, max_sequence_length, tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "\n",
    "    def preprocess(self, example):\n",
    "        max_sequence_length = 256\n",
    "        tokenized_transcription = self.tokenizer(example['transcription'], \n",
    "                                                truncation=True, \n",
    "                                                max_length = self.max_sequence_length, \n",
    "                                                padding = 'max_length',\n",
    "                                                return_tensors = 'pt')\n",
    "        \n",
    "        return {'input_ids': tokenized_transcription['input_ids'],\n",
    "                'attention_mask': tokenized_transcription['attention_mask'],\n",
    "                'medical_specialty': example['medical_specialty']}\n",
    "\n",
    "    def tokenize_and_split(self, examples):\n",
    "        return self.tokenizer(examples['transcription'],\n",
    "                              truncation = True,\n",
    "                              max_length = self.max_sequence_length,\n",
    "                              return_overflowing_tokens = True)\n",
    "\n",
    "    def process_dataset(self, dataset_path):\n",
    "        dataset = load_dataset('csv', data_files = dataset_path)\n",
    "        dataset_streamed = load_dataset('csv', data_files = dataset_path, streaming = True)\n",
    "\n",
    "        tokenized_dataset = dataset_streamed.map(self.preprocess, batched = True, batch_size = 16)\n",
    "        tokenized_dataset = tokenized_dataset.shuffle(buffer_size = 10_000, seed = 42)\n",
    "\n",
    "        for split in dataset.keys():\n",
    "            assert len(dataset[split]) == len(dataset[split].unique('Unnamed: 0'))\n",
    "        dataset = dataset.rename_column(original_column_name = 'Unnamed: 0', new_column_name = 'patient_id')\n",
    "        tokenized_dataset = dataset.map(self.tokenize_and_split,\n",
    "                                        batched = True,\n",
    "                                        remove_columns = dataset['train'].column_names)\n",
    "\n",
    "        input_ids = np.array(tokenized_dataset['train']['input_ids'])\n",
    "        sequence_length = max(len(ids) for ids in input_ids)\n",
    "        input_ids = [ids + [0] * (sequence_length - len(ids)) for ids in input_ids]\n",
    "        input_ids = torch.tensor(input_ids)\n",
    "\n",
    "        attention_mask = tokenized_dataset['train']['attention_mask']\n",
    "        attention_mask = [mask + [0] * (sequence_length - len(mask)) for mask in attention_mask]\n",
    "        attention_mask = torch.tensor(attention_mask)\n",
    "\n",
    "        token_type_ids = tokenized_dataset['train']['token_type_ids']\n",
    "        token_type_ids = [mask + [0] * (sequence_length - len(mask)) for mask in token_type_ids]\n",
    "        token_type_ids = torch.tensor(token_type_ids)\n",
    "\n",
    "        return input_ids, attention_mask, token_type_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetText(df):\n",
    "    '''\n",
    "    Parameters: df (Dataframe)\n",
    "    Outputs: text, labels, num_classes\n",
    "    We primarily used this to acquire the labels and process  the labels for the DataLoader\n",
    "    '''\n",
    "    text_column = df['transcription'].astype('str')\n",
    "    label_encoder = LabelEncoder()\n",
    "    labels = label_encoder.fit_transform(df['medical_specialty'])\n",
    "    labels = torch.tensor(labels)\n",
    "    num_classes = len(label_encoder.classes_)\n",
    "    return text_column, labels, num_classes\n",
    "\n",
    "def TokenizeDataset(text_column, use_medical_tokenizer = False):\n",
    "    if not use_medical_tokenizer:\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    else:\n",
    "        token_path = '../data/tokenizer.json'\n",
    "        tokenizer = Tokenizer.from_file(token_path)\n",
    "        tokenizer = BertTokenizerFast(tokenizer_object = tokenizer)\n",
    "\n",
    "    encoded_inputs = tokenizer.batch_encode_plus(\n",
    "                        text_column.tolist(),\n",
    "                        max_length = 64,\n",
    "                        padding = 'max_length',\n",
    "                        truncation = True,\n",
    "                        return_attention_mask = True,\n",
    "                        return_tensors = 'pt')\n",
    "\n",
    "    input_ids = encoded_inputs['input_ids']\n",
    "    attention_mask = encoded_inputs['attention_mask']\n",
    "    token_type_ids = encoded_inputs['token_type_ids']\n",
    "    return input_ids, attention_mask, token_type_ids\n",
    "\n",
    "\n",
    "def TokenizeSentenceDataset(text_column, use_medical_tokenizer = False):\n",
    "    if not use_medical_tokenizer:\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    else:\n",
    "        token_path = '../data/tokenizer.json'\n",
    "        tokenizer = Tokenizer.from_file(token_path)\n",
    "        tokenizer = BertTokenizerFast(tokenizer_object=tokenizer)\n",
    "\n",
    "    sentences = [sent_tokenize(text) for text in text_column]\n",
    "    sentences = [sentence for sublist in sentences for sentence in sublist]\n",
    "\n",
    "    encoded_inputs = tokenizer.batch_encode_plus(\n",
    "        sentences,\n",
    "        max_length = 512,\n",
    "        padding = 'max_length',\n",
    "        truncation = True,\n",
    "        return_attention_mask = True,\n",
    "        return_tensors = 'pt')\n",
    "\n",
    "    input_ids = encoded_inputs['input_ids']\n",
    "    attention_mask = encoded_inputs['attention_mask']\n",
    "    token_type_ids = encoded_inputs['token_type_ids']\n",
    "    return input_ids, attention_mask, token_type_ids\n",
    "\n",
    "\n",
    "df = load_preprocessing(preprocess = False)\n",
    "text_column, labels, num_classes = GetText(df)\n",
    "input_ids, attention_mask, token_type_ids = TokenizeDataset(text_column, \n",
    "                                                            use_medical_tokenizer = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below defines a function <code>GENERATE_DATALOADER</code> that generates data loaders for a given set of input IDs, attention masks, and labels from the <code>TokenizationProcessor</code> class. The data loaders are used to efficiently load and process data in batches during training, validation, and testing phases. Particularly, the <code>GENERATE_DATALOADER</code> class takes input IDs, attention masks, labels, and the optional parameters (batch size and use_sampler with default values of 64 and True, respectively).\n",
    "\n",
    "If <code>use_sampler</code> is True, the code performs random oversampling to address class imbalance in the dataset. Othersie,  the code proceeds with the original dataset without oversampling. Here, we utilized the RandomOverSampler from the imbalanced-learn library to balance the number of samples for each class. The input IDs and attention masks are then concatenated, and oversampling is applied to both the concatenated features (X) and the labels (y). The oversampled dataset is then randomly split into training, validation, and testing using <code>random_split</code> from PyTorch. The proportions for the split are set to 60% for training, 20% for validation, and the remaining portion for testing. The resulting data loaders, including the training, validation, and testing data loaders, are returned by the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GENERATE_DATALOADER(input_ids, attention_mask, labels, batch_size = 64, use_sampler = True):\n",
    "    ''' \n",
    "    Generates the batch data loader for every split (i.e., train, valid, test)\n",
    "    Parameters:\n",
    "        input_ids (output from TokenizationProcessor().tokenize_dataset)\n",
    "        attention_mask (output from TokenizationProcessor().tokenize_dataset)\n",
    "        labels (output from GetText())\n",
    "        batch_size (integer), default value is 64\n",
    "        use_sampler (boolean): use RandomOverSampler or not\n",
    "    Outputs:\n",
    "        train_loader, valid_loader, test_loader\n",
    "    '''\n",
    "    if use_sampler:\n",
    "        oversampler = RandomOverSampler(random_state = 42)\n",
    "        X = np.concatenate((input_ids, attention_mask), axis = -1)\n",
    "        y = np.ravel(labels)\n",
    "\n",
    "        X_resampled, y_resampled = oversampler.fit_resample(X, y)\n",
    "\n",
    "        input_ids_resampled      = X_resampled[:, :input_ids.shape[1]]\n",
    "        attention_mask_resampled = X_resampled[:, input_ids.shape[1]:]\n",
    "        labels_resampled = y_resampled\n",
    "\n",
    "        dataset = TensorDataset(torch.tensor(input_ids_resampled),\n",
    "                                torch.tensor(attention_mask_resampled),\n",
    "                                torch.tensor(labels_resampled))\n",
    "        \n",
    "        train_size = int(0.6 * len(dataset))\n",
    "        valid_size = int(0.2 * len(dataset))\n",
    "        tests_size = len(dataset) - train_size - valid_size\n",
    "        train_dataset, valid_dataset, test_dataset = random_split(dataset, [train_size, valid_size, tests_size])\n",
    "        \n",
    "    else:\n",
    "        dataset = TensorDataset(torch.tensor(input_ids), \n",
    "                                torch.tensor(attention_mask), \n",
    "                                torch.tensor(labels))\n",
    "        \n",
    "        train_size = int(0.8 * len(dataset))\n",
    "        valid_size = int(0.1 * len(dataset))\n",
    "        tests_size = len(dataset) - train_size - valid_size\n",
    "\n",
    "        train_dataset, valid_dataset, test_dataset = random_split(dataset, [train_size, valid_size, tests_size])\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        sampler = RandomSampler(train_dataset),\n",
    "        batch_size = batch_size)\n",
    "    validation_dataloader = DataLoader(\n",
    "        valid_dataset,\n",
    "        sampler = SequentialSampler(valid_dataset),\n",
    "        batch_size = batch_size)\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset,\n",
    "        sampler = SequentialSampler(test_dataset),\n",
    "        batch_size = batch_size)\n",
    "    return train_dataloader, validation_dataloader, test_dataloader\n",
    "\n",
    "class NLPDATASET(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        sequence = self.sequences[index]\n",
    "        label = self.labels[index]\n",
    "        return sequence, label"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is a medical specialty classification based on transcription texts, we utilized a deep learning-based models that can handle sequential data. The code below shows a simple LSTM-based model that only utilizes one LSTM layer and one fully-connected (dense) layer. The <code>LSTMClassifier</code> serves as the baseline classifier for the medical specialty classification task. We also created a slightly improved version called <code>LSTMClassifierModified</code> and <code>LSTMClassifierExtended</code> which can be helpful for comparisons.\n",
    "\n",
    "We created the function <code>count_parameters</code> so that we can determine the number of parameters for each layer. With this, we can evaluate the complexity and size of the model. Suppose <code>LSTMClassifier</code> has an input size, hidden size, and output size of 128, 50, and 2, respectively. Hence, the LSTM model only has 36102 parameters - from which considerably smaller compared to other pretrained LSTM models found in the PyTorch library. This information is useful for selecting the most appropriate model and designing an efficient architecture that balances effectiveness with computational and training requirements. If a model has too many parameters, it may overfit or require excessive resources, while too few parameters may not capture the complexity of the data adequately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   25600\n",
      "   10000\n",
      "     200\n",
      "     200\n",
      "     100\n",
      "       2\n",
      "________\n",
      "   36102\n"
     ]
    }
   ],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        _, (hidden, _) = self.lstm(inputs)\n",
    "        hidden = hidden.squeeze(0)  \n",
    "        output = self.fc(hidden)\n",
    "        return output\n",
    "    \n",
    "class LSTMClassifierModified(nn.Module):\n",
    "    def __init__(self, in_features, hidden_size, out_features):\n",
    "        super(LSTMClassifierModified, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(in_features, hidden_size)\n",
    "        self.fc2 = torch.nn.Linear(hidden_size, 32)\n",
    "        self.fc3 = torch.nn.Linear(32, out_features)\n",
    "                \n",
    "    def forward(self, inputs):\n",
    "        x = F.relu(self.fc1(inputs.squeeze(1)))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        logits = self.fc3(x)\n",
    "        probs = F.relu(logits)\n",
    "        return probs\n",
    "    \n",
    "class LSTMClassifierExtended(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTMClassifierExtended, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        _, (hidden, _) = self.lstm(inputs)\n",
    "        hidden = hidden[-1] \n",
    "        output = self.fc(hidden)\n",
    "        return output\n",
    "    \n",
    "def count_parameters(model, print_all = True):\n",
    "    ''' \n",
    "        Arguments:\n",
    "            model: Deep learning to be analyzed in terms of number of parameters\n",
    "            print_all: Print the number of parameters for each layer\n",
    "        Returns:\n",
    "            number of parameters\n",
    "    '''\n",
    "    params = [p.numel() for p in model.parameters() if p.requires_grad]\n",
    "    if print_all:\n",
    "        for item in params:\n",
    "            print(f'{item:>8}')\n",
    "    print(f'________\\n{sum(params):>8}')\n",
    "\n",
    "input_size, hidden_size, output_size = 128, 50, 2\n",
    "count_parameters(LSTMClassifier(input_size, hidden_size, output_size), print_all = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BERT_EMBEDDING(input_ids, attention_mask, token_type_ids):\n",
    "    bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "    bert_model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(input_ids = input_ids, \n",
    "                             attention_mask = attention_mask, \n",
    "                             token_type_ids = token_type_ids)\n",
    "        bert_embeddings = outputs.last_hidden_state\n",
    "\n",
    "    batch_size = bert_embeddings.size(0)\n",
    "    sequence_length = bert_embeddings.size(1)\n",
    "    bert_embeddings = bert_embeddings.view(batch_size, sequence_length, -1)\n",
    "    embeddings  = bert_embeddings.permute(1, 0, 2)\n",
    "    return bert_model, embeddings\n",
    "\n",
    "def ROBERTA_EMBEDDING(input_ids, attention_mask, token_type_ids):\n",
    "    model_name = 'roberta-base'\n",
    "    model = RobertaModel.from_pretrained(model_name)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids = input_ids,\n",
    "                        attention_mask = attention_mask,\n",
    "                        token_type_ids = token_type_ids)\n",
    "        embeddings = outputs.last_hidden_state\n",
    "\n",
    "    batch_size = embeddings.size(0)\n",
    "    sequence_length = embeddings.size(1)\n",
    "    embeddings = embeddings.view(batch_size, sequence_length, -1)\n",
    "    embeddings = embeddings.permute(1, 0, 2)\n",
    "    return model, embeddings\n",
    "\n",
    "def LSTM_BASELINES(bert_model, embeddings, basic_classifier = False):\n",
    "    input_size = bert_model.config.hidden_size\n",
    "    hidden_size, num_classes = 50, 9\n",
    "    if basic_classifier:\n",
    "        lstm_model   = LSTMClassifierModified(input_size, hidden_size, num_classes)\n",
    "    else:\n",
    "        lstm_model   = LSTMClassifier(input_size, hidden_size, num_classes)\n",
    "    lstm_output  = lstm_model(embeddings)\n",
    "    output_probs = nn.functional.softmax(lstm_output, dim = 0)\n",
    "    _, predicted_labels = torch.max(output_probs, dim = 0)\n",
    "    return output_probs, predicted_labels\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "def get_accuracy(preds, y):\n",
    "    batch_corr = (preds == y).sum()\n",
    "    acc = batch_corr / len(y)\n",
    "    return acc\n",
    "\n",
    "def eval_predictions(predicted_labels, labels):\n",
    "    true_labels = labels.numpy()\n",
    "    accuracy  = accuracy_score(true_labels,  predicted_labels)\n",
    "    precision = precision_score(true_labels, predicted_labels, average = 'weighted')\n",
    "    recall = recall_score(true_labels, predicted_labels, average = 'weighted')\n",
    "    f1 = f1_score(true_labels, predicted_labels, average = 'weighted')\n",
    "\n",
    "    print('Classification Metrics: ')\n",
    "    print(f'\\t Accuracy:  \\t {np.round(accuracy, 5)}')\n",
    "    print(f'\\t Precision: \\t {np.round(precision, 5)}')\n",
    "    print(f'\\t Recall:    \\t {np.round(recall, 5)}')\n",
    "    print(f'\\t F1-score:  \\t {np.round(f1, 5)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(train_losses, valid_losses, train_accurs, valid_accurs):\n",
    "    alpha = 0.3\n",
    "    smoothed_train_losses = [train_losses[0]]\n",
    "    smoothed_valid_losses = [valid_losses[0]]\n",
    "    smoothed_train_accurs = [train_accurs[0]]\n",
    "    smoothed_valid_accurs = [valid_accurs[0]]\n",
    "    \n",
    "    for i in range(1, len(train_losses)):\n",
    "        smoothed_train_losses.append(alpha * train_losses[i] + (1-alpha) * smoothed_train_losses[-1])\n",
    "        smoothed_valid_losses.append(alpha * valid_losses[i] + (1-alpha) * smoothed_valid_losses[-1])\n",
    "        smoothed_train_accurs.append(alpha * train_accurs[i] + (1-alpha) * smoothed_train_accurs[-1])\n",
    "        smoothed_valid_accurs.append(alpha * valid_accurs[i] + (1-alpha) * smoothed_valid_accurs[-1])\n",
    "    \n",
    "    smoothed_train_losses = train_losses\n",
    "    smoothed_train_accurs = train_accurs\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (12, 5))\n",
    "    ax1.plot(smoothed_train_losses, label = 'Train')\n",
    "    ax1.plot(smoothed_valid_losses, label = 'Valid')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title('Losses')\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2.plot(smoothed_train_accurs, label='Train')\n",
    "    ax2.plot(smoothed_valid_accurs, label='Valid')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.set_title('Accuracies')\n",
    "    ax2.legend()\n",
    "    plt.show()\n",
    "\n",
    "def _train(model, loader, optimizer, criterion, batch_size = 16, device = 'cpu'):\n",
    "    epoch_train_loss = 0\n",
    "    epoch_train_accu = 0\n",
    "    model.train()\n",
    "    epoch_train_prediction = []\n",
    "\n",
    "    for idx, data in enumerate(loader):\n",
    "        inputs, attens, labels = data\n",
    "        inputs, attens, labels = inputs.to(device), attens.to(device), labels.to(device, dtype = torch.long)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(input_ids = inputs, attention_mask = attens)\n",
    "        embedds = outputs.last_hidden_state\n",
    "\n",
    "        batch_size, seq_length = embedds.size(0), embedds.size(1)\n",
    "        embeddings = embedds.view(batch_size, seq_length, -1)\n",
    "        embeddings = embeddings.permute(1, 0, 2)\n",
    "\n",
    "        input_size = model.config.hidden_size\n",
    "        hidden_size, num_classes = 50, 9\n",
    "\n",
    "        lstm_model = LSTMClassifier(input_size, hidden_size, num_classes)\n",
    "        lstm_output  = lstm_model(embeddings)\n",
    "        loss = criterion(lstm_output, labels)\n",
    "\n",
    "        output_probs = nn.functional.softmax(lstm_output, dim = 0)    \n",
    "        _, predicted_labels = torch.max(output_probs, dim = 1) \n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "          \n",
    "        epoch_train_prediction.append(predicted_labels)\n",
    "        accuracy = get_accuracy(predicted_labels, labels) \n",
    "        loss = np.round(loss.item(), 3)\n",
    "        epoch_train_loss += loss.item()\n",
    "        epoch_train_accu += accuracy.item()\n",
    "    epoch_train_loss = epoch_train_loss / len(loader)\n",
    "    epoch_train_accu = epoch_train_accu / len(loader)\n",
    "    return epoch_train_loss, epoch_train_accu, epoch_train_prediction\n",
    "    \n",
    "def _evals(model, loader, criterion, batch_size = 64, device = 'cpu', display = False):\n",
    "    epoch_valid_loss = 0\n",
    "    epoch_valid_accu = 0\n",
    "    model.eval()\n",
    "    epoch_valid_prediction = []\n",
    "    with torch.no_grad():\n",
    "        for idx, data in enumerate(loader):\n",
    "            inputs, attens, labels = data \n",
    "            inputs, attens, labels = inputs.to(device), attens.to(device), labels.to(device,  dtype = torch.long)\n",
    "\n",
    "            outputs = model(input_ids = inputs, attention_mask = attens)\n",
    "            embedds = outputs.last_hidden_state\n",
    "\n",
    "            batch_size, seq_length = embedds.size(0), embedds.size(1)\n",
    "            embeddings = embedds.view(batch_size, seq_length, -1)\n",
    "            embeddings = embeddings.permute(1, 0, 2)  \n",
    "\n",
    "            \n",
    "            input_size = model.config.hidden_size\n",
    "            hidden_size, num_classes = 128, 9\n",
    "            lstm_model = LSTMClassifier(input_size, hidden_size, num_classes)\n",
    "            lstm_output  = lstm_model(embeddings)\n",
    "            loss = criterion(lstm_output, labels)\n",
    "            loss = np.round(loss.item(), 3)\n",
    "\n",
    "            output_probs = nn.functional.softmax(lstm_output, dim = 0)    \n",
    "            _, predicted_labels = torch.max(output_probs, dim = 1)   \n",
    "            epoch_valid_prediction.append(predicted_labels)\n",
    "            accuracy = np.round(get_accuracy(predicted_labels, labels), 5)\n",
    "            epoch_valid_loss += loss.item()\n",
    "            epoch_valid_accu += accuracy.item()\n",
    "    epoch_valid_loss = epoch_valid_loss / len(loader)\n",
    "    epoch_valid_accu = epoch_valid_accu / len(loader)\n",
    "    if display:\n",
    "        print(f'Loss: {loss} \\t Accuracy: {accuracy}')\n",
    "    return epoch_valid_loss, epoch_valid_accu, epoch_valid_prediction\n",
    "\n",
    "def train(num_epochs, model, train_loader, valid_loader, test_loader, optimizer, criterion, device, accuracy = True):\n",
    "    best_valid_loss = float('inf')\n",
    "    train_losses, valid_losses = [], []\n",
    "    train_accurs, valid_accurs = [], []\n",
    "    trainpredict, testspredict = [], []\n",
    "\n",
    "    epoch_times = []\n",
    "    list_best_epochs = []\n",
    "    start = time()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        train_loss, train_accu, tr_predict = _train(model, train_loader, optimizer, criterion, device)\n",
    "        valid_loss, valid_accu, ts_predict = _evals(model, valid_loader, criterion, device)\n",
    "        \n",
    "        if accuracy:\n",
    "            print(f'Epoch: {epoch + 1} \\t Training: Loss {np.round(train_loss, 5)}   \\t Accuracy: {np.round(train_accu, 5)} \\t Validation Loss  {np.round(valid_loss, 5)} \\t Accuracy: {np.round(valid_accu, 5)}')\n",
    "        else:\n",
    "            print(f'Epoch: {epoch + 1} \\t Training: Loss {np.round(train_loss, 5)} \\t Validation Loss  {np.round(valid_loss, 5)}')\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        train_accurs.append(train_accu)\n",
    "        valid_losses.append(valid_loss)\n",
    "        valid_accurs.append(valid_accu)\n",
    "        trainpredict.append(tr_predict)\n",
    "        testspredict.append(ts_predict)\n",
    "\n",
    "        end_time = time()\n",
    "        epoch_mins, epoch_secs = epoch_time(start, end_time)\n",
    "        \n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            best_model = copy.deepcopy(model)\n",
    "            best_epoch = epoch\n",
    "        list_best_epochs.append(best_epoch)\n",
    "    test_loss, test_accu, test_predict  = _evals(best_model, test_loader, criterion, device)\n",
    "    print(f'Training time: {np.round(time() - start, 4)} seconds')\n",
    "    print(f'Final Best Model from Best Epoch {best_epoch + 1} Test Loss = {test_loss}, Test Accuracy = {test_accu}')\n",
    "    return train_losses, valid_losses, train_accurs, valid_accurs, test_loss, test_accu, best_epoch, epoch_times, test_predict, best_model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <code>Question(s) 3 - 5</code>. Create the baseline model with BERT and LSTM \n",
    "Specific instructions:\n",
    "- Use BERT without fine tuning on the LSTM with 50 neurons for the baseline model. Other hyperparameters are not controlled. \n",
    "- Report the baseline results with appropriate metrics obtained from the baseline model. \n",
    "- Criticize the results from the baseline model based on theoretical and practical perspective with supporting literature.   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve Question 3-5, we first define a function called <code>main</code> that summarizes all of our training and inferences to this medical specialty classification task. This function takes input parameters <code>df, bert_model, train_process</code>. The <code>df</code> is acquired from the function <code>load_processing</code> which then takes as an input to <code>main</code> to extract data from the provided dataframe. The function returns three values but we only consider the second value <code>labels</code>. It then creates the <code>TokenizationProcessor</code> object with a maximum sequence length of 128 and a tokenizer initialized with the <code>bert-base-uncased</code> pretrained model. This object will tokenize the modified dataset using the aforementioned parameters. Then it calls the <code>GENERATE_DATALOADER</code> function, passing the input_ids, attention_mask, and labels tensors, and specifying use_sampler = True. This function generates three data loaders: train_loader, valid_loader, and test_loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(df, bert_model, train_process = False):\n",
    "    _, labels, _ = GetText(df)\n",
    "    start = time()\n",
    "    processor = TokenizationProcessor(max_sequence_length = 128, tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased'))\n",
    "    input_ids, attention_mask, token_type_ids = processor.process_dataset('../data/mtsamples_modified.csv')\n",
    "    if input_ids.size(0) != labels.size(0):    \n",
    "        desired_length = input_ids.size(0)\n",
    "        padding_length = desired_length - len(labels)\n",
    "        padding = torch.zeros(padding_length, dtype = torch.long)\n",
    "        labels  = torch.cat((labels, padding))\n",
    "\n",
    "    train_loader, valid_loader, test_loader = GENERATE_DATALOADER(input_ids, attention_mask, labels, use_sampler = True)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    if train_process:\n",
    "        num_epochs = 5\n",
    "        optimizer  = torch.optim.AdamW(bert_model.parameters(), lr = 0.001)\n",
    "        train_losses, valid_losses, train_accurs, valid_accurs, test_loss, test_accu, best_epoch, epoch_times, test_predict, best_model = train(num_epochs, bert_model, \n",
    "                                                                                                                                                train_loader, valid_loader, \n",
    "                                                                                                                                                test_loader, optimizer, criterion, \n",
    "                                                                                                                                                device, accuracy = True)\n",
    "        return train_losses, valid_losses, train_accurs, valid_accurs, test_loss, test_accu, best_epoch, epoch_times, test_predict, best_model\n",
    "    else:\n",
    "        epoch_valid_loss, epoch_valid_accu, epoch_valid_prediction = _evals(bert_model, \n",
    "                                                                            test_loader, \n",
    "                                                                            criterion, \n",
    "                                                                            batch_size = 64, \n",
    "                                                                            device = 'cpu', \n",
    "                                                                            display = False)\n",
    "        for idx, data in enumerate(test_loader):\n",
    "            _, _, labels = data \n",
    "            break\n",
    "    eval_predictions(epoch_valid_prediction[0], labels)\n",
    "    print(f'Fit and predict time: {np.round(time() - start, 4)} seconds')\n",
    "    return epoch_valid_loss, epoch_valid_accu, epoch_valid_prediction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 1: Baseline Model = BERT(Tokenizer + Embedding) + LSTM(50)\n",
    "\n",
    "As specified in the given instructions, we utilized the pretrained BERT without fine-tuning as a tokenizer and for word embeddings. As a tokenizer, BERT tokenizes raw text into subword units called WordPieces. It breaks down words into smaller meaningful subwords and assigns special tokens like <code>[CLS]</code> (for classification) and <code>[SEP]</code> (to separate sentences or segments). BERT uses a vocabulary that includes both whole words and subwords, allowing it to handle out-of-vocabulary words and capture fine-grained subword information. In addition, BERT, as an embedding model, can generate dense vector representations (embeddings) for each token in the input text. These embeddings capture contextual information, taking into account the surrounding words in a sentence or a sequence. BERT's embeddings are powerful because they are pretrained on a large corpus of text using a masked language modeling objective, enabling them to capture rich semantic and syntactic information. We then used the embedding vectors as an input to the <code>LSTMClassifier</code> to classify the results based on their corresponding predictions.\n",
    "\n",
    "To execute this experiment, we utilized the function <code>main</code> that inputs dataframe and BERT model. We adopted the prebuilt frameworks from the HuggingFace and PyTorch libraries that provides the interface for BERT tokenizer and embeddings. Since we only utilized pre-trained BERT without fine-tuning, we don't necessarily train this model. Hence, we instantiate <code>train_process = False</code>. We adopted the default values initialized from the function <code>main</code> above.\n",
    "\n",
    "We evaluated the results using five classification metrics, including accuracy, precision, recall, F1-score, and AUC-ROC. Since our dataset is highly imbalanced, it is preferable to consider the F1-score and AUC-ROC as primary metrics for the results. Accuracy may not be sufficient on its own for imbalanced datasets where certain medical specialties may have fewer instances than others. In such cases, accuracy can be misleading because even a high accuracy score may not reflect the model's performance on the minority classes. Meanwhile, F1-score is often useful when there is an imbalance between the classes and a trade-off between precision and recall is desired. This metric considers both false positives and false negatives and provides a balanced evaluation of the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "WARNING:datasets.builder:Found cached dataset csv (C:/Users/Renan/.cache/huggingface/datasets/csv/default-b7921370535cb5b2/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 16.33it/s]\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at C:\\Users\\Renan\\.cache\\huggingface\\datasets\\csv\\default-b7921370535cb5b2\\0.0.0\\6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-bb432f5c21b2e9d1.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Metrics: \n",
      "\t Accuracy:  \t 0.7031\n",
      "\t Precision: \t 0.73228\n",
      "\t Recall:    \t 0.56248\n",
      "\t F1-score:  \t 0.59376\n",
      "Fit and predict time: 5720.4624 seconds\n"
     ]
    }
   ],
   "source": [
    "df = load_preprocessing(preprocess = False)\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "epoch_valid_loss, epoch_valid_accu, epoch_valid_prediction = main(df, bert_model, train_process = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion 1: Baseline Model = BERT + LSTM(50)\n",
    "\n",
    "The model's performance in this classification task presents a nuanced picture. While achieving an accuracy of 0.7031 is generally considered good, evaluating the precision and recall values provides deeper insights. The relatively high precision score (0.73228) indicates a low rate of false positives, suggesting that the model is often correct when predicting a positive class. However, the lower recall score (0.56248) implies that the model struggles to capture all positive instances, leading to a higher rate of false negatives. Despite this, the F1-score of 0.59376 suggests a reasonable balance between precision and recall.\n",
    "\n",
    "It is worth noting that the baseline model required a significant amount of time to run the simulation. The fit and predict time was measured to be 5720.4624 seconds (equivalent to approximately 1.58 hours). This duration represents the time taken to train the model on the available data and make predictions on unseen data. The lengthy simulation time aligns with previous studies <code>(Zhang et al., 2021, Lai et al., 2021)</code>, confirming that the BERT language model is computationally expensive due to its size, which consists of 109,482,240 parameters in the pretrained BERT. The immense size of BERT necessitates a considerable amount of computation, resulting in slower performance due to the multitude of weights that need to be updated.\n",
    "\n",
    "The pretrained BERT was trained on various corpus - including BooksCorpus with 800 million words and English Wikipedia with 2.5 billion words <code>(Vaswani et al., 2017; Devlin, et al., 2018)</code>. BERT's training on a mixture of general texts might not adequately capture the specialized vocabulary, terminology, and context specific to medical transcriptions. As a result, the model may struggle to understand and accurately represent medical concepts. In addition, BERT's knowledge is limited to what it learned from its training corpus. It may lack comprehensive understanding of specific medical conditions, treatments, and procedures that are critical for accurate transcription analysis in the medical domain. The model might not fully grasp the nuanced meanings or relationships between medical terms, potentially leading to inaccuracies or misinterpretations. \n",
    "\n",
    "In a recent study by <code>Gao et al. (2021)</code>, they highlighted the limitations of BERT when it comes to clinical text classification. Utilizing BERT models for lengthy clinical text poses challenges due to the complexities of adapting BERT. One challenge is that BERT has a maximum token limit of 512 WordPiece tokens, roughly equivalent to 400 words. Unfortunately, the <code>mtsamples</code> dataset consistently exceeds this limit as the average transcription text consists of approximately 700 word tokens. Another challenge arises from the need to pretrain BERT on a text corpus that aligns with the domain of the downstream application task to achieve optimal performance. This means that clinical practitioners who desire to employ BERT but lack the computational resources or data required to pretrain their own models must resort to downloading existing pretrained models like BlueBERT.\n",
    "\n",
    "BlueBERT <code>(Peng et al., 2019)</code>, specifically trained on biomedical text from the PubMed corpus, serves as a valuable alternative. Through extensive training on a substantial collection of biomedical literature, including PubMed articles, BlueBERT has acquired the ability to comprehend the contextual representations of words and sentences. As a result, BlueBERT excels at capturing domain-specific knowledge and semantic understanding within biomedical texts.\n",
    "\n",
    "While existing studies has exposed the computational inefficiency of BERT to medical text classification, recent studies have also focused on developing BERT-based and non-BERT-based models. \n",
    "\n",
    "Combining BERT for word embeddings with LSTM for classification has proven to yield good results in this classification task. Nonetheless, its results can still be greatly improved. The performance achieved by this model will serve as our baseline for future comparisons with other models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <code>Question(s) 6 - 8</code> Propose the better model with supporting assumptions why your proposed model should be better than the baseline model.  \n",
    "- Clear explanations are required in which aspect that you would like to improve from the baseline model.\n",
    "- For example, prediction performance, runtime, â€¦ etc. and why such aspects should be concerned in this problem.\n",
    "- Conduct an experiment on your proposed model.\n",
    "- Report the results obtained from your proposed model with appropriate metrics\n",
    "\n",
    "As mentioned in Discussion 1, we explored to other BERT-based models such as RoBERTa, DistilBERT, and BlueBERT to propose better model than our baseline model. We believe that the conventional BERT model may provide poor performance due to its size, complexity, and inefficiency. BERT was pretrained on a large corpus of general domain text, such as Wikipedia articles, which might not fully capture the nuances and intricacies of medical terminology and context. In contrast, RoBERTa, DistilBERT, and BlueBERT have been pretrained on domain-specific data, with RoBERTa and BlueBERT specifically trained on biomedical text. This targeted pretraining allows these models to better understand medical jargon and grasp the context unique to the medical field. Moreover, we also used DistilBERT as one of our proposed model since it is a distiled version of the conventional BERT that retains much of its performance while significantly reducing its size and computational requirements. All of these three models can become an efficient choice for medical specialty classification task, especially to our computational resources that are highly limited. \n",
    "\n",
    "To create an unbiased comparison among BERT, RoBERTa, DistilBERT, and BlueBERT, we still used our simplified <code>LSTMClassifier</code> as the baseline classifier of this medical specialty task. However, we utilized LSTMClassifier at different number of hidden layers. \n",
    "\n",
    "#### Experiment 2.1: RoBERTa Model + LSTM(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "WARNING:datasets.builder:Found cached dataset csv (C:/Users/Renan/.cache/huggingface/datasets/csv/default-6d6e490111dd0d23/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 190.06it/s]\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at C:\\Users\\Renan\\.cache\\huggingface\\datasets\\csv\\default-6d6e490111dd0d23\\0.0.0\\6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-edb2f1d343437643.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Metrics: \n",
      "\t Accuracy:  \t 0.75\n",
      "\t Precision: \t 0.76667\n",
      "\t Recall:    \t 0.75\n",
      "\t F1-score:  \t 0.66073\n",
      "Fit and predict time: 391.2881 seconds\n"
     ]
    }
   ],
   "source": [
    "df = load_preprocessing(preprocess = False)\n",
    "bert_model = RobertaModel.from_pretrained('roberta-base')\n",
    "epoch_valid_loss, epoch_valid_accu, epoch_valid_prediction = main(df, bert_model, train_process = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The precision score increased from 0.73228 (baseline model) to 0.76667, reflecting a better ability of the model to accurately classify positive instances. The recent precision score indicates that approximately 76.67% of the positive predictions are correct, demonstrating an improvement in avoiding false positives. In addition, the F1-score increased from 0.59376 to 0.66073, indicating an improvement in the model's overall performance. Ultimately, experiment 2.1's fit and predict time of 391.2881 seconds is significantly lower than the previous time of 5720.4624 seconds (baseline). This suggests a substantial improvement in the model's efficiency, as it now takes less time for training and making predictions. Overall, the recent result shows an improvement in accuracy, precision, F1-score, and efficiency compared to the baseline result. This further implies that using RoBERTa model can be utilized for this medical specialty task."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 2.2: DistilBERT Model + LSTM(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "WARNING:datasets.builder:Found cached dataset csv (C:/Users/Renan/.cache/huggingface/datasets/csv/default-6d6e490111dd0d23/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 496.07it/s]\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at C:\\Users\\Renan\\.cache\\huggingface\\datasets\\csv\\default-6d6e490111dd0d23\\0.0.0\\6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-edb2f1d343437643.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Metrics: \n",
      "\t Accuracy:  \t 0.84375\n",
      "\t Precision: \t 0.74519\n",
      "\t Recall:    \t 0.84375\n",
      "\t F1-score:  \t 0.73438\n",
      "Fit and predict time: 190.5946 seconds\n"
     ]
    }
   ],
   "source": [
    "df = load_preprocessing(preprocess = False)\n",
    "bert_model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "epoch_valid_loss, epoch_valid_accu, epoch_valid_prediction = main(df, bert_model, train_process = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of 0.8 in Experiment 2.2 shows an improvement compared to the baseline accuracy of 0.7031. The DistilBERT model has achieved a higher level of overall correctness in predicting the class labels. : The F1-score of 0.70225 in Experiment 2.2 is higher than the baseline F1-score of 0.59376, indicating a better balance between precision and recall. Moreover, experiment 2.2 simulated much less time than both the baseline model and experiment 2.1 - indicating an improved performance and efficiency with the DistilBERT.\n",
    "\n",
    "While the DistilBERT model shows improved performance compared to the baseline and Experiment 2.1 in terms of accuracy, precision, recall, and fit and predict time, it is important to consider that DistilBERT is a compressed and smaller version of the original BERT model. It achieves efficiency gains by reducing the number of layers, attention heads, and parameters. This reduction in model capacity may lead to a slight degradation in performance especially when we choose to fine-tune this model with our final proposed model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 2.3: RoBERTa Model + LSTM(128)\n",
    "\n",
    "In this experiment, we try to expand the LSTM to 128 neurons. We need to determine if this change can improve the classification performance. We still evaluated this LSTM(128) to three BERT_based models, i.e., RoBERTa, DistilBERT, and BlueBERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "WARNING:datasets.builder:Found cached dataset csv (C:/Users/Renan/.cache/huggingface/datasets/csv/default-6d6e490111dd0d23/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 331.91it/s]\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at C:\\Users\\Renan\\.cache\\huggingface\\datasets\\csv\\default-6d6e490111dd0d23\\0.0.0\\6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-edb2f1d343437643.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Metrics: \n",
      "\t Accuracy:  \t 0.23438\n",
      "\t Precision: \t 0.12773\n",
      "\t Recall:    \t 0.23438\n",
      "\t F1-score:  \t 0.16211\n",
      "Fit and predict time: 467.0552 seconds\n"
     ]
    }
   ],
   "source": [
    "df = load_preprocessing(preprocess = False)\n",
    "bert_model = RobertaModel.from_pretrained('roberta-base')\n",
    "epoch_valid_loss, epoch_valid_accu, epoch_valid_prediction = main(df, bert_model, train_process = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, the results of Experiment 2.3 indicate a significant drop in performance compared to the previous results. The F1-score of 0.16211 in Experiment 2.3 is substantially lower compared to the previous F1-scores. This suggests a poor balance between precision and recall, indicating that the model's performance is compromised in terms of both correctly identifying positive instances and avoiding false positives. The primary difference in Experiment 2.3 is the use of an LSTM with 128 neurons, compared to the previous experiments that used 50 neurons. The decrease in performance observed in Experiment 2.3 could potentially be attributed to this change in the architecture. The increased complexity and capacity of the LSTM with 128 neurons might have affected the model's ability to learn and capture meaningful patterns in the data effectively.\n",
    "\n",
    "Overall, Experiment 2.3 using the RoBERTa model with an LSTM of 128 neurons resulted in a significant decline in performance compared to the previous experiments. Let's try to the two remaining model if they also have significant drop in performance in using LSTM(128)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 2.4: DistilBERT Model + LSTM(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "WARNING:datasets.builder:Found cached dataset csv (C:/Users/Renan/.cache/huggingface/datasets/csv/default-6d6e490111dd0d23/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 253.45it/s]\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at C:\\Users\\Renan\\.cache\\huggingface\\datasets\\csv\\default-6d6e490111dd0d23\\0.0.0\\6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-edb2f1d343437643.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Metrics: \n",
      "\t Accuracy:  \t 0.85938\n",
      "\t Precision: \t 0.44868\n",
      "\t Recall:    \t 0.85938\n",
      "\t F1-score:  \t 0.5609\n",
      "Fit and predict time: 231.6236 seconds\n"
     ]
    }
   ],
   "source": [
    "df = load_preprocessing(preprocess = False)\n",
    "bert_model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "epoch_valid_loss, epoch_valid_accu, epoch_valid_prediction = main(df, bert_model, train_process = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment 2.4 utilizes the combination of DistilBERT and an LSTM with 128 neurons. This configuration has resulted in improved performance compared to previous experiments, especially to Experiment 2.4. The use of DistilBERT, which is a compressed version of BERT, coupled with the larger LSTM architecture seems to have positively impacted the model's ability to learn and capture meaningful patterns in the data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 2.5: BlueBERT Model + LSTM(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bionlp/bluebert_pubmed_uncased_L-12_H-768_A-12 were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "WARNING:datasets.builder:Found cached dataset csv (C:/Users/Renan/.cache/huggingface/datasets/csv/default-6d6e490111dd0d23/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 124.39it/s]\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at C:\\Users\\Renan\\.cache\\huggingface\\datasets\\csv\\default-6d6e490111dd0d23\\0.0.0\\6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-edb2f1d343437643.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Metrics: \n",
      "\t Accuracy:  \t 0.78125\n",
      "\t Precision: \t 0.59286\n",
      "\t Recall:    \t 0.78125\n",
      "\t F1-score:  \t 0.7107\n",
      "Fit and predict time: 444.3507 seconds\n"
     ]
    }
   ],
   "source": [
    "df = load_preprocessing(preprocess = False)\n",
    "bert_model = BertModel.from_pretrained('bionlp/bluebert_pubmed_uncased_L-12_H-768_A-12')\n",
    "epoch_valid_loss, epoch_valid_accu, epoch_valid_prediction = main(df, bert_model, train_process = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy and F1-score of 0.78125 and 0.7107, respectively, in Experiment 2.5 are moderately higher compared to some previous results. Moreover, the fit and predict time of 444.3507 seconds in Experiment 2.5 is comparable to the previous experiments - indicating that the model's efficiency remains consistent with the previous experiments. This configuration has shown improvements in accuracy, precision, recall, and F1-score compared to some previous experiments. BlueBERT, which is a domain-specific pre-trained model, may have captured domain-specific features and improved the model's ability to understand and classify medical specialty text."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 2.6: RoBERTa Model + LSTM(256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "WARNING:datasets.builder:Found cached dataset csv (C:/Users/Renan/.cache/huggingface/datasets/csv/default-6d6e490111dd0d23/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 251.94it/s]\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at C:\\Users\\Renan\\.cache\\huggingface\\datasets\\csv\\default-6d6e490111dd0d23\\0.0.0\\6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-edb2f1d343437643.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Metrics: \n",
      "\t Accuracy:  \t 0.46875\n",
      "\t Precision: \t 0.35506\n",
      "\t Recall:    \t 0.46875\n",
      "\t F1-score:  \t 0.43313\n",
      "Fit and predict time: 422.9488 seconds\n"
     ]
    }
   ],
   "source": [
    "df = load_preprocessing(preprocess = False)\n",
    "bert_model = RobertaModel.from_pretrained('roberta-base')\n",
    "epoch_valid_loss, epoch_valid_accu, epoch_valid_prediction = main(df, bert_model, train_process = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 2.7: DistilBERT Model + LSTM(256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "WARNING:datasets.builder:Found cached dataset csv (C:/Users/Renan/.cache/huggingface/datasets/csv/default-6d6e490111dd0d23/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 329.02it/s]\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at C:\\Users\\Renan\\.cache\\huggingface\\datasets\\csv\\default-6d6e490111dd0d23\\0.0.0\\6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-edb2f1d343437643.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Metrics: \n",
      "\t Accuracy:  \t 0.54688\n",
      "\t Precision: \t 0.5\n",
      "\t Recall:    \t 0.54688\n",
      "\t F1-score:  \t 0.54282\n",
      "Fit and predict time: 245.4527 seconds\n"
     ]
    }
   ],
   "source": [
    "df = load_preprocessing(preprocess = False)\n",
    "bert_model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "epoch_valid_loss, epoch_valid_accu, epoch_valid_prediction = main(df, bert_model, train_process = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 2.8: BlueBERT Model + LSTM(256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bionlp/bluebert_pubmed_uncased_L-12_H-768_A-12 were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "WARNING:datasets.builder:Found cached dataset csv (C:/Users/Renan/.cache/huggingface/datasets/csv/default-6d6e490111dd0d23/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 34.26it/s]\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at C:\\Users\\Renan\\.cache\\huggingface\\datasets\\csv\\default-6d6e490111dd0d23\\0.0.0\\6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-edb2f1d343437643.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Metrics: \n",
      "\t Accuracy:  \t 0.625\n",
      "\t Precision: \t 0.4625\n",
      "\t Recall:    \t 0.625\n",
      "\t F1-score:  \t 0.58594\n",
      "Fit and predict time: 459.1182 seconds\n"
     ]
    }
   ],
   "source": [
    "df = load_preprocessing(preprocess = False)\n",
    "bert_model = BertModel.from_pretrained('bionlp/bluebert_pubmed_uncased_L-12_H-768_A-12')\n",
    "epoch_valid_loss, epoch_valid_accu, epoch_valid_prediction = main(df, bert_model, train_process = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 2.9: RoBERTa Model + LSTM(512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "WARNING:datasets.builder:Found cached dataset csv (C:/Users/Renan/.cache/huggingface/datasets/csv/default-6d6e490111dd0d23/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 22.22it/s]\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at C:\\Users\\Renan\\.cache\\huggingface\\datasets\\csv\\default-6d6e490111dd0d23\\0.0.0\\6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-edb2f1d343437643.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Metrics: \n",
      "\t Accuracy:  \t 0.625\n",
      "\t Precision: \t 0.55392\n",
      "\t Recall:    \t 0.625\n",
      "\t F1-score:  \t 0.62891\n",
      "Fit and predict time: 384.0356 seconds\n"
     ]
    }
   ],
   "source": [
    "df = load_preprocessing(preprocess = False)\n",
    "bert_model = RobertaModel.from_pretrained('roberta-base')\n",
    "epoch_valid_loss, epoch_valid_accu, epoch_valid_prediction = main(df, bert_model, train_process = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 2.10: DistilBERT Model + LSTM(512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "WARNING:datasets.builder:Found cached dataset csv (C:/Users/Renan/.cache/huggingface/datasets/csv/default-6d6e490111dd0d23/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 99.67it/s]\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at C:\\Users\\Renan\\.cache\\huggingface\\datasets\\csv\\default-6d6e490111dd0d23\\0.0.0\\6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-edb2f1d343437643.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Metrics: \n",
      "\t Accuracy:  \t 0.625\n",
      "\t Precision: \t 0.875\n",
      "\t Recall:    \t 0.625\n",
      "\t F1-score:  \t 0.69705\n",
      "Fit and predict time: 179.0502 seconds\n"
     ]
    }
   ],
   "source": [
    "df = load_preprocessing(preprocess = False)\n",
    "bert_model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "epoch_valid_loss, epoch_valid_accu, epoch_valid_prediction = main(df, bert_model, train_process = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 2.11: BlueBERT Model + LSTM(512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bionlp/bluebert_pubmed_uncased_L-12_H-768_A-12 were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "WARNING:datasets.builder:Found cached dataset csv (C:/Users/Renan/.cache/huggingface/datasets/csv/default-6d6e490111dd0d23/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 43.42it/s]\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at C:\\Users\\Renan\\.cache\\huggingface\\datasets\\csv\\default-6d6e490111dd0d23\\0.0.0\\6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-edb2f1d343437643.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Metrics: \n",
      "\t Accuracy:  \t 0.54688\n",
      "\t Precision: \t 0.7107\n",
      "\t Recall:    \t 0.54688\n",
      "\t F1-score:  \t 0.58892\n",
      "Fit and predict time: 452.2441 seconds\n"
     ]
    }
   ],
   "source": [
    "df = load_preprocessing(preprocess = False)\n",
    "bert_model = BertModel.from_pretrained('bionlp/bluebert_pubmed_uncased_L-12_H-768_A-12')\n",
    "epoch_valid_loss, epoch_valid_accu, epoch_valid_prediction = main(df, bert_model, train_process = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion of Experiment(s) 2.1 - 2.11"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results above, we choose the DistilBERT model with LSTM having 128 neurons for our proposed model. If we were going to prioritize the performance and efficieny, we select the model that consistently demonstrates higher values for these metrics across multiple experiments. A model with higher accuracy and balanced precision and recall would be preferable. The DistilBERT model consistently demonstrated good performance across multiple metrics such as accuracy, precision, recall, and F1-score in the experiments. It achieved competitive results and showed potential for accurate medical specialty classification. In terms of efficiency, the fit and predict time for the DistilBERT model with LSTM 128 neurons was comparable to other models tested. It provides a good balance between performance and efficiency, making it a practical choice for real-world applications. In addition, DistilBERT is a compressed version of BERT that offers a smaller model size while maintaining good performance. This reduces computational resources required for training and inference compared to the larger models like RoBERTa or BlueBERT.\n",
    "\n",
    "The table below shows the summary of experiments 2.1 - 2.11 in terms of F1-score (%):\n",
    "\n",
    "|   Model        | Baseline LSTM (50) | BaselineLSTM (128) | BaselineLSTM (256) |\n",
    "|----------------|--------------------|--------------------|--------------------|\n",
    "|   BERT         |  59.376            | 55.468             | 47.264             |\n",
    "|   RoBERTa      |  66.073            | 56.211             | 43.313             |\n",
    "|   DistilBERT   |  73.438            | 56.099             | 54.282             |\n",
    "|   BlueBERT     |  71.070            | 67.160             | 58.594             |\n",
    "\n",
    "Based on these factors, the DistilBERT model with LSTM having 128 neurons appears to be a well-rounded choice, offering a balance between performance, efficiency, and general-purpose suitability for medical specialty classification tasks.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's train our customized medical tokenizer!\n",
    "\n",
    "As mentioned in Discussion 1, the vocabulary used in the field of medicine is extremely specialized, unique, and frequently made up of complicated jargon that is only applicable to that field. Therefore, using a specialized tokenizer created especially for medical texts can have a number of advantages in terms of precision, comprehension, and context preservation. Many medical phrases contain compound words, acronyms, abbreviations, and unique symbols that may not be properly handled by general tokenizers. \n",
    "\n",
    "BlueBERT and RoBERTa are some of the tokenizers which can handle medical texts. However, for the sake of learning, we want to train our own tokenizer using the given dataset as part of the proposed model to determine if it can improved our classification performance. We can construct our own tokenizer to successfully handle such scenarios by using the given dataset <code>mtsamples</code>. Becuase of this, better tokenization of medical terminology is made possible by this level of personalization, avoiding the loss of important information during preprocessing. \n",
    "\n",
    "BERT's tokenizer utilizes subword tokenization, which means that words are split into subword units based on learned vocabulary. This is indicated by the presence of '##' preceding some tokens, such as <code>'##ache', '##ost', '##omy', 'bro', '##nch', '##os', '##co', '##py', 'tr', '##ache', '##al', 'ste', '##nt', 'dil', '##ation', 'tr', '##ache', '##a', 'shi', '##ley', 'can', '##nu', '##la', 'tr', '##ache', '##ost', '##omy'</code>. This subword tokenization enables BERT to handle out-of-vocabulary words and capture subword-level information. This example is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['neck', 'exploration', ';', 'tr', '##ache', '##ost', '##omy', ';',\n",
       "       'urgent', 'flexible', 'bro', '##nch', '##os', '##co', '##py',\n",
       "       'via', 'tr', '##ache', '##ost', '##omy', 'site', ';', 'removal',\n",
       "       'of', 'foreign', 'body', ',', 'tr', '##ache', '##al', 'metallic',\n",
       "       'ste', '##nt', 'material', ';', 'dil', '##ation', 'distal', 'tr',\n",
       "       '##ache', '##a', ';', 'placement', 'of', '#', '8', 'shi', '##ley',\n",
       "       'single', 'can', '##nu', '##la', 'tr', '##ache', '##ost', '##omy',\n",
       "       'tube', '.'], dtype='<U11')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "np.array(tokenizer.tokenize(df['description'][5]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a customized tokenizer, we apply the high-level steps in the tokenization pipeline as presented from the HuggingFace library. Before splitting a text into tokens, the customer tokenizer should be initialized first by performing two steps to the dataset: normalization and pre-tokenization. The normalization step involves some general cleanup, such as removing needless whitespace, lowercasing, and/or removing accents. If youâ€™re familiar with Unicode normalization (such as NFC or NFKC), this is also something the tokenizer may apply.\n",
    "\n",
    "We start creating the tokenizer by instantiating a <code>Tokenizer</code> object with a model, then set its <code>normalizer, pre_tokenizer, post_processor</code>, and <code>decoder</code> attribute to the values we want. We specified the <code>[UNK]</code> token so that the model knows what to return when it encounters characters it hasn't seen before. We utilized the WordPiece tokenizer as the foundation of our customized tokenizer. During tokenization, the first step is normalization. Since BERT is widely used, there is a BertNormalizer with the classic options we can set for BERT: lowercase and strip_accents; clean_text to remove all control characters and replace repeating spaces with a single one.\n",
    "\n",
    "<center>\n",
    "<img src = '../figures/medical_tokenizer.PNG' width = '1500'/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateNewTokenizer:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = Tokenizer(models.WordPiece(unk_token = '[UNK]'))\n",
    "        self.tokenizer.normalizer = normalizers.Sequence([\n",
    "            normalizers.NFD(),\n",
    "            normalizers.Lowercase(),\n",
    "            normalizers.StripAccents()])\n",
    "        \n",
    "        self.tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()\n",
    "        self.special_tokens = ['[UNK]', '[PAD]', '[CLS]', '[SEP]', '[MASK]']\n",
    "        self.trainer = trainers.WordPieceTrainer(\n",
    "            vocab_size = 30000,\n",
    "            special_tokens = self.special_tokens)\n",
    "        \n",
    "        self.cls_token_id = None\n",
    "        self.sep_token_id = None\n",
    "\n",
    "    def load_dataset(self, filepath):\n",
    "        dataset = pd.read_csv(filepath)\n",
    "        dataset['transcription'].fillna(dataset['description'], inplace = True)\n",
    "        return dataset\n",
    "\n",
    "    def get_training_corpus(self, dataset):\n",
    "        for i in range(0, len(dataset), 1000):\n",
    "            yield dataset[i: i + 1000]['transcription']\n",
    "\n",
    "    def train_tokenizer(self, dataset):\n",
    "        self.tokenizer.train_from_iterator(\n",
    "            self.get_training_corpus(dataset),\n",
    "            trainer = self.trainer)\n",
    "        \n",
    "        self.cls_token_id = self.tokenizer.token_to_id('[CLS]')\n",
    "        self.sep_token_id = self.tokenizer.token_to_id('[SEP]')\n",
    "\n",
    "        self.tokenizer.post_processor = processors.TemplateProcessing(\n",
    "            single = f'[CLS]:0 $A:0 [SEP]:0',\n",
    "            pair   = f'[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1',\n",
    "            special_tokens = [('[CLS]', self.cls_token_id), ('[SEP]', self.sep_token_id)])\n",
    "        self.tokenizer.decoder = decoders.WordPiece(prefix = '##')\n",
    "\n",
    "    def save_tokenizer(self, filepath):\n",
    "        self.tokenizer.save(filepath)\n",
    "\n",
    "    def generate_tokenizer(self, dataset_filepath, tokenizer_filepath):\n",
    "        dataset = self.load_dataset(dataset_filepath)\n",
    "        self.train_tokenizer(dataset)\n",
    "        self.save_tokenizer(tokenizer_filepath)\n",
    "\n",
    "class CombinedTokenizer:\n",
    "    def __init__(self, bert_tokenizer, medical_tokenizer):\n",
    "        self.bert_tokenizer = bert_tokenizer\n",
    "        self.medical_tokenizer = medical_tokenizer\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        bert_tokens = self.bert_tokenizer.tokenize(text)\n",
    "        medical_tokens = self.medical_tokenizer.tokenize(text)\n",
    "        combined_tokens = bert_tokens + medical_tokens  \n",
    "        return combined_tokens\n",
    "\n",
    "\n",
    "tokenizer = GenerateNewTokenizer()\n",
    "tokenizer.generate_tokenizer('../data/mtsamples_modified.csv', '../data/tokenizer.json')\n",
    "token_path = '../data/tokenizer.json'\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "medical_tokenizer  = Tokenizer.from_file(token_path)\n",
    "medical_tokenizer  = BertTokenizerFast(tokenizer_object = medical_tokenizer)\n",
    "medical_tokenizer.model_max_length = 128\n",
    "combined_tokenizer = CombinedTokenizer(bert_tokenizer, medical_tokenizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our tokenizer correctly recognizes compound words like 'bronchoscopy', 'foreign body', 'distal trachea', 'tracheostomy tube' as individual tokens. BERT's tokenizer, on the other hand, splits some of these compound words into subwords, resulting in <code>'bro', '##nch', '##os', '##co', '##py', 'foreign', 'body', 'distal', 'tr', '##ache', '##a', 'tube'</code>. This split may result from the subword tokenization process of BERT's tokenizer, where it aims to create a more flexible and general-purpose tokenization scheme. Our tokenizer, which recognizes complete words and compound words as tokens, may better preserve the context and semantic meaning of the original text. This can be beneficial for tasks where precise word-level analysis or semantic understanding is important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['neck', 'exploration', ';', 'tracheostomy', ';', 'urgent',\n",
       "       'flexible', 'bronchoscopy', 'via', 'tracheostomy', 'site', ';',\n",
       "       'removal', 'of', 'foreign', 'body', ',', 'tracheal', 'metallic',\n",
       "       'stent', 'material', ';', 'dilation', 'distal', 'trachea', ';',\n",
       "       'placement', 'of', '#', '8', 'shiley', 'single', 'cannula',\n",
       "       'tracheostomy', 'tube', '.'], dtype='<U12')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(medical_tokenizer.tokenize(df['description'][5]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to train our proposed model using our medical tokenizer and DistilBERT. First, we'll try to analyze the results if we set the maximum sequence length of the tokenizer into 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset csv (C:/Users/Renan/.cache/huggingface/datasets/csv/default-1c41b1566159cdfa/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 248.42it/s]\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at C:\\Users\\Renan\\.cache\\huggingface\\datasets\\csv\\default-1c41b1566159cdfa\\0.0.0\\6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-7a2df9aea606f588.arrow\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \t Training: Loss 0.22216   \t Accuracy: 0.52802 \t Validation Loss  0.22097 \t Accuracy: 0.40326\n",
      "Epoch: 2 \t Training: Loss 0.22193   \t Accuracy: 0.57661 \t Validation Loss  0.22131 \t Accuracy: 0.39123\n",
      "Epoch: 3 \t Training: Loss 0.22202   \t Accuracy: 0.55516 \t Validation Loss  0.22169 \t Accuracy: 0.42208\n",
      "Epoch: 4 \t Training: Loss 0.22208   \t Accuracy: 0.53161 \t Validation Loss  0.221 \t Accuracy: 0.49128\n",
      "Epoch: 5 \t Training: Loss 0.2218   \t Accuracy: 0.56304 \t Validation Loss  0.22193 \t Accuracy: 0.4898\n",
      "Training time: 2947.5158 seconds\n",
      "Final Best Model from Best Epoch 1 Test Loss = 0.2206896551724139, Test Accuracy = 0.10300206881144951\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(101)\n",
    "df = load_preprocessing(preprocess = False)\n",
    "text_column, labels, num_classes = GetText(df)\n",
    "\n",
    "processor = TokenizationProcessor(max_sequence_length = 32, tokenizer = medical_tokenizer)\n",
    "input_ids, attention_mask, token_type_ids = processor.process_dataset('../data/mtsamples_modified.csv')\n",
    "if input_ids.size(0) != labels.size(0):    \n",
    "    desired_length = input_ids.size(0)\n",
    "    padding_length = desired_length - len(labels)\n",
    "    padding = torch.zeros(padding_length, dtype = torch.long)\n",
    "    labels  = torch.cat((labels, padding))\n",
    "    \n",
    "train_loader, valid_loader, test_loader = GENERATE_DATALOADER(input_ids, attention_mask, labels, use_sampler = True)\n",
    "\n",
    "bert_model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "num_epochs = 5\n",
    "criterion  = nn.CrossEntropyLoss()\n",
    "optimizer  = torch.optim.Adam(bert_model.parameters(), lr = 0.001)\n",
    "train_losses, valid_losses, train_accurs, valid_accurs, test_loss, test_accu, best_epoch, epoch_times, test_predict, best_model = train(num_epochs, bert_model, train_loader, valid_loader, test_loader, \n",
    "                                                                                                                                        optimizer, criterion, device, accuracy = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about let's try 64?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset csv (C:/Users/Renan/.cache/huggingface/datasets/csv/default-1c41b1566159cdfa/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 45.63it/s]\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at C:\\Users\\Renan\\.cache\\huggingface\\datasets\\csv\\default-1c41b1566159cdfa\\0.0.0\\6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-7a2df9aea606f588.arrow\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \t Training: Loss 0.21085   \t Accuracy: 0.70833 \t Validation Loss  0.22024 \t Accuracy: 0.57794\n",
      "Epoch: 2 \t Training: Loss 0.21085   \t Accuracy: 0.66727 \t Validation Loss  0.22069 \t Accuracy: 0.63679\n",
      "Epoch: 3 \t Training: Loss 0.2108   \t Accuracy: 0.68032 \t Validation Loss  0.22069 \t Accuracy: 0.52107\n",
      "Epoch: 4 \t Training: Loss 0.21102   \t Accuracy: 0.65409 \t Validation Loss  0.22031 \t Accuracy: 0.50521\n",
      "Epoch: 5 \t Training: Loss 0.21099   \t Accuracy: 0.66272 \t Validation Loss  0.22059 \t Accuracy: 0.58406\n",
      "Training time: 3293.4381 seconds\n",
      "Final Best Model from Best Epoch 1 Test Loss = 0.22055172413793106, Test Accuracy = 0.5723224092146446\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(101)\n",
    "df = load_preprocessing(preprocess = False)\n",
    "text_column, labels, num_classes = GetText(df)\n",
    "\n",
    "processor = TokenizationProcessor(max_sequence_length = 64, tokenizer = medical_tokenizer)\n",
    "input_ids, attention_mask, token_type_ids = processor.process_dataset('../data/mtsamples_modified.csv')\n",
    "if input_ids.size(0) != labels.size(0):    \n",
    "    desired_length = input_ids.size(0)\n",
    "    padding_length = desired_length - len(labels)\n",
    "    padding = torch.zeros(padding_length, dtype = torch.long)\n",
    "    labels  = torch.cat((labels, padding))\n",
    "    \n",
    "train_loader, valid_loader, test_loader = GENERATE_DATALOADER(input_ids, attention_mask, labels, use_sampler = True)\n",
    "\n",
    "bert_model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "num_epochs = 5\n",
    "criterion  = nn.CrossEntropyLoss()\n",
    "optimizer  = torch.optim.AdamW(bert_model.parameters(), lr = 0.0001)\n",
    "train_losses, valid_losses, train_accurs, valid_accurs, test_loss, test_accu, best_epoch, epoch_times, test_predict, best_model = train(num_epochs, bert_model, train_loader, valid_loader, test_loader, \n",
    "                                                                                                                                        optimizer, criterion, device, accuracy = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to use 128 as the maximum sequence length of the medical tokenizer. Some practices utilize 128 sequence length for efficiency without performance degredation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset csv (C:/Users/Renan/.cache/huggingface/datasets/csv/default-1c41b1566159cdfa/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 100.00it/s]\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at C:\\Users\\Renan\\.cache\\huggingface\\datasets\\csv\\default-1c41b1566159cdfa\\0.0.0\\6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-7a2df9aea606f588.arrow\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \t Training: Loss 0.17282   \t Accuracy: 0.95131 \t Validation Loss  0.22186 \t Accuracy: 0.83904\n",
      "Epoch: 2 \t Training: Loss 0.17306   \t Accuracy: 0.90805 \t Validation Loss  0.22293 \t Accuracy: 0.77683\n",
      "Epoch: 3 \t Training: Loss 0.17315   \t Accuracy: 0.92241 \t Validation Loss  0.22414 \t Accuracy: 0.76097\n",
      "Epoch: 4 \t Training: Loss 0.17279   \t Accuracy: 0.91252 \t Validation Loss  0.22172 \t Accuracy: 0.71851\n",
      "Epoch: 5 \t Training: Loss 0.17257   \t Accuracy: 0.81034 \t Validation Loss  0.22159 \t Accuracy: 0.68505\n",
      "Training time: 4535.5318 seconds\n",
      "Final Best Model from Best Epoch 5 Test Loss = 0.22286206896551727, Test Accuracy = 0.48537241230750905\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(101)\n",
    "df = load_preprocessing(preprocess = False)\n",
    "text_column, labels, num_classes = GetText(df)\n",
    "\n",
    "processor = TokenizationProcessor(max_sequence_length = 128, tokenizer = medical_tokenizer)\n",
    "input_ids, attention_mask, token_type_ids = processor.process_dataset('../data/mtsamples_modified.csv')\n",
    "if input_ids.size(0) != labels.size(0):    \n",
    "    desired_length = input_ids.size(0)\n",
    "    padding_length = desired_length - len(labels)\n",
    "    padding = torch.zeros(padding_length, dtype = torch.long)\n",
    "    labels  = torch.cat((labels, padding))\n",
    "    \n",
    "train_loader, valid_loader, test_loader = GENERATE_DATALOADER(input_ids, attention_mask, labels, use_sampler = True)\n",
    "\n",
    "bert_model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "num_epochs = 5\n",
    "criterion  = nn.CrossEntropyLoss()\n",
    "optimizer  = torch.optim.AdamW(bert_model.parameters(), lr = 0.001)\n",
    "train_losses, valid_losses, train_accurs, valid_accurs, test_loss, test_accu, best_epoch, epoch_times, test_predict, best_model = train(num_epochs, bert_model, train_loader, valid_loader, test_loader, \n",
    "                                                                                                                                        optimizer, criterion, device, accuracy = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decreasing accuracy values on both training and validation data, along with the increasing loss on the validation set, indicate that the model is likely overfitting. This occurs because our model learns to perform exceptionally well on the training data but struggles to generalize to new data. It is advisable to consider methods to mitigate overfitting, such as increasing the dataset size, using regularization techniques (e.g., dropout), or exploring more complex model architectures. Moreover, this indicate that the model is not capturing the underlying patterns and features that differentiate between the medical specialties effectively. Further optimization techniques like gradient accumulation, distributed training, or hardware acceleration can be explored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Metrics: \n",
      "\t Accuracy:  \t 0.84375\n",
      "\t Precision: \t 0.7302\n",
      "\t Recall:    \t 0.84375\n",
      "\t F1-score:  \t 0.71742\n"
     ]
    }
   ],
   "source": [
    "df = load_preprocessing(preprocess = False)\n",
    "epoch_valid_loss, epoch_valid_accu, epoch_valid_prediction = main(df, best_model, train_process = False)\n",
    "for idx, data in enumerate(test_loader):\n",
    "    inputs, attens, labels = data \n",
    "    break\n",
    "eval_predictions(epoch_valid_prediction, labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <code>Question 9</code> Criticize the results from the proposed model based on theoretical and practical perspective with supporting literature.\n",
    "\n",
    "We utilized the DistilBERT model with an LSTM layer, comprising 128 neurons, for the purpose of classifying medical specialties based on medical transcription text. To prepare the dataset, we employed our customized medical tokenizer, initialized using the BertTokenizerFast algorithm. Due to limited computational resources, we simulated the training process for only five epochs. The training was simulated using PyTorch version 3.9, with an Intel 8th generation CPU, and an NVIDIA RTX 1050Ti 4GB graphics card, similar to previous experiments. The results indicated potential signs of overfitting during training, as both the training and validation accuracies displayed a decreasing yet oscillating trend. To improve the training process, it would be beneficial to increase the number of epochs to an appropriate value once the model converges to the local (or global) minima. However, this would necessitate higher memory or GPU resources.\n",
    "\n",
    "Recent studies have validated DistilBERT as a viable alternative to the original BERT model, particularly for handling medical text. For instance, <code>Abadeer (2020)</code> evaluated the performance of DistilBERT for the Named Entity Recognition (NER) task in medical records. This study aimed to determine how DistilBERT performs when fine-tuned with medical corpora compared to the pre-trained versions of BERT. The results demonstrated that DistilBERT achieved nearly identical performance to the medical versions of BERT in terms of F1-score. This implies that DistilBERT can provide excellent results, similar to BERT, but with reduced complexity, as the conventional BERT model contains 110 million parameters. However, a major limitation of DistilBERT is that it requires setting a maximum length of 512 tokens (similar to BERT), which proved insufficient for most sequences in the medical transcription dataset. Consequently, we had to split, truncate, and pad our tokens, potentially leading to highly sparse embedding vectors. Additionally, our findings align with <code>Abadeer (2020)'s</code> study, indicating that DistilBERT outperformed BlueBERT. <code>Abadeer (2020)</code> compared the two models and demonstrated that DistilBERT achieved comparable results with double the runtime speed. Thus, utilizing DistilBERT can offer a faster and more efficient model, particularly when fine-tuned, without sacrificing classification performance.\n",
    "\n",
    "Most studies in natural language processing (NLP) focus on the pretrain-finetuning approach (PFA), which may present certain disadvantages, particularly for industries lacking sufficient server environments but requiring efficiency and high accuracy. Instead of solely searching for the best model to fulfill these objectives, researchers have developed their own tokenizers by examining the limitations of byte pair encoding (BPE) <code>(Sennrich et al., 2015)</code> and SentencePiece <code>(Kudo and Richardson, 2018)</code>. <code>Park et al. (2021)</code> adopted this approach and proposed an optimal tokenization method to improve machine translation performance based on morphological segmentation and vocabulary techniques. In our case, we trained a customized tokenizer with WordPiece as its foundation. Similarly, <code>Bilal et al. (2023)</code> employed a similar method to ours, loading a JSON file as an initial tokenizer for the BertTokenizerFast, which significantly enhanced their baseline models. The primary motivation behind creating such a tokenizer was the limited vocabulary of their language, Roman Urdu. Utilizing a specialized tokenizer designed specifically for medical texts provides several advantages in terms of precision, comprehension, and context preservation. Many medical phrases contain compound words, acronyms, abbreviations, and unique symbols that may not be properly handled by general tokenizers. Tokenizers like BlueBERT <code>(Peng et al., 2020)</code> and RoBERTa are capable of handling medical texts. However, for the purpose of learning, we opted to train our own tokenizer using the provided dataset, <code>mtsamples</code>, as part of the proposed model, to determine if it could improve our classification performance. This level of personalization enables better tokenization of medical terminology, avoiding the loss of important information during preprocessing.\n",
    "\n",
    "In our proposed model, we relied solely on a unidirectional LSTM as our classifier. However, recent studies indicate that this approach may result in a limited contextual understanding <code>(Anki et al., 2021; Abduljabbar et al., 2021)</code>. Since the unidirectional LSTM only considers the past context of a given token in the sequence, it fails to capture crucial future information necessary for accurate classification of medical transcriptions. For instance, <code>Abduljabbar et al. (2021)</code> compared LSTM and BiLSTM in the evaluation of short-term traffic prediction models. In their work, unidirectional LSTM is extended to bidirectional LSTM (BiLSTM) networks which train the input data twice through forward and backward directions. he results showed BiLSTM performed better for variable prediction horizons for both speed and flow. Other tasks like sentiment analysis or named entity recognition, which rely on the influence of future words, can be challenging for the unidirectional LSTM, leading to suboptimal classification results. Furthermore, our dataset comprises long sequences of tokens, and the unidirectional LSTM suffers from information loss at the beginning or end of the text. As the network processes the sequence, the initial words or tokens receive diminishing influence from future words, potentially resulting in a weaker representation of the overall meaning. This loss of contextual information at the sequence ends can significantly hinder text classification tasks that require a comprehensive understanding of the text. To address these limitations, a more viable solution is the use of bidirectional LSTM (BiLSTM) in text classification tasks. The BiLSTM can consider both past and future contexts, allowing for a more holistic understanding of the text and capturing bidirectional dependencies. By incorporating future information, the BiLSTM effectively mitigates the drawbacks associated with the unidirectional LSTM and generally exhibits superior performance in various text classification scenarios."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "```\n",
    "[1] Zhang, W., Wei, W., Wang, W., Jin, L., & Cao, Z. (2021, March). Reducing BERT computation by padding removal and curriculum learning. In 2021 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS) (pp. 90-92). IEEE.\n",
    "\n",
    "[2] Lai, T., Ji, H., & Zhai, C. (2021). BERT might be overkill: A tiny but effective biomedical entity linker based on residual convolutional neural networks. arXiv preprint arXiv:2109.02237.\n",
    "\n",
    "[3] Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.\n",
    "\n",
    "[4] Peng, Y., Yan, S., & Lu, Z. (2019). Transfer learning in biomedical natural language processing: an evaluation of BERT and ELMo on ten benchmarking datasets. arXiv preprint arXiv:1906.05474.\n",
    "\n",
    "[5] Loshchilov, I., & Hutter, F. (2017). Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101.\n",
    "\n",
    "[6] Abadeer, M. (2020, November). Assessment of DistilBERT performance on named entity recognition task for the detection of protected health information and medical concepts. In Proceedings of the 3rd clinical natural language processing workshop (pp. 158-167).\n",
    "\n",
    "[7] Peng, Y., Chen, Q., & Lu, Z. (2020). An empirical study of multi-task learning on BERT for biomedical text mining. arXiv preprint arXiv:2005.02799.\n",
    "\n",
    "[8] Park, C., Eo, S., Moon, H., & Lim, H. S. (2021, June). Should we find another model?: Improving neural machine translation performance with one-piece tokenization method without model modification. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers (pp. 97-104).\n",
    "\n",
    "[9] Bilal, M., Khan, A., Jan, S., Musa, S., & Ali, S. (2023). Roman Urdu hate speech detection using transformer-based model for cyber security applications. Sensors, 23(8), 3909.\n",
    "\n",
    "[10] Anki, P., & Bustamam, A. (2021). Measuring the accuracy of LSTM and BiLSTM models in the application of artificial intelligence by applying chatbot programme. Indonesian Journal of Electrical Engineering and Computer Science, 23(1), 197-205.\n",
    "\n",
    "[11] Abduljabbar, R. L., Dia, H., & Tsai, P. W. (2021). Unidirectional and bidirectional LSTM models for short-term traffic prediction. Journal of Advanced Transportation, 2021, 1-16.\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
